import os
import json
from typing import Dict, List, Optional, TypedDict, Annotated
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage as AnyMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

class CPXEvaluationState(TypedDict):
    """CPX í‰ê°€ ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    user_id: str
    scenario_id: str
    conversation_log: List[Dict]
    
    # ë¶„ì„ ê²°ê³¼ë“¤
    conversation_analysis: Optional[Dict]
    checklist_results: Optional[Dict]
    question_analysis: Optional[Dict]
    empathy_analysis: Optional[Dict]
    
    # ì œì–´ í”Œë˜ê·¸ë“¤
    confidence_score: float
    retry_count: int
    needs_enhancement: bool
    
    # ìµœì¢… ê²°ê³¼
    final_scores: Optional[Dict]
    feedback: Optional[Dict]
    
    # ë©”íƒ€ë°ì´í„°
    evaluation_metadata: Optional[Dict]
    
    # ë©”ì‹œì§€ ì¶”ì 
    messages: Annotated[List[AnyMessage], add_messages]

class LangGraphEvaluationService:
    def __init__(self):
        """LangGraph ê¸°ë°˜ CPX í‰ê°€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # OpenAI API ì„¤ì •
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
        self.llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-3.5-turbo",
            temperature=0.3,
            max_tokens=2000
        )
        
        # ë³‘ë ¥ì²­ì·¨ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
        self.history_taking_checklist = {
            "ìê¸°ì†Œê°œ_ë°_ë©´ë‹´_ì£¼ì œ_í˜‘ìƒ": {
                "name": "ìê¸°ì†Œê°œ ë° ë©´ë‹´ ì£¼ì œ í˜‘ìƒ",
                "required_elements": [
                    "ì•ˆë…•í•˜ì„¸ìš”, í•™ìƒì˜ì‚¬ â—‹â—‹â—‹ì…ë‹ˆë‹¤",
                    "í™˜ìë¶„ ì„±í•¨ê³¼ ë‚˜ì´ í™•ì¸", 
                    "ë³‘ì›ì— ì˜¤ì‹œëŠ” ë° ë¶ˆí¸í•˜ì§€ëŠ” ì•Šìœ¼ì…¨ë‚˜ìš”?",
                    "ì˜¤ëŠ˜ ___ê°€ ë¶ˆí¸í•´ì„œ ì˜¤ì…¨êµ°ìš”",
                    "ì•½ 10ë¶„ ì •ë„ ë¬¸ì§„ê³¼ ì‹ ì²´ì§„ì°°ì„ í•œ ë’¤, ì„¤ëª…ì„ ë“œë¦¬ê³  í•©ë‹ˆë‹¤"
                ],
                "weight": 0.1
            },
            "ì–¸ì œ_OPQRST": {
                "name": "ì–¸ì œ (OPQRST)",
                "required_elements": [
                    "ì¦ìƒì˜ ë°œìƒ ì‹œì  (O: Onset)",
                    "ì¦ìƒì˜ ìœ ì§€ ê¸°ê°„ (D: Duration)", 
                    "ì¦ìƒì˜ ì•…í™”/ì™„í™” ì–‘ìƒ (Co: Course)",
                    "ê³¼ê±° ìœ ì‚¬í•œ ì¦ìƒì˜ ê²½í—˜ ì—¬ë¶€ (Ex: Experience)"
                ],
                "weight": 0.25
            },
            "ì–´ë””ì„œ": {
                "name": "ì–´ë””ì„œ",
                "required_elements": [
                    "(í†µì¦, ë°œì§„ ë“± ì¼ë¶€ ì„ìƒí‘œí˜„) ì¦ìƒì˜ ìœ„ì¹˜ (L: Location)",
                    "ì¦ìƒì´ ì´‰ë°œ/ì•…í™”/ì™„í™”ë˜ëŠ” ìƒí™© (F: Factor)"
                ],
                "weight": 0.15
            },
            "1ì°¨_ìš”ì•½": {
                "name": "1ì°¨ ìš”ì•½ (3ê°€ì§€ ì´ìƒ ì–¸ê¸‰)",
                "required_elements": [
                    "ëª¸ ìƒíƒœì— ëŒ€í•´ ëª‡ ê°€ì§€ ë” ì—¬ì­¤ê² ìŠµë‹ˆë‹¤"
                ],
                "weight": 0.1
            },
            "ì–´ë–»ê²Œ": {
                "name": "ì–´ë–»ê²Œ",
                "required_elements": [
                    "ì¦ìƒì˜ ì–‘ìƒ, ë¬¼ì„±í•¨ì˜ ì •ë„ (C: Character)",
                    "ì‘ê¸‰, ì¶œí˜ˆê²½í–¥ì„± ë“±",
                    "ì£¼ì†Œì— í”íˆ ë™ë°˜ë˜ëŠ” ë‹¤ë¥¸ ì¦ìƒ (ê³„í†µ ë¬¸ì§„) (A: Associated sx.)"
                ],
                "weight": 0.15
            },
            "ì™œ": {
                "name": "ì™œ",
                "required_elements": [
                    "ê°ë³„ì— ë„ì›€ì´ ë˜ëŠ” ë‹¤ë¥¸ ì¦ìƒ"
                ],
                "weight": 0.1
            },
            "ëˆ„ê°€": {
                "name": "ëˆ„ê°€", 
                "required_elements": [
                    "ê³¼ê±° ë³‘ë ¥/ìˆ˜ìˆ  ì´ë ¥/ê±´ê°•ê²€ì§„ ì—¬ë¶€ (ê³¼: ê³¼ê±°ë ¥)",
                    "ì•½ë¬¼ë ¥ (ì•½: ì•½ë¬¼ë ¥)",
                    "ì§ì—…, ìŒì£¼, í¡ì—°, ìƒí™œìŠµê´€ (ì‚¬: ì‚¬íšŒë ¥)",
                    "ê°€ì¡±ë ¥ (ê°€: ê°€ì¡±ë ¥)"
                ],
                "weight": 0.1
            },
            "2ì°¨_ìš”ì•½": {
                "name": "2ì°¨ ìš”ì•½ (3ê°€ì§€ ì´ìƒ ì–¸ê¸‰)",
                "required_elements": [
                    "ì§ˆë¬¸ ë‚´ìš©ì´ ë§ì•˜ëŠ”ë°, ì˜ ëŒ€ë‹µí•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
                    "ë§ì€ ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤"
                ],
                "weight": 0.05
            }
        }
        
        # ì›Œí¬í”Œë¡œìš° ìƒì„±
        self.workflow = self._create_evaluation_workflow()

    def _create_evaluation_workflow(self):
        """CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        
        # StateGraph ìƒì„± (LangGraph 0.6.3 ë°©ì‹)
        workflow = StateGraph(CPXEvaluationState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("initialize", self._initialize_evaluation)
        workflow.add_node("analyze_conversation", self._analyze_conversation)
        workflow.add_node("evaluate_checklist", self._evaluate_checklist)
        workflow.add_node("evaluate_questions", self._evaluate_questions)
        workflow.add_node("evaluate_empathy", self._evaluate_empathy)
        workflow.add_node("quality_check", self._quality_check)
        workflow.add_node("enhanced_evaluation", self._enhanced_evaluation)
        workflow.add_node("calculate_scores", self._calculate_final_scores)
        workflow.add_node("generate_feedback", self._generate_feedback)
        workflow.add_node("finalize_results", self._finalize_results)
        
        # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ì„¤ì • (LangGraph 0.6.3 ë°©ì‹)
        workflow.set_entry_point("initialize")
        
        # ìˆœì°¨ ì‹¤í–‰ ì—£ì§€
        workflow.add_edge("initialize", "analyze_conversation")
        workflow.add_edge("analyze_conversation", "evaluate_checklist")
        workflow.add_edge("evaluate_checklist", "evaluate_questions")
        workflow.add_edge("evaluate_questions", "evaluate_empathy")
        workflow.add_edge("evaluate_empathy", "quality_check")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ… (LangGraph 0.6.3 ë°©ì‹)
        workflow.add_conditional_edges(
            "quality_check",
            self._should_enhance_evaluation,
            {
                "enhance": "enhanced_evaluation",
                "calculate": "calculate_scores"
            }
        )
        
        workflow.add_edge("enhanced_evaluation", "calculate_scores")
        workflow.add_edge("calculate_scores", "generate_feedback")
        workflow.add_edge("generate_feedback", "finalize_results")
        workflow.add_edge("finalize_results", END)
        
        return workflow.compile()

    # =============================================================================
    # ì›Œí¬í”Œë¡œìš° ë…¸ë“œë“¤ (ê°ê° ë‹¨ì¼ ì±…ì„)
    # =============================================================================
    
    def _initialize_evaluation(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """1ë‹¨ê³„: í‰ê°€ ì´ˆê¸°í™”"""
        print(f"ğŸ¯ [{state['user_id']}] CPX í‰ê°€ ì´ˆê¸°í™” - ì‹œë‚˜ë¦¬ì˜¤: {state['scenario_id']}")
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "user_id": state["user_id"],
            "scenario_id": state["scenario_id"],
            "evaluation_date": datetime.now().isoformat(),
            "total_interactions": len(state["conversation_log"]),
            "conversation_duration_minutes": len(state["conversation_log"]) * 0.5,
            "voice_recording_path": "s3ë¡œ ì €ì¥",
            "conversation_transcript": json.dumps(state["conversation_log"], ensure_ascii=False)
        }
        
        return {
            **state,
            "evaluation_metadata": metadata,
            "confidence_score": 0.0,
            "retry_count": 0,
            "needs_enhancement": False,
            "messages": [HumanMessage(content="CPX í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")]
        }

    def _analyze_conversation(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """2ë‹¨ê³„: ëŒ€í™” ê¸°ë³¸ ë¶„ì„"""
        print(f"ğŸ“Š [{state['user_id']}] ëŒ€í™” ë¶„ì„ ì‹œì‘")
        
        conversation_log = state["conversation_log"]
        
        if not conversation_log:
            return {
                **state,
                "conversation_analysis": {"total_questions": 0, "duration_minutes": 0, "question_types": {}},
                "messages": state["messages"] + [HumanMessage(content="ëŒ€í™” ë¡œê·¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")]
            }
        
        total_questions = len([msg for msg in conversation_log if msg.get("type") == "student"])
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        open_questions = 0
        closed_questions = 0
        
        for msg in conversation_log:
            if msg.get("type") == "student":
                question = msg.get("content", "")
                if any(word in question for word in ["ì–´ë–¤", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””"]):
                    open_questions += 1
                elif any(word in question for word in ["ìˆë‚˜ìš”", "ì—†ë‚˜ìš”", "ë§ë‚˜ìš”"]):
                    closed_questions += 1
        
        analysis = {
            "total_questions": total_questions,
            "duration_minutes": len(conversation_log) * 0.5,
            "question_types": {
                "open_questions": open_questions,
                "closed_questions": closed_questions,
                "open_ratio": open_questions / max(total_questions, 1)
            }
        }
        
        print(f"âœ… [{state['user_id']}] ëŒ€í™” ë¶„ì„ ì™„ë£Œ - ì´ ì§ˆë¬¸: {total_questions}")
        
        return {
            **state,
            "conversation_analysis": analysis,
            "messages": state["messages"] + [HumanMessage(content=f"ëŒ€í™” ë¶„ì„ ì™„ë£Œ: ì´ {total_questions}ê°œ ì§ˆë¬¸")]
        }

    def _evaluate_checklist(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """3ë‹¨ê³„: ë³‘ë ¥ì²­ì·¨ ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€"""
        print(f"ğŸ“‹ [{state['user_id']}] ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ ì‹œì‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        checklist_results = {}
        total_confidence = 0.0
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ (ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
        for category_id, category_info in self.history_taking_checklist.items():
            try:
                result = self._evaluate_checklist_category_sync(
                    conversation_text, 
                    category_info, 
                    state["scenario_id"]
                )
                checklist_results[category_id] = result
                total_confidence += result.get("completion_rate", 0) * result.get("quality_score", 5) / 10
                
            except Exception as e:
                print(f"âŒ [{state['user_id']}] ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ {category_id} í‰ê°€ ì˜¤ë¥˜: {e}")
                checklist_results[category_id] = {
                    "completed_elements": [],
                    "missing_elements": category_info['required_elements'],
                    "completion_rate": 0.0,
                    "quality_score": 5,
                    "specific_feedback": f"í‰ê°€ ì˜¤ë¥˜: {str(e)}"
                }
        
        # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
        confidence_score = total_confidence / len(self.history_taking_checklist)
        
        print(f"âœ… [{state['user_id']}] ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence_score:.2f}")
        
        return {
            **state,
            "checklist_results": checklist_results,
            "confidence_score": confidence_score,
            "messages": state["messages"] + [HumanMessage(content=f"ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ - ì‹ ë¢°ë„: {confidence_score:.2f}")]
        }

    def _evaluate_questions(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """4ë‹¨ê³„: ì§ˆë¬¸ ê¸°ë²• í‰ê°€"""
        print(f"â“ [{state['user_id']}] ì§ˆë¬¸ ê¸°ë²• í‰ê°€ ì‹œì‘")
        
        conversation_log = state["conversation_log"]
        student_questions = [msg for msg in conversation_log if msg.get("type") == "student"]
        
        if not student_questions:
            return {
                **state,
                "question_analysis": {
                    "total_questions": 0,
                    "positive_techniques": 0,
                    "negative_techniques": 0,
                    "technique_analysis": {}
                },
                "messages": state["messages"] + [HumanMessage(content="í•™ìƒ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")]
            }
        
        positive_count = 0
        negative_count = 0
        technique_details = {}
        
        for i, question_msg in enumerate(student_questions):
            question = question_msg.get("content", "")
            
            # ê¸ì •ì  ê¸°ë²• ì²´í¬
            if self._is_open_question(question):
                positive_count += 1
                technique_details[f"ê°œë°©í˜•_ì§ˆë¬¸_{i}"] = {
                    "type": "positive",
                    "technique": "ê°œë°©í˜• ì§ˆë¬¸ ì‚¬ìš©",
                    "example": question[:50] + "..."
                }
            
            if self._is_empathetic_question(question):
                positive_count += 1
                technique_details[f"ê³µê°ì _ì§ˆë¬¸_{i}"] = {
                    "type": "positive", 
                    "technique": "ê³µê°ì  í‘œí˜„ ì‚¬ìš©",
                    "example": question[:50] + "..."
                }
            
            # ë¶€ì •ì  ê¸°ë²• ì²´í¬
            if self._is_leading_question(question):
                negative_count += 1
                technique_details[f"ìœ ë„_ì§ˆë¬¸_{i}"] = {
                    "type": "negative",
                    "technique": "ìœ ë„ ì§ˆë¬¸ ì‚¬ìš© (ê°œì„  í•„ìš”)",
                    "example": question[:50] + "..."
                }
        
        technique_score = max(1, min(10, (positive_count * 2 - negative_count) + 5))
        
        analysis = {
            "total_questions": len(student_questions),
            "positive_techniques": positive_count,
            "negative_techniques": negative_count,
            "technique_score": technique_score,
            "technique_analysis": technique_details
        }
        
        print(f"âœ… [{state['user_id']}] ì§ˆë¬¸ ê¸°ë²• í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {technique_score}")
        
        return {
            **state,
            "question_analysis": analysis,
            "messages": state["messages"] + [HumanMessage(content=f"ì§ˆë¬¸ ê¸°ë²• í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {technique_score}")]
        }

    def _evaluate_empathy(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """5ë‹¨ê³„: ê³µê° ëŠ¥ë ¥ í‰ê°€ (ìƒˆë¡œìš´ í‰ê°€ ê¸°ì¤€)"""
        print(f"ğŸ’ [{state['user_id']}] ê³µê° ëŠ¥ë ¥ í‰ê°€ ì‹œì‘")
        
        conversation_log = state["conversation_log"]
        student_questions = [msg for msg in conversation_log if msg.get("type") == "student"]
        
        empathy_score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        empathy_indicators = []
        
        for question_msg in student_questions:
            question = question_msg.get("content", "")
            
            # ê³µê°ì  í‘œí˜„ ì²´í¬
            empathy_keywords = ["í˜ë“œì‹œê² ì–´ìš”", "ê±±ì •", "ì´í•´", "ë¶ˆí¸", "ê´œì°®ìœ¼ì‹œ", "í¸í•˜ê²Œ", "ì²œì²œíˆ"]
            if any(keyword in question for keyword in empathy_keywords):
                empathy_score += 1.0
                empathy_indicators.append(f"ê³µê°ì  í‘œí˜„: '{question[:30]}...'")
            
            # í™˜ì ì¤‘ì‹¬ì  í‘œí˜„ ì²´í¬
            patient_centered = ["í™˜ìë¶„", "ì–´ë– ì„¸ìš”", "ë§ì”€í•´ì£¼ì„¸ìš”", "ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”"]
            if any(phrase in question for phrase in patient_centered):
                empathy_score += 0.5
                empathy_indicators.append(f"í™˜ì ì¤‘ì‹¬ì : '{question[:30]}...'")
        
        empathy_score = min(10.0, empathy_score)
        
        analysis = {
            "empathy_score": empathy_score,
            "empathy_indicators": empathy_indicators,
            "total_empathetic_expressions": len(empathy_indicators)
        }
        
        print(f"âœ… [{state['user_id']}] ê³µê° ëŠ¥ë ¥ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {empathy_score}")
        
        return {
            **state,
            "empathy_analysis": analysis,
            "messages": state["messages"] + [HumanMessage(content=f"ê³µê° ëŠ¥ë ¥ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {empathy_score}")]
        }

    def _quality_check(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """6ë‹¨ê³„: í‰ê°€ í’ˆì§ˆ ì²´í¬"""
        print(f"ğŸ” [{state['user_id']}] í‰ê°€ í’ˆì§ˆ ì²´í¬")
        
        confidence_score = state.get("confidence_score", 0.0)
        checklist_results = state.get("checklist_results", {})
        
        # í’ˆì§ˆ ì²´í¬ ê¸°ì¤€
        needs_enhancement = False
        quality_issues = []
        
        # ì‹ ë¢°ë„ ì²´í¬
        if confidence_score < 0.6:
            needs_enhancement = True
            quality_issues.append(f"ë‚®ì€ ì‹ ë¢°ë„: {confidence_score:.2f}")
        
        # ì™„ë£Œìœ¨ ì²´í¬
        low_completion_categories = []
        for category_id, result in checklist_results.items():
            if result.get("completion_rate", 0) < 0.3:
                low_completion_categories.append(category_id)
        
        if len(low_completion_categories) > 3:
            needs_enhancement = True
            quality_issues.append(f"ë‚®ì€ ì™„ë£Œìœ¨ ì¹´í…Œê³ ë¦¬ {len(low_completion_categories)}ê°œ")
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
        if state.get("retry_count", 0) >= 2:
            needs_enhancement = False  # ë” ì´ìƒ ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
            quality_issues.append("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
        
        print(f"ğŸ” [{state['user_id']}] í’ˆì§ˆ ì²´í¬ ì™„ë£Œ - í–¥ìƒ í•„ìš”: {needs_enhancement}")
        
        return {
            **state,
            "needs_enhancement": needs_enhancement,
            "quality_issues": quality_issues,
            "messages": state["messages"] + [HumanMessage(content=f"í’ˆì§ˆ ì²´í¬ ì™„ë£Œ - ì´ìŠˆ: {len(quality_issues)}ê°œ")]
        }

    def _enhanced_evaluation(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """7ë‹¨ê³„: í–¥ìƒëœ í‰ê°€ (í•„ìš”ì‹œ)"""
        print(f"ğŸš€ [{state['user_id']}] í–¥ìƒëœ í‰ê°€ ì‹œì‘ (ì¬ì‹œë„: {state.get('retry_count', 0) + 1})")
        
        # ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        conversation_text = self._build_conversation_text(state["conversation_log"])
        enhanced_results = {}
        
        # ê¸°ì¡´ ê²°ê³¼ì—ì„œ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì¹´í…Œê³ ë¦¬ë§Œ ì¬í‰ê°€
        checklist_results = state.get("checklist_results", {})
        
        for category_id, category_info in self.history_taking_checklist.items():
            existing_result = checklist_results.get(category_id, {})
            
            # ì™„ë£Œìœ¨ì´ ë‚®ì€ ì¹´í…Œê³ ë¦¬ë§Œ ì¬í‰ê°€
            if existing_result.get("completion_rate", 0) < 0.5:
                try:
                    enhanced_result = self._evaluate_checklist_category_enhanced(
                        conversation_text, 
                        category_info, 
                        state["scenario_id"]
                    )
                    enhanced_results[category_id] = enhanced_result
                    print(f"ğŸ”„ [{state['user_id']}] {category_id} ì¬í‰ê°€ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"âŒ [{state['user_id']}] {category_id} ì¬í‰ê°€ ì˜¤ë¥˜: {e}")
                    enhanced_results[category_id] = existing_result
            else:
                enhanced_results[category_id] = existing_result
        
        # ì‹ ë¢°ë„ ì¬ê³„ì‚°
        total_confidence = 0.0
        for result in enhanced_results.values():
            total_confidence += result.get("completion_rate", 0) * result.get("quality_score", 5) / 10
        
        new_confidence = total_confidence / len(enhanced_results)
        
        print(f"âœ… [{state['user_id']}] í–¥ìƒëœ í‰ê°€ ì™„ë£Œ - ì‹ ë¢°ë„: {new_confidence:.2f}")
        
        return {
            **state,
            "checklist_results": enhanced_results,
            "confidence_score": new_confidence,
            "retry_count": state.get("retry_count", 0) + 1,
            "messages": state["messages"] + [HumanMessage(content=f"í–¥ìƒëœ í‰ê°€ ì™„ë£Œ - ì‹ ë¢°ë„: {new_confidence:.2f}")]
        }

    def _calculate_final_scores(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """8ë‹¨ê³„: ìµœì¢… ì ìˆ˜ ê³„ì‚°"""
        print(f"ğŸ§® [{state['user_id']}] ìµœì¢… ì ìˆ˜ ê³„ì‚° ì‹œì‘")
        
        checklist_results = state.get("checklist_results", {})
        question_analysis = state.get("question_analysis", {})
        empathy_analysis = state.get("empathy_analysis", {})
        conversation_analysis = state.get("conversation_analysis", {})
        
        # 1. ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ì ìˆ˜ (60%)
        checklist_score = 0
        total_weight = 0
        
        for category_id, result in checklist_results.items():
            category_weight = self.history_taking_checklist[category_id]["weight"]
            category_score = result.get("completion_rate", 0) * result.get("quality_score", 5)
            checklist_score += category_score * category_weight
            total_weight += category_weight
        
        if total_weight > 0:
            checklist_score = checklist_score / total_weight
        
        # 2. ì§ˆë¬¸ ê¸°ë²• ì ìˆ˜ (25%)
        technique_score = question_analysis.get("technique_score", 5)
        
        # 3. ê³µê° ëŠ¥ë ¥ ì ìˆ˜ (10%)
        empathy_score = empathy_analysis.get("empathy_score", 5)
        
        # 4. ì „ë°˜ì  ì˜ì‚¬ì†Œí†µ ì ìˆ˜ (5%)
        communication_score = min(10, max(1, conversation_analysis.get("question_types", {}).get("open_ratio", 0.3) * 10))
        
        # ìµœì¢… ì ìˆ˜ (100ì  ë§Œì )
        final_score = (
            checklist_score * 0.6 + 
            technique_score * 0.25 + 
            empathy_score * 0.1 +
            communication_score * 0.05
        ) * 10
        
        scores = {
            "total_score": round(final_score, 1),
            "checklist_score": round(checklist_score * 10, 1),
            "technique_score": round(technique_score, 1),
            "empathy_score": round(empathy_score, 1),
            "communication_score": round(communication_score, 1),
            "grade": self._calculate_grade(final_score)
        }
        
        print(f"âœ… [{state['user_id']}] ìµœì¢… ì ìˆ˜ ê³„ì‚° ì™„ë£Œ - ì´ì : {final_score:.1f}")
        
        return {
            **state,
            "final_scores": scores,
            "messages": state["messages"] + [HumanMessage(content=f"ìµœì¢… ì ìˆ˜ ê³„ì‚° ì™„ë£Œ - ì´ì : {final_score:.1f}ì ")]
        }

    def _generate_feedback(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """9ë‹¨ê³„: ìƒì„¸ í”¼ë“œë°± ìƒì„±"""
        print(f"ğŸ“ [{state['user_id']}] í”¼ë“œë°± ìƒì„± ì‹œì‘")
        
        checklist_results = state.get("checklist_results", {})
        question_analysis = state.get("question_analysis", {})
        empathy_analysis = state.get("empathy_analysis", {})
        
        # ê°•ì  ë¶„ì„
        strengths = []
        for category_id, result in checklist_results.items():
            if result.get("completion_rate", 0) > 0.7:
                category_name = self.history_taking_checklist[category_id]["name"]
                strengths.append(f"{category_name} ì˜ì—­ì„ ì˜ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤")
        
        if question_analysis.get("positive_techniques", 0) > question_analysis.get("negative_techniques", 0):
            strengths.append("ì ì ˆí•œ ì§ˆë¬¸ ê¸°ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤")
            
        if empathy_analysis.get("empathy_score", 0) > 7:
            strengths.append("í™˜ìì— ëŒ€í•œ ê³µê°ì  íƒœë„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤")
        
        # ê°œì„ ì  ë¶„ì„  
        improvements = []
        for category_id, result in checklist_results.items():
            if result.get("completion_rate", 0) < 0.5:
                category_name = self.history_taking_checklist[category_id]["name"]
                improvements.append(f"{category_name} ì˜ì—­ì˜ í•„ìˆ˜ ìš”ì†Œë“¤ì„ ë” í™•ì¸í•˜ì„¸ìš”")
        
        if question_analysis.get("negative_techniques", 0) > 0:
            improvements.append("ìœ ë„ ì§ˆë¬¸ë³´ë‹¤ëŠ” ê°œë°©í˜• ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì„¸ìš”")
            
        if empathy_analysis.get("empathy_score", 0) < 6:
            improvements.append("í™˜ìì˜ ê°ì •ì— ë” ê³µê°í•˜ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        
        feedback = {
            "overall_feedback": f"ì „ë°˜ì ì¸ ë³‘ë ¥ì²­ì·¨ ìˆ˜í–‰ë„ì— ëŒ€í•œ í‰ê°€ì…ë‹ˆë‹¤. ì´ì : {state.get('final_scores', {}).get('total_score', 0)}ì ",
            "strengths": strengths[:3],  # ìƒìœ„ 3ê°œ
            "improvements": improvements[:3],  # ìƒìœ„ 3ê°œ  
            "specific_recommendations": [
                "OPQRST êµ¬ì¡°ë¥¼ í™œìš©í•œ ì²´ê³„ì  ë³‘ë ¥ì²­ì·¨ë¥¼ ì—°ìŠµí•˜ì„¸ìš”",
                "í™˜ìì˜ ê°ì •ì— ê³µê°í•˜ëŠ” í‘œí˜„ì„ ë” ì‚¬ìš©í•˜ì„¸ìš”",
                "ì˜í•™ìš©ì–´ë³´ë‹¤ëŠ” í™˜ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”"
            ]
        }
        
        print(f"âœ… [{state['user_id']}] í”¼ë“œë°± ìƒì„± ì™„ë£Œ")
        
        return {
            **state,
            "feedback": feedback,
            "messages": state["messages"] + [HumanMessage(content="ìƒì„¸ í”¼ë“œë°± ìƒì„± ì™„ë£Œ")]
        }

    def _finalize_results(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """10ë‹¨ê³„: ê²°ê³¼ ìµœì¢…í™”"""
        print(f"ğŸ¯ [{state['user_id']}] í‰ê°€ ê²°ê³¼ ìµœì¢…í™”")
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì¡°í™” (ê¸°ì¡´ê³¼ ë™ì¼í•œ í˜•ì‹)
        final_result = {
            "evaluation_metadata": state.get("evaluation_metadata", {}),
            "scores": state.get("final_scores", {}),
            "checklist_results": state.get("checklist_results", {}),
            "question_analysis": state.get("question_analysis", {}),
            "empathy_analysis": state.get("empathy_analysis", {}),  # ìƒˆ í•­ëª©
            "feedback": state.get("feedback", {}),
            "conversation_summary": state.get("conversation_analysis", {}),
            "quality_info": {
                "confidence_score": state.get("confidence_score", 0),
                "retry_count": state.get("retry_count", 0),
                "quality_issues": state.get("quality_issues", [])
            }
        }
        
        print(f"ğŸ‰ [{state['user_id']}] CPX í‰ê°€ ì™„ë£Œ - ì´ì : {final_result['scores'].get('total_score', 0)}ì ")
        
        return {
            **state,
            "final_evaluation_result": final_result,
            "messages": state["messages"] + [HumanMessage(content="CPX í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")]
        }

    # =============================================================================
    # ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜ë“¤
    # =============================================================================
    
    def _should_enhance_evaluation(self, state: CPXEvaluationState) -> str:
        """í‰ê°€ í–¥ìƒì´ í•„ìš”í•œì§€ ê²°ì •"""
        needs_enhancement = state.get("needs_enhancement", False)
        retry_count = state.get("retry_count", 0)
        
        if needs_enhancement and retry_count < 2:
            return "enhance"
        else:
            return "calculate"

    # =============================================================================
    # í—¬í¼ ë©”ì„œë“œë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
    # =============================================================================
    
    def _build_conversation_text(self, conversation_log: List[Dict]) -> str:
        """ëŒ€í™” ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        conversation_parts = []
        for msg in conversation_log:
            speaker = "í•™ìƒ" if msg.get("type") == "student" else "í™˜ì"
            content = msg.get("content", "")
            conversation_parts.append(f"{speaker}: {content}")
        
        return "\n".join(conversation_parts)

    def _evaluate_checklist_category_sync(self, conversation_text: str, category_info: Dict, scenario_id: str) -> Dict:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ (ë™ê¸° ë²„ì „)"""
        
        evaluation_prompt = f"""
ë‹¹ì‹ ì€ ì˜ê³¼ëŒ€í•™ CPX í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë³‘ë ¥ì²­ì·¨ ëŒ€í™”ì—ì„œ "{category_info['name']}" ì¹´í…Œê³ ë¦¬ì˜ í•„ìˆ˜ ìš”ì†Œë“¤ì´ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰ë˜ì—ˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ã€í‰ê°€ ì¹´í…Œê³ ë¦¬ã€‘: {category_info['name']}

ã€í•„ìˆ˜ ìš”ì†Œë“¤ã€‘:
{chr(10).join([f"- {element}" for element in category_info['required_elements']])}

ã€ëŒ€í™” ë‚´ìš©ã€‘:
{conversation_text}

ã€í‰ê°€ ê¸°ì¤€ã€‘:
- ê° í•„ìˆ˜ ìš”ì†Œê°€ ëŒ€í™”ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
- ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆëŠ”ì§€ í‰ê°€
- í™˜ìì™€ì˜ ì†Œí†µì´ íš¨ê³¼ì ì´ì—ˆëŠ”ì§€ íŒë‹¨

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "completed_elements": ["ì™„ë£Œëœ í•„ìˆ˜ ìš”ì†Œë“¤"],
    "missing_elements": ["ëˆ„ë½ëœ í•„ìˆ˜ ìš”ì†Œë“¤"],
    "completion_rate": ì™„ë£Œìœ¨(0.0-1.0),
    "quality_score": ìˆ˜í–‰ í’ˆì§ˆ ì ìˆ˜(1-10),
    "specific_feedback": "êµ¬ì²´ì ì¸ í”¼ë“œë°±"
}}
"""

        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì˜ê³¼ëŒ€í•™ CPX í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                HumanMessage(content=evaluation_prompt)
            ]
            
            response = self.llm(messages)
            result_text = response.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                return {
                    "completed_elements": [],
                    "missing_elements": category_info['required_elements'],
                    "completion_rate": 0.0,
                    "quality_score": 5,
                    "specific_feedback": "í‰ê°€ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ìƒì„¸ ë¶„ì„ì´ ë¶ˆê°€í•©ë‹ˆë‹¤."
                }
                
        except Exception as e:
            print(f"âŒ ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                "completed_elements": [],
                "missing_elements": category_info['required_elements'],
                "completion_rate": 0.0,
                "quality_score": 5,
                "specific_feedback": f"í‰ê°€ ì˜¤ë¥˜: {str(e)}"
            }

    def _evaluate_checklist_category_enhanced(self, conversation_text: str, category_info: Dict, scenario_id: str) -> Dict:
        """í–¥ìƒëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ (ë” ìƒì„¸í•œ ë¶„ì„)"""
        
        enhanced_prompt = f"""
ë‹¹ì‹ ì€ ì˜ê³¼ëŒ€í•™ CPX í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë³‘ë ¥ì²­ì·¨ ëŒ€í™”ì—ì„œ "{category_info['name']}" ì¹´í…Œê³ ë¦¬ë¥¼ ë§¤ìš° ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

ã€í‰ê°€ ì¹´í…Œê³ ë¦¬ã€‘: {category_info['name']}

ã€í•„ìˆ˜ ìš”ì†Œë“¤ã€‘:
{chr(10).join([f"- {element}" for element in category_info['required_elements']])}

ã€ëŒ€í™” ë‚´ìš©ã€‘:
{conversation_text}

ã€í–¥ìƒëœ í‰ê°€ ê¸°ì¤€ã€‘:
- ê° í•„ìˆ˜ ìš”ì†Œì˜ ì§ì ‘ì /ê°„ì ‘ì  í¬í•¨ ì—¬ë¶€ ëª¨ë‘ ê³ ë ¤
- ì˜ë„ëŠ” ìˆì—ˆìœ¼ë‚˜ í‘œí˜„ì´ ë¶€ì¡±í•œ ê²½ìš°ë„ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬
- í™˜ìì˜ ë°˜ì‘ì„ í†µí•´ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ë„ ê³ ë ¤
- ì „ì²´ì ì¸ ë§¥ë½ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ëª©ì  ë‹¬ì„±ë„ í‰ê°€

ë”ìš± ì •í™•í•˜ê³  ê´€ëŒ€í•œ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”:
{{
    "completed_elements": ["ì™„ë£Œëœ í•„ìˆ˜ ìš”ì†Œë“¤"],
    "partially_completed": ["ë¶€ë¶„ì ìœ¼ë¡œ ì™„ë£Œëœ ìš”ì†Œë“¤"],
    "missing_elements": ["ì™„ì „íˆ ëˆ„ë½ëœ í•„ìˆ˜ ìš”ì†Œë“¤"],
    "completion_rate": ì™„ë£Œìœ¨(0.0-1.0),
    "quality_score": ìˆ˜í–‰ í’ˆì§ˆ ì ìˆ˜(1-10),
    "specific_feedback": "ìƒì„¸í•œ í”¼ë“œë°±ê³¼ ê·¼ê±°"
}}
"""

        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ì˜ê³¼ëŒ€í•™ CPX í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë…¸ë ¥ì„ ì¸ì •í•˜ë˜ ì •í™•í•œ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”."),
                HumanMessage(content=enhanced_prompt)
            ]
            
            response = self.llm(messages)
            result_text = response.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                # ë¶€ë¶„ ì™„ë£Œëœ ìš”ì†Œë“¤ì„ ì™„ë£Œëœ ìš”ì†Œì— 0.5 ê°€ì¤‘ì¹˜ë¡œ ì¶”ê°€
                partial_count = len(result.get("partially_completed", []))
                complete_count = len(result.get("completed_elements", []))
                total_elements = len(category_info['required_elements'])
                
                enhanced_completion_rate = min(1.0, (complete_count + partial_count * 0.5) / max(total_elements, 1))
                result["completion_rate"] = enhanced_completion_rate
                
                return result
            else:
                return self._evaluate_checklist_category_sync(conversation_text, category_info, scenario_id)
                
        except Exception as e:
            print(f"âŒ í–¥ìƒëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€ ì˜¤ë¥˜: {e}")
            return self._evaluate_checklist_category_sync(conversation_text, category_info, scenario_id)

    def _is_open_question(self, question: str) -> bool:
        """ê°œë°©í˜• ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨"""
        open_keywords = ["ì–´ë–¤", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ë¬´ì—‡", "ì–´ëŠ", "ì„¤ëª…", "ì–˜ê¸°"]
        return any(keyword in question for keyword in open_keywords)
    
    def _is_empathetic_question(self, question: str) -> bool:
        """ê³µê°ì  ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨"""
        empathy_keywords = ["í˜ë“œì‹œê² ì–´ìš”", "ê±±ì •", "ì´í•´", "ë¶ˆí¸", "ê´œì°®ìœ¼ì‹œ", "í¸í•˜ê²Œ"]
        return any(keyword in question for keyword in empathy_keywords)
    
    def _is_leading_question(self, question: str) -> bool:
        """ìœ ë„ ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨"""
        leading_patterns = ["~ì•„ë‹ˆì—ìš”?", "~ì£ ?", "~ê²ƒ ê°™ì€ë°", "~ì¼ê¹Œìš”?"]
        return any(pattern in question for pattern in leading_patterns)

    def _calculate_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ê³„ì‚°"""
        if score >= 90:
            return "A+"
        elif score >= 85:
            return "A"
        elif score >= 80:
            return "B+"
        elif score >= 75:
            return "B"
        elif score >= 70:
            return "C+"
        elif score >= 65:
            return "C"
        else:
            return "F"

    # =============================================================================
    # ê³µê°œ ë©”ì„œë“œë“¤
    # =============================================================================
    
    async def evaluate_conversation(self, user_id: str, scenario_id: str, conversation_log: List[Dict]) -> Dict:
        """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•œ CPX í‰ê°€ ì‹¤í–‰"""
        
        # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
        initial_state = CPXEvaluationState(
            user_id=user_id,
            scenario_id=scenario_id,
            conversation_log=conversation_log,
            conversation_analysis=None,
            checklist_results=None,
            question_analysis=None,
            empathy_analysis=None,
            confidence_score=0.0,
            retry_count=0,
            needs_enhancement=False,
            final_scores=None,
            feedback=None,
            evaluation_metadata=None,
            messages=[]
        )
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            print(f"ğŸš€ [{user_id}] LangGraph CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
            final_state = self.workflow.invoke(initial_state)
            
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜
            result = final_state.get("final_evaluation_result", {})
            print(f"ğŸ‰ [{user_id}] LangGraph CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            print(f"âŒ [{user_id}] LangGraph í‰ê°€ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "user_id": user_id,
                "scenario_id": scenario_id,
                "evaluation_date": datetime.now().isoformat()
            }

    def get_evaluation_summary(self, user_id: str) -> Dict:
        """ì‚¬ìš©ìì˜ í‰ê°€ ìš”ì•½ ì •ë³´"""
        return {
            "user_id": user_id,
            "total_evaluations": 0,
            "average_score": 0.0,
            "recent_evaluations": []
        }