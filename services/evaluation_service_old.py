from typing import Dict, List, Optional, TypedDict, Annotated
from datetime import datetime
from pathlib import Path
import json
import asyncio
import aiofiles
import logging
import os
import sys
import re
from collections import Counter

# LangGraph ê´€ë ¨ import
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage as AnyMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# CPX ê´€ë ¨ import
from services.cpx_service import CpxService
from core.config import get_db

# RAG ê°€ì´ë“œë¼ì¸ import
from RAG.guideline_retriever import GuidelineRetriever

# CPX í‰ê°€ ìƒíƒœ ì •ì˜ (Multi-Step Reasoning ì „ìš©)
class CPXEvaluationState(TypedDict):
    """CPX í‰ê°€ ìƒíƒœ ì •ì˜ - Multi-Step Reasoning ì „ìš©"""
    # ì…ë ¥ ë°ì´í„°
    user_id: str
    scenario_id: str
    conversation_log: List[Dict]
    
    # Multi-Step Reasoning ê²°ê³¼ë“¤ (í•µì‹¬)
    medical_context_analysis: Optional[Dict]
    question_intent_analysis: Optional[Dict]
    completeness_assessment: Optional[Dict]
    quality_evaluation: Optional[Dict]
    appropriateness_validation: Optional[Dict]
    
    # ì¢…í•© í‰ê°€ ê²°ê³¼
    comprehensive_evaluation: Optional[Dict]
    
    # ìµœì¢… ê²°ê³¼
    final_scores: Optional[Dict]
    feedback: Optional[Dict]
    
    # ë©”íƒ€ë°ì´í„°
    evaluation_metadata: Optional[Dict]
    
    # ë©”ì‹œì§€ ì¶”ì 
    messages: Annotated[List[AnyMessage], add_messages]

class EvaluationService:
    def __init__(self):
        """CPX í‰ê°€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ëŠ” ì œê±°í•˜ê³  JSON ê¸°ë°˜ìœ¼ë¡œ í†µí•©
        
        self.session_data = {}  # ì„¸ì…˜ë³„ í‰ê°€ ë°ì´í„°
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬
        self.evaluation_dir = Path("evaluation_results")
        self.evaluation_dir.mkdir(parents=True, exist_ok=True)
        
        # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê´€ë ¨
        self.llm = None
        self.workflow = None
        self._initialize_langgraph_components()
        
        # RAG ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.guideline_retriever = None
        self._initialize_guideline_retriever()
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì ìš© ìš”ì†Œë“¤ ì •ì˜
        self.scenario_applicable_elements = self._initialize_scenario_elements()

    def _initialize_guideline_retriever(self):
        """RAG ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        try:
            # ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
            rag_path = Path(__file__).parent.parent / "RAG"
            index_path = rag_path / "faiss_guideline_index"
            self.guideline_retriever = GuidelineRetriever(index_path=str(index_path))
            
            if self.guideline_retriever.vectorstore:
                print("âœ… RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸ RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
                self.guideline_retriever = None
                
        except Exception as e:
            print(f"âŒ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.guideline_retriever = None

    def _initialize_scenario_elements(self) -> Dict:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì ìš© ìš”ì†Œë“¤ ì´ˆê¸°í™” - scenarios/ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ"""
        scenario_elements = {}
        scenario_dir = Path("scenarios")
        
        if not scenario_dir.exists():
            print("âš ï¸ scenarios ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        for json_file in scenario_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                scenario_info = data.get("scenario_info", {})
                category = scenario_info.get("category", "unknown")
                
                # ì‹œë‚˜ë¦¬ì˜¤ IDë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì˜ˆ: memory_impairment)
                scenario_id = category.replace(" ", "_").lower()
                
                scenario_elements[scenario_id] = {
                    "name": category,
                    "description": scenario_info.get("case_presentation", ""),
                    "patient_name": scenario_info.get("patient_name", ""),
                    "primary_diagnosis": scenario_info.get("primary_diagnosis", ""),
                    "differential_diagnoses": scenario_info.get("differential_diagnoses", []),
                    "applicable_areas": [
                        "history_taking",
                        "physical_examination", 
                        "patient_education"
                    ]
                }
                
                print(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ: {category} ({json_file.name})")
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
            except Exception as e:
                print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
        
        return scenario_elements

    def _get_scenario_category(self, scenario_id: str) -> Optional[str]:
        """ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë“œ"""
        try:
            scenario_path = Path(f"scenarios/neurology_dementia_case.json")  # í˜„ì¬ëŠ” í•˜ë‚˜ì˜ ì‹œë‚˜ë¦¬ì˜¤ë§Œ
            if not scenario_path.exists():
                return None
            
            with open(scenario_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("scenario_info", {}).get("category")
        except Exception as e:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _parse_structured_sections(self, document) -> dict:
        """ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì„¹ì…˜ íŒŒì‹±"""
        import re
        
        structured_sections = {}
        
        # ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        if hasattr(document, 'page_content'):
            document_text = document.page_content
        elif isinstance(document, dict):
            document_text = document.get('content', '') or document.get('page_content', '') or str(document)
        else:
            document_text = str(document)
        
        # ì„¹ì…˜ íŒ¨í„´: ã€ì„¹ì…˜ëª…ã€‘
        section_pattern = re.compile(r'ã€([^ã€‘]+)ã€‘')
        # í•­ëª© íŒ¨í„´: â€¢ ë˜ëŠ” - ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
        bullet_pattern = re.compile(r'^\s*[â€¢\-\*]\s+(.+)$', re.MULTILINE)
        
        sections = section_pattern.split(document_text)
        
        for i in range(1, len(sections), 2):
            if i + 1 < len(sections):
                section_name = sections[i].strip()
                section_content = sections[i + 1]
                
                # ì„¹ì…˜ ë‚´ìš©ì—ì„œ í•„ìˆ˜ í•­ëª© ì¶”ì¶œ
                required_items = bullet_pattern.findall(section_content)
                if required_items:
                    structured_sections[section_name] = required_items
        
        return structured_sections

    def _build_full_conversation_text(self, conversation_log: list) -> str:
        """ì „ì²´ ëŒ€í™”ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        
        chunks = []
        current_chunk = []
        max_chunk_size = 20  # ìµœëŒ€ ì²­í¬ í¬ê¸°
        
        i = 0
        while i < len(conversation_log):
            msg = conversation_log[i]
            current_chunk.append(msg)
            role = msg.get("role", "")
            
            # ì²­í¬ í¬ê¸°ê°€ ìµœëŒ€ì¹˜ì— ë„ë‹¬í–ˆì„ ë•Œ
            if len(current_chunk) >= max_chunk_size:
                # í˜„ì¬ ë©”ì‹œì§€ê°€ ì˜ì‚¬ ì§ˆë¬¸ì´ë©´ í™˜ì ë‹µë³€ê¹Œì§€ ê¸°ë‹¤ë¦¼
                if role == "student":
                    # ë‹¤ìŒ í™˜ì ë‹µë³€ì„ ì°¾ì•„ì„œ í¬í•¨ì‹œí‚´
                    while i + 1 < len(conversation_log):
                        i += 1
                        next_msg = conversation_log[i]
                        current_chunk.append(next_msg)
                        if next_msg.get("role") == "patient":
                            break
                
                # í˜„ì¬ê°€ í™˜ì ë‹µë³€ì´ë©´, ë‹¤ìŒì´ ë˜ í™˜ì ë‹µë³€ì¸ì§€ í™•ì¸
                elif role == "patient":
                    # ì—°ì†ëœ í™˜ì ë‹µë³€ë“¤ì„ ëª¨ë‘ í¬í•¨
                    while i + 1 < len(conversation_log):
                        next_msg = conversation_log[i + 1]
                        if next_msg.get("role") == "patient":
                            i += 1
                            current_chunk.append(next_msg)
                        else:
                            break
                
                # ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ì €ì¥
                chunk_text = ""
                for msg in current_chunk:
                    content = msg.get("content") or msg.get("text", "")
                    role = msg.get("role") or msg.get("speaker_role", "")
                    
                    if not content:
                        raise ValueError(f"ë©”ì‹œì§€ì— contentê°€ ì—†ìŠµë‹ˆë‹¤: {msg}")
                    
                    if not role:
                        raise ValueError(f"ë©”ì‹œì§€ì— roleì´ ì—†ìŠµë‹ˆë‹¤: {msg}")
                        
                    if role == "student":
                        speaker = "ì˜ì‚¬"
                    elif role == "patient":
                        speaker = "í™˜ì"
                    else:
                        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” roleì…ë‹ˆë‹¤: {role}. í—ˆìš©ë˜ëŠ” role: student, patient")
                        
                    chunk_text += f"{speaker}: {content}\n"
                
                chunks.append(chunk_text)
                current_chunk = []
            
            i += 1
        
        # ë‚¨ì€ ë©”ì‹œì§€ë“¤ ì²˜ë¦¬
        if current_chunk:
            chunk_text = ""
            for msg in current_chunk:
                content = msg.get("content") or msg.get("text", "")
                role = msg.get("role") or msg.get("speaker_role", "")
                
                if not content:
                    raise ValueError(f"ë©”ì‹œì§€ì— contentê°€ ì—†ìŠµë‹ˆë‹¤: {msg}")
                
                if not role:
                    raise ValueError(f"ë©”ì‹œì§€ì— roleì´ ì—†ìŠµë‹ˆë‹¤: {msg}")
                    
                if role == "student":
                    speaker = "ì˜ì‚¬"
                elif role == "patient":
                    speaker = "í™˜ì"
                else:
                    raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” roleì…ë‹ˆë‹¤: {role}. í—ˆìš©ë˜ëŠ” role: student, patient")
                    
                chunk_text += f"{speaker}: {content}\n"
            
            chunks.append(chunk_text)
        
        return chunks

    def check_all_guidelines_in_chunk(self, chunk_text: str, structured_sections: dict, area_name: str) -> dict:
        """ì²­í¬ì—ì„œ ëª¨ë“  ê°€ì´ë“œë¼ì¸ ì„¹ì…˜ì„ í•œë²ˆì— ì²´í¬ (íš¨ìœ¨ì )"""
        
        if not chunk_text.strip():
            return {section_name: {"found_evidence": False} for section_name in structured_sections.keys()}
        
        # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì²´í¬
        sections_text = ""
        for section_name, questions in structured_sections.items():
            sections_text += f"\nã€{section_name}ã€‘\n"
            for q in questions:
                sections_text += f"  â€¢ {q}\n"
        
        # ê³ ë„í™”ëœ í‰ê°€ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ CPX(Clinical Performance Examination) í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ê³¼ëŒ€í•™ìƒì´ í‘œì¤€í™” í™˜ìì™€ ë‚˜ëˆˆ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ê° ê°€ì´ë“œë¼ì¸ í•­ëª©ì´ ì ì ˆíˆ ë‹¤ë¤„ì¡ŒëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

=== í‰ê°€ ëŒ€ìƒ ëŒ€í™” ===
{chunk_text}

=== {area_name} í‰ê°€ ê°€ì´ë“œë¼ì¸ ===
{sections_text}

=== ì˜ì—­ë³„ í‰ê°€ ì›ì¹™ ===
{self._get_area_specific_guidelines(area_name)}

## í‰ê°€ ë°©ë²•
1. **ê° ê°€ì´ë“œë¼ì¸ í•­ëª©ë³„ë¡œ** í•´ë‹¹ í•­ëª©ì˜ required_questions/actionsì™€ **ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ** ì˜ì‚¬ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
2. **1ìˆœìœ„**: ì˜ì‚¬ê°€ í•´ë‹¹ í•­ëª©ì˜ required_questionsì™€ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ í–ˆìœ¼ë©´ "found: true"
3. **2ìˆœìœ„**: ì˜ì‚¬ ì§ˆë¬¸ì´ ì „í˜€ ì—†ì§€ë§Œ í™˜ìê°€ ê´€ë ¨ ì •ë³´ë¥¼ ìë°œì ìœ¼ë¡œ ì–¸ê¸‰í–ˆìœ¼ë©´ "found: true"
4. **ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œì˜ ì§ˆë¬¸ì´ê±°ë‚˜ ì „í˜€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìœ¼ë©´** "found: false"

## Evidence ìˆ˜ì§‘ ê·œì¹™
 **í•´ë‹¹ í•­ëª©ì˜ required_questions/actionsì™€ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ì§ˆë¬¸ë§Œ evidenceë¡œ ì‚¬ìš©**
- **1ìˆœìœ„**: í•´ë‹¹ í•­ëª©ì˜ required_questionsì™€ ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ "ì˜ì‚¬:" ì§ˆë¬¸ ë¬¸ì¥
- **2ìˆœìœ„**: ì˜ì‚¬ê°€ í•´ë‹¹ í•­ëª©ì— ëŒ€í•´ ì „í˜€ ì§ˆë¬¸í•˜ì§€ ì•Šì•˜ì§€ë§Œ í™˜ìê°€ ê´€ë ¨ ì •ë³´ë¥¼ ì–¸ê¸‰í•œ "í™˜ì:" ë¬¸ì¥
- **ì£¼ì˜**: ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œì˜ ì˜ì‚¬ ì§ˆë¬¸ì€ evidenceê°€ ë  ìˆ˜ ì—†ìŒ
- **ì˜ˆì‹œ**: "ê³¼ê±°ë ¥" í•­ëª©ì—ì„œ "ì˜ì‚¬: ê°€ì¡± ì¤‘ì— ì¹˜ë§¤ í™˜ìê°€ ìˆìœ¼ì„¸ìš”?" â†’ ì´ê±´ ê°€ì¡±ë ¥ ì§ˆë¬¸ì´ë¯€ë¡œ ê³¼ê±°ë ¥ evidenceê°€ ë  ìˆ˜ ì—†ìŒ

## ìœ ì—°í•œ íŒë‹¨ ê¸°ì¤€
- í‘œí˜„ì´ ë‹¬ë¼ë„ ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ì¸ì •
- ì§ì ‘ì  ì§ˆë¬¸ì´ ì•„ë‹ˆì–´ë„ ê´€ë ¨ ì •ë³´ë¥¼ ì–»ìœ¼ë ¤ëŠ” ì˜ë„ê°€ ëª…í™•í•˜ë©´ ì¸ì •
- í•œ ë²ˆì˜ ì§ˆë¬¸ìœ¼ë¡œ ì—¬ëŸ¬ í•­ëª©ì„ ì»¤ë²„í•  ìˆ˜ ìˆìŒ

ê° í•­ëª©ë³„ë¡œ ì •í™•íˆ í‰ê°€í•˜ì—¬ ë‹µë³€:
{{
{', '.join([f'    "{section_name}": {{"found": true/false, "evidence": "í•´ë‹¹ í•­ëª©ì˜ required_questionsì™€ ì¼ì¹˜í•˜ëŠ” ì˜ì‚¬ ì§ˆë¬¸ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´"}}' for section_name in structured_sections.keys()])}
}}"""
        
        print("=" * 80)
        print("ğŸ“‹ LLM í”„ë¡¬í”„íŠ¸:")
        print("=" * 80)
        print(prompt)
        print("=" * 80)
        
        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm.invoke(messages)
            result_text = response.content
            
            # JSON ì¶”ì¶œ
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                converted_result = {}
                for section_name in structured_sections.keys():
                    section_data = result.get(section_name, {})
                    converted_result[section_name] = {
                        "found_evidence": section_data.get("found", False),
                        "evidence": section_data.get("evidence", ""),
                        "answered_questions": [section_name] if section_data.get("found", False) else []
                    }
                
                return converted_result
            else:
                print(f"âŒ JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_text[:100]}")
                return {section_name: {"found_evidence": False} for section_name in structured_sections.keys()}
                
        except Exception as e:
            print(f"âŒ ì²­í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {section_name: {"found_evidence": False} for section_name in structured_sections.keys()}

    def _get_area_specific_guidelines(self, area_name: str) -> str:
        """ì˜ì—­ë³„ íŠ¹í™”ëœ í‰ê°€ ì›ì¹™ ë°˜í™˜"""
        
        if area_name == "ë³‘ë ¥ ì²­ì·¨":
            return """
### ë³‘ë ¥ ì²­ì·¨ í‰ê°€ ì›ì¹™
- ê° ì„¹ì…˜ì˜ **í•µì‹¬ ì£¼ì œ**ì— ëŒ€í•´ ì˜ì‚¬ê°€ ì ì ˆíˆ **ì§ˆë¬¸**í–ˆëŠ”ì§€ í‰ê°€
- **ëª¨ë“  ê°œë³„ ì§ˆë¬¸ì„ ë‹¤ ë¬¼ì–´ë³¼ í•„ìš” ì—†ìŒ** - ë¹„ìŠ·í•œ ì§ˆë¬¸ë“¤ì€ í•˜ë‚˜ë¡œ ì»¤ë²„ ê°€ëŠ¥
- **í™˜ìë¡œë¶€í„° í•´ë‹¹ ì •ë³´ë¥¼ ì–»ì—ˆë‹¤ë©´ ì™„ë£Œë¡œ ì¸ì •** (ì§ˆë¬¸ ë°©ì‹ì´ë‚˜ í‘œí˜„ ë¬´ê´€)
- **ê´€ë ¨ëœ ì£¼ì œë¥¼ ë‹¤ë¤˜ë‹¤ë©´ í•´ë‹¹ ì„¹ì…˜ìœ¼ë¡œ ì¸ì •** (ê°„ì ‘ì  ì§ˆë¬¸ë„ í¬í•¨)
- í™˜ìê°€ ë¨¼ì € ì •ë³´ë¥¼ ì œê³µí•´ë„ ì˜ì‚¬ê°€ í™•ì¸í–ˆë‹¤ë©´ ì™„ë£Œë¡œ ì¸ì •

### ì„¹ì…˜ë³„ ìœ ì—°í•œ í•´ì„ ê¸°ì¤€
- **ê° ì„¹ì…˜ì˜ ê°€ì´ë“œë¼ì¸ ì§ˆë¬¸ê³¼ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë‹¤ë¤˜ëŠ”ì§€ íŒë‹¨**
- **í‘œí˜„ì´ ë‹¬ë¼ë„ ê°™ì€ ì •ë³´ë¥¼ ì–»ìœ¼ë ¤ëŠ” ì˜ë„ë©´ í•´ë‹¹ ì„¹ì…˜ìœ¼ë¡œ ì¸ì •**
- **í™˜ì ì‘ë‹µì„ í†µí•´ í•´ë‹¹ ì„¹ì…˜ì˜ ëª©ì ì— ë§ëŠ” ì •ë³´ë¥¼ í™•ì¸í–ˆë‹¤ë©´ ì™„ë£Œ**
- **ì§ì ‘ì  ì§ˆë¬¸ì´ ì•„ë‹ˆì–´ë„ ê´€ë ¨ ì •ë³´ë¥¼ ì–»ì—ˆë‹¤ë©´ ì¸ì •**"""
            
        elif area_name == "ì‹ ì²´ ì§„ì°°":
            return """
### ì‹ ì²´ ì§„ì°° í‰ê°€ ì›ì¹™
- **ê°€ì´ë“œë¼ì¸ì— ëª…ì‹œëœ ê²€ì‚¬ëª…ì„ ì–¸ê¸‰í–ˆëŠ”ì§€ í™•ì¸**
- **"â—‹â—‹ ê²€ì‚¬ë¥¼ ì‹œí–‰í•˜ê² ìŠµë‹ˆë‹¤" í˜•íƒœë¡œ ì–¸ê¸‰í•˜ë©´ ì™„ë£Œë¡œ ì¸ì •**
- **í˜„ì¬ ì‹œì ì—ì„œ ì‹¤ì‹œí•˜ëŠ” ê²€ì‚¬ì™€ ë¯¸ë˜ ê³„íšì€ êµ¬ë¶„**
- **í™˜ìêµìœ¡ì—ì„œ "ë‚˜ì¤‘ì— í•„ìš”í•œ ê²€ì‚¬" ì–¸ê¸‰ì€ ì‹ ì²´ì§„ì°°ê³¼ ë³„ê°œ**

### ì™„ë£Œ ì¡°ê±´ (í˜„ì‹¤ì )
- ã€ì§„ì°° ì¤€ë¹„ã€‘: í™˜ìì—ê²Œ ì§„ì°° ì‹œì‘ ì•ˆë‚´
- ã€ê²€ì‚¬ ìˆ˜í–‰ã€‘: ê°€ì´ë“œë¼ì¸ì˜ êµ¬ì²´ì  ê²€ì‚¬ëª… ì–¸ê¸‰ (ì§ˆí™˜ë³„ë¡œ ë‹¤ë¦„)
- **"ì§€ê¸ˆ â—‹â—‹ê²€ì‚¬ë¥¼ í•˜ê² ìŠµë‹ˆë‹¤" = ì™„ë£Œ**, **"ì¶”í›„ â—‹â—‹ê²€ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤" = í™˜ìêµìœ¡**"""
            
        elif area_name == "í™˜ì êµìœ¡":
            return """
### í™˜ì êµìœ¡ í‰ê°€ ì›ì¹™
- ì˜ì‚¬ê°€ í™˜ìì—ê²Œ **ì„¤ëª…, ì•ˆë‚´, êµìœ¡**ì„ ì œê³µí–ˆëŠ”ì§€ í‰ê°€
- **ê³µê°**: í™˜ìì˜ ê±±ì •ì´ë‚˜ ê°ì •ì— ëŒ€í•œ ì´í•´ í‘œí˜„
- **ì¶”ì • ì§„ë‹¨**: êµ¬ì²´ì  ì§ˆí™˜ëª…ì„ ì–¸ê¸‰í•œ ì§„ë‹¨ ì œì‹œ
- **ê²€ì‚¬ ê³„íš**: êµ¬ì²´ì  ê²€ì‚¬ëª…ì´ë‚˜ ê²€ì‚¬ ë°©ë²• ì–¸ê¸‰
- **ì¹˜ë£Œ ê³„íš**: í–¥í›„ ì¹˜ë£Œ ë°©í–¥ì´ë‚˜ ê´€ë¦¬ ë°©ë²• ì•ˆë‚´

### êµ¬ì²´ì  ì˜ˆì‹œ
- ã€ê³µê°ã€‘: "ë§ì´ ê±±ì •ë˜ì…¨ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤" â†’ ì™„ë£Œ
- ã€ì¶”ì • ì§„ë‹¨ã€‘: "â—‹â—‹ì§ˆí™˜ ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤" â†’ ì™„ë£Œ
- ã€ê²€ì‚¬ ê³„íšã€‘: "â—‹â—‹ ê²€ì‚¬ë¥¼ í†µí•´ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤" â†’ ì™„ë£Œ
- ã€ì¹˜ë£Œ ê³„íšã€‘: "â—‹â—‹ ì¹˜ë£Œë¥¼ ì‹œì‘í•˜ê³  ì •ê¸°ì ìœ¼ë¡œ ê²½ê³¼ë¥¼ ë³´ê² ìŠµë‹ˆë‹¤" â†’ ì™„ë£Œ"""
            
        else:
            return """
### ì¼ë°˜ í‰ê°€ ì›ì¹™
- ê° ì„¹ì…˜ì˜ í•µì‹¬ ì£¼ì œê°€ ì ì ˆíˆ ë‹¤ë¤„ì¡ŒëŠ”ì§€ í‰ê°€
- í‘œí˜„ì´ ë‹¬ë¼ë„ ì˜ë¯¸ìƒ ë™ì¼í•œ ë‚´ìš©ì„ í™•ì¸í–ˆë‹¤ë©´ ì™„ë£Œë¡œ íŒë‹¨"""

    def evaluate_area_with_chunks(self, conversation_log: list, area_name: str, structured_sections: dict) -> dict:
        """ì²­í¬ ê¸°ë°˜ìœ¼ë¡œ ì˜ì—­ í‰ê°€ - ì¦‰ì‹œ ì²´í¬ ë°©ì‹"""
        
        # 1. ëŒ€í™”ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        chunk_texts = self.split_and_build_chunks(conversation_log)
        
        # 2. ê°€ì´ë“œë¼ì¸ í•­ëª©ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        guideline_checklist = {}
        for section_name, questions in structured_sections.items():
            guideline_checklist[section_name] = {
                "completed": False,
                "evidence": ""
            }
        
        # 3. ê° ì²­í¬ì—ì„œ ëª¨ë“  ê°€ì´ë“œë¼ì¸ì„ í•œë²ˆì— ì²´í¬
        for i, chunk_text in enumerate(chunk_texts):
            chunk_result = self.check_all_guidelines_in_chunk(chunk_text, structured_sections, area_name)
            
            # ê²°ê³¼ë¥¼ ê° ê°€ì´ë“œë¼ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ë°˜ì˜
            for section_name, section_result in chunk_result.items():
                if section_name in guideline_checklist and not guideline_checklist[section_name]["completed"]:
                    checklist = guideline_checklist[section_name]
                    
                    if section_result.get("found_evidence"):
                        checklist["completed"] = True
                        if not checklist["evidence"]:  # ì²« ë²ˆì§¸ evidenceë§Œ ì €ì¥
                            evidence = section_result.get("evidence", "")
                            if evidence:
                                checklist["evidence"] = evidence
        
        # 4. ë‹¨ìˆœí•œ ê²°ê³¼ ìƒì„±
        guideline_evaluations = []
        completed_count = 0
        
        for section_name, checklist in guideline_checklist.items():
            if checklist["completed"]:
                completed_count += 1
            
            guideline_evaluations.append({
                "guideline_item": section_name,
                "completed": checklist["completed"],
                "evidence": checklist["evidence"]
            })
        
        # 5. ì™„ì„±ë„ ê³„ì‚°
        total_count = len(guideline_checklist)
        completion_rate = completed_count / total_count if total_count > 0 else 0
        

        
        return {
            "area_name": area_name,
            "completion_rate": f"{completion_rate:.1%}",
            "guideline_evaluations": guideline_evaluations,
            "total_guidelines": total_count,
            "completed_guidelines": completed_count
        }

    async def start_evaluation_session(self, user_id: str, scenario_id: str, result_id: Optional[int] = None) -> str:
        """í‰ê°€ ì„¸ì…˜ ì‹œì‘"""
        session_id = f"{user_id}_{scenario_id}_{int(datetime.now().timestamp())}"
        
        self.session_data[session_id] = {
            "user_id": user_id,
            "scenario_id": scenario_id,
            "result_id": result_id,  # CPX result_id ì €ì¥
            "start_time": datetime.now(),
            "conversation_entries": [],  # ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„°
            # "audio_files": [],  # ì„ì‹œ ì €ì¥ëœ wav íŒŒì¼ ê²½ë¡œë“¤
            "status": "active"
        }
        
        return session_id

    async def add_conversation_entry(self, session_id: str, audio_file_path: str, 
                                   text: str, speaker_role: str, emotion_analysis: Optional[Dict] = None) -> Dict:
        """ì‹¤ì‹œê°„ ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€ (SER ê²°ê³¼ëŠ” queueì—ì„œ ì „ë‹¬ë°›ìŒ)"""
        if session_id not in self.session_data:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            timestamp = datetime.now()
            
            # SER ê²°ê³¼ ë¡œê¹… (queueì—ì„œ ì „ë‹¬ë°›ì€ ê²½ìš°)
            if emotion_analysis:
                print(f"ğŸ­ [{session_id}] ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹ : {emotion_analysis['predicted_emotion']} ({emotion_analysis['confidence']:.2f})")
            
            # ëŒ€í™” ì—”íŠ¸ë¦¬ ìƒì„±
            conversation_entry = {
                "timestamp": timestamp.isoformat(),
                "text": text,
                "emotion": emotion_analysis,
                "speaker_role": speaker_role,  # "student" (ì˜ì‚¬) or "patient" (í™˜ì)
                "audio_file_path": audio_file_path
            }
            
            # ì„¸ì…˜ ë°ì´í„°ì— ì¶”ê°€
            session = self.session_data[session_id]
            session["conversation_entries"].append(conversation_entry)
            if "audio_files" not in session:
                session["audio_files"] = []
            session["audio_files"].append(audio_file_path)
            
            print(f"ğŸ“ [{session_id}] ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€: {speaker_role} - {text[:50]}...")
            
            # í‰ê°€ ì™„ë£Œ í›„ ì„ì‹œ WAV íŒŒì¼ë“¤ ì‚­ì œ
            try:
                await self._cleanup_audio_files(audio_file_path)
            except Exception as e:
                print(f"âŒ [{audio_file_path}] ì„ì‹œ WAV íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "entry": conversation_entry,
                "total_entries": len(session["conversation_entries"])
            }
            
        except Exception as e:
            print(f"âŒ [{session_id}] ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def end_evaluation_session(self, session_id: str) -> Dict:
        """í‰ê°€ ì„¸ì…˜ ì¢…ë£Œ ë° ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        if session_id not in self.session_data:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        session = self.session_data[session_id]
        session["end_time"] = datetime.now()
        session["status"] = "completed"
        
        # ì¢…í•© í‰ê°€ ì‹¤í–‰
        evaluation_result = await self._comprehensive_evaluation(session_id, session)
        
        # CPX ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        await self._update_cpx_database_after_evaluation(session_id, evaluation_result)
        
        return evaluation_result

    def get_session_summary(self, user_id: str) -> list:
        """ì‚¬ìš©ìì˜ ì„¸ì…˜ ìš”ì•½"""
        return [
            {
                "session_id": sid,
                "scenario_id": data["scenario_id"],
                "date": data["start_time"].strftime("%Y-%m-%d %H:%M"),
                "status": data["status"]
            }
            for sid, data in self.session_data.items()
            if data["user_id"] == user_id
        ]

    async def _comprehensive_evaluation(self, session_id: str, session: Dict) -> Dict:
        """ì¢…í•©ì ì¸ ì„¸ì…˜ í‰ê°€ ìˆ˜í–‰ (SER + LangGraph í†µí•©)"""
        print(f"ğŸ” [{session_id}] ì¢…í•© í‰ê°€ ì‹œì‘...")
        
        # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ (ìƒˆë¡œìš´ ëŒ€í™” ë°ì´í„° ì‚¬ìš©)
        langgraph_analysis = None
        if self.llm and self.workflow:
            try:
                # ìƒˆë¡œìš´ conversation_entriesë¥¼ conversation_log í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                conversation_log = []
                for entry in session.get("conversation_entries", []):
                    conversation_log.append({
                        "role": entry["speaker_role"],
                        "content": entry["text"],
                        "timestamp": entry["timestamp"],
                        "emotion": entry.get("emotion")
                    })
                
                if conversation_log:  # ëŒ€í™” ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í‰ê°€
                    langgraph_analysis = await self.evaluate_conversation_with_langgraph(
                        session["user_id"], 
                        session["scenario_id"], 
                        conversation_log
                    )
                    print(f"âœ… [{session_id}] LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ [{session_id}] ëŒ€í™” ë°ì´í„°ê°€ ì—†ì–´ LangGraph í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                
            except Exception as e:
                print(f"âŒ [{session_id}] LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
                langgraph_analysis = {"error": str(e)}
        
        # ì¢…í•© ê²°ê³¼ êµ¬ì„±
        evaluation_result = {
            "session_id": session_id,
            "user_id": session["user_id"],
            "scenario_id": session["scenario_id"],
            "start_time": session["start_time"].isoformat(),
            "end_time": session["end_time"].isoformat(),
            "duration_minutes": (session["end_time"] - session["start_time"]).total_seconds() / 60,
            
            # ìƒì„¸ ë¶„ì„ ê²°ê³¼
            "langgraph_text_analysis": langgraph_analysis,  # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼
            
            # ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„° (ê°ì • ë¶„ì„ í¬í•¨)
            "conversation_entries": [
                {
                    "timestamp": entry["timestamp"],
                    "text": entry["text"],
                    "speaker_role": entry["speaker_role"],
                    "emotion": entry.get("emotion"),
                    "audio_file": entry["audio_file_path"]
                }
                for entry in session.get("conversation_entries", [])
            ]
        }
        
        print(f"âœ… [{session_id}] ì¢…í•© í‰ê°€ ì™„ë£Œ")
        return evaluation_result

    async def _save_evaluation_result(self, session_id: str, result: Dict):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # JSON íŒŒì¼ë¡œ ì €ì¥
            json_path = self.evaluation_dir / f"{session_id}_evaluation.json"
            
            async with aiofiles.open(json_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(result, ensure_ascii=False, indent=2))
            
            print(f"ğŸ’¾ [{session_id}] í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
            
        except Exception as e:
            print(f"âŒ [{session_id}] í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def get_evaluation_result(self, session_id: str) -> Dict:
        """ì €ì¥ëœ í‰ê°€ ê²°ê³¼ ì¡°íšŒ"""
        json_path = self.evaluation_dir / f"{session_id}_evaluation.json"
        
        if not json_path.exists():
            return {"error": "í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            async with aiofiles.open(json_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                return json.loads(content)
        except Exception as e:
            return {"error": f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}"}

    # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê¸°ëŠ¥
    
    def _initialize_langgraph_components(self):
        """LangGraph ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # OpenAI API ì„¤ì •
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.llm = ChatOpenAI(
                    openai_api_key=api_key,
                    model_name="gpt-3.5-turbo",
                    temperature=0.3,
                    max_tokens=4000
                )
                
                # ì›Œí¬í”Œë¡œìš° ìƒì„±
                self.workflow = self._create_evaluation_workflow()
                print("âœ… LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ í‰ê°€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            print(f"âŒ LangGraph ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm = None
            self.workflow = None



    async def evaluate_conversation(self, user_id: str, scenario_id: str, conversation_log: List[Dict]) -> Dict:
        """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•œ CPX í‰ê°€ ì‹¤í–‰"""
        # ì´ˆê¸° ìƒíƒœ êµ¬ì„± (Multi-Step ì „ìš©)
        initial_state = CPXEvaluationState(
            user_id=user_id,
            scenario_id=scenario_id,
            conversation_log=conversation_log,
            medical_context_analysis=None,
            question_intent_analysis=None,
            completeness_assessment=None,
            quality_evaluation=None,
            appropriateness_validation=None,
            comprehensive_evaluation=None,
            final_scores=None,
            feedback=None,
            evaluation_metadata=None,
            messages=[]
        )
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            print(f"ğŸš€ [{user_id}] CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
            final_state = self.workflow.invoke(initial_state)
            
            # ê°„ë‹¨í•œ ëŒ€í™” ìš”ì•½ ì •ë³´ ìƒì„±
            student_questions = [msg for msg in conversation_log if msg.get("role") == "student"]
            conversation_summary = {
                "total_questions": len(student_questions),
                "duration_minutes": len(conversation_log) * 0.5
            }
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "evaluation_metadata": final_state.get("evaluation_metadata", {}),
                "scores": final_state.get("final_scores", {}),
                "feedback": final_state.get("feedback", {}),
                "conversation_summary": conversation_summary,
                "detailed_analysis": {
                    "medical_context": final_state.get("medical_context_analysis", {}),
                    "question_intent": final_state.get("question_intent_analysis", {}),
                    "completeness": final_state.get("completeness_assessment", {}),
                    "quality": final_state.get("quality_evaluation", {}),
                    "appropriateness": final_state.get("appropriateness_validation", {}),
                    "comprehensive": final_state.get("comprehensive_evaluation", {})
                },
                "evaluation_method": "6ë‹¨ê³„ ì˜í•™ì  ë¶„ì„",
                "system_info": {
                    "version": "v2.0",
                    "evaluation_steps": 6
                }
            }
            
            print(f"ğŸ‰ [{user_id}] CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            print(f"âŒ [{user_id}] í‰ê°€ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "user_id": user_id,
                "scenario_id": scenario_id,
                "evaluation_date": datetime.now().isoformat()
            }

    def _create_evaluation_workflow(self):
        """CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ìƒì„± (3ë‹¨ê³„ ëª…í™•í™”)"""
        workflow = StateGraph(CPXEvaluationState)

        workflow.add_node("initialize", self._initialize_evaluation)
        workflow.add_node("step1_rag_completeness", self._evaluate_rag_completeness)
        workflow.add_node("step2_quality_assessment", self._evaluate_quality_assessment)
        workflow.add_node("step3_comprehensive_results", self._generate_comprehensive_results)

        workflow.set_entry_point("initialize")
        workflow.add_edge("initialize", "step1_rag_completeness")
        workflow.add_edge("step1_rag_completeness", "step2_quality_assessment")
        workflow.add_edge("step2_quality_assessment", "step3_comprehensive_results")
        workflow.add_edge("step3_comprehensive_results", END)

        return workflow.compile()

    def _initialize_evaluation(self, state: CPXEvaluationState) -> CPXEvaluationState:
        print(f"ğŸ¯ [{state['user_id']}] CPX í‰ê°€ ì´ˆê¸°í™” - ì‹œë‚˜ë¦¬ì˜¤: {state['scenario_id']}")
        
        metadata = {
            "user_id": state["user_id"],
            "scenario_id": state["scenario_id"],
            "evaluation_date": datetime.now().isoformat(),
            "conversation_duration_minutes": len(state["conversation_log"]) * 0.5,
            "voice_recording_path": "s3ë¡œ ì €ì¥",
            "conversation_transcript": json.dumps(state["conversation_log"], ensure_ascii=False)
        }
        
        return {
            **state,
            "evaluation_metadata": metadata,
            "messages": [HumanMessage(content="CPX í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")]
        }

    def _evaluate_rag_completeness(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ (ë³‘ë ¥ì²­ì·¨, ì‹ ì²´ì§„ì°°, í™˜ìêµìœ¡)"""
        print(f"ğŸ“‹ [{state['user_id']}] 1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì‹œì‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        scenario_id = state["scenario_id"]
        
        # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë“œ
        scenario_category = self._get_scenario_category(scenario_id)
        if not scenario_category:
            raise ValueError(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_id}'ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        rag_data = {
            "scenario_id": scenario_id,
            "category": scenario_category
        }
        
        # 3ê°œ ì˜ì—­ë³„ ì²­í¬ ê¸°ë°˜ í‰ê°€
        areas_evaluation = {}
        
        for area_key, area_name in [("history_taking", "ë³‘ë ¥ ì²­ì·¨"), ("physical_examination", "ì‹ ì²´ ì§„ì°°"), ("patient_education", "í™˜ì êµìœ¡")]:
            # RAGì—ì„œ ê°€ì´ë“œë¼ì¸ ê°€ì ¸ì˜¤ê¸°
            criteria_data = self.guideline_retriever.get_evaluation_criteria(scenario_category, area_name)
            documents = criteria_data.get("documents", [])
            
            if not documents or not documents[0]:
                raise ValueError(f"âŒ {area_name} ê°€ì´ë“œë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # êµ¬ì¡°í™”ëœ ì„¹ì…˜ íŒŒì‹±
            structured_sections = self._parse_structured_sections(documents[0])
            
            # ì²­í¬ ê¸°ë°˜ í‰ê°€ ì‹¤í–‰
            areas_evaluation[area_key] = self.evaluate_area_with_chunks(
                state["conversation_log"], area_name, structured_sections
            )
        
        # ì „ì²´ ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
        total_guidelines = sum(area.get("total_guidelines", 0) for area in areas_evaluation.values())
        completed_guidelines = sum(area.get("completed_guidelines", 0) for area in areas_evaluation.values())
        overall_completeness = completed_guidelines / total_guidelines if total_guidelines > 0 else 0
        
        # ì „ì²´ ì™„ë£Œ/ëˆ„ë½ í•­ëª© ìˆ˜ì§‘ (ìƒˆë¡œìš´ JSON í˜•ì‹ ëŒ€ì‘)
        all_completed_items = []
        all_missing_items = []
        for area_data in areas_evaluation.values():
            # ê¸°ì¡´ í˜•ì‹ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
            if "completed_items" in area_data:
                all_completed_items.extend(area_data.get("completed_items", []))
            if "missing_items" in area_data:
                all_missing_items.extend(area_data.get("missing_items", []))
            
            # ìƒˆë¡œìš´ section_evaluations í˜•ì‹ ì§€ì›
            section_evals = area_data.get("section_evaluations", [])
            for section in section_evals:
                if section.get("status") == "completed":
                    covered_desc = section.get("how_covered", "")
                    if covered_desc:
                        all_completed_items.append(f"{section.get('section_name', '')}: {covered_desc}")
                elif section.get("status") in ["partial", "missing"]:
                    missing_aspects = section.get("missing_aspects", [])
                    if missing_aspects:
                        all_missing_items.extend([f"{section.get('section_name', '')}: {aspect}" for aspect in missing_aspects])
                    elif section.get("status") == "missing":
                        all_missing_items.append(f"{section.get('section_name', '')}: ì „ì²´ ëˆ„ë½")
        
        rag_completeness_result = {
            "category": scenario_category or scenario_id,
            "overall_completeness": round(overall_completeness, 2),
            "areas_evaluation": areas_evaluation,
            "total_completed_items": len(all_completed_items),
            "total_missing_items": len(all_missing_items),
            "completed_items": all_completed_items,
            "missing_items": all_missing_items,
            "evaluation_method": "rag_three_areas"
        }
        
        print(f"âœ… [{state['user_id']}] 1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ - ì™„ì„±ë„: {overall_completeness:.2%}")
        
        return {
            **state,
            "completeness_assessment": rag_completeness_result,
            "messages": state["messages"] + [HumanMessage(content="1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ")]
        }

    


    def _build_conversation_text(self, conversation_log: List[Dict]) -> str:
        """ëŒ€í™” ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        conversation_parts = []
        for msg in conversation_log:
            speaker = "í•™ìƒ" if msg.get("role") == "student" else "í™˜ì"
            content = msg.get("content", "")
            conversation_parts.append(f"{speaker}: {content}")
        return "\n".join(conversation_parts)

    def _evaluate_quality_assessment(self, state: CPXEvaluationState) -> CPXEvaluationState:
        print(f"â­ [{state['user_id']}] 2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        scenario_id = state["scenario_id"]
        scenario_info = self.scenario_applicable_elements.get(scenario_id, {})
        
        quality_assessment_prompt = f"""
ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ CPX ëŒ€í™”ì˜ í’ˆì§ˆì„ 4ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ã€í•™ìƒ-í™˜ì ëŒ€í™”ã€‘: {conversation_text}
ã€ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ã€‘: {scenario_info.get('name', scenario_id)}

ë‹¤ìŒ 4ê°€ì§€ í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:

ã€1. ì˜í•™ì  ì •í™•ì„± (Medical Accuracy)ã€‘:
- ì§ˆë¬¸ì˜ ì˜í•™ì  íƒ€ë‹¹ì„±ê³¼ ì •í™•ì„±
- ì§„ë‹¨ì  ì ‘ê·¼ì˜ ë…¼ë¦¬ì„±
- ì˜í•™ ìš©ì–´ ì‚¬ìš©ì˜ ì ì ˆì„±
- ì„ìƒì  íŒë‹¨ì˜ í•©ë¦¬ì„±

ã€2. ì˜ì‚¬ì†Œí†µ íš¨ìœ¨ì„± (Communication Efficiency)ã€‘:
- í™˜ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
- ì§ˆë¬¸ì˜ ëª…í™•ì„±ê³¼ êµ¬ì²´ì„±
- í™˜ì ë°˜ì‘ì— ëŒ€í•œ ì ì ˆí•œ í›„ì† ì§ˆë¬¸
- ëŒ€í™” íë¦„ì˜ ìì—°ìŠ¤ëŸ¬ì›€

ã€3. ì „ë¬¸ì„± (Professionalism)ã€‘:
- ì˜ë£Œì§„ë‹¤ìš´ íƒœë„ì™€ ì˜ˆì˜
- í™˜ìì— ëŒ€í•œ ê³µê°ê³¼ ë°°ë ¤
- ì²´ê³„ì ì´ê³  ë…¼ë¦¬ì ì¸ ì ‘ê·¼
- ìì‹ ê° ìˆëŠ” ì§„ë£Œ íƒœë„

ã€4. ì‹œë‚˜ë¦¬ì˜¤ ì í•©ì„± (Scenario Appropriateness)ã€‘:
- ì£¼ì–´ì§„ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ëŠ” ì ‘ê·¼
- í™˜ì ì—°ë ¹/ì„±ë³„/ìƒí™© ê³ ë ¤
- ì‹œê°„ ì œì•½ ë‚´ íš¨ìœ¨ì  ì§„í–‰
- ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì²´ê³„ì  ì ‘ê·¼

ê° í•­ëª©ì„ 1-10ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ì „ì²´ í’ˆì§ˆ ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "medical_accuracy": ì˜í•™ì ì •í™•ì„±ì ìˆ˜(1-10),
    "communication_efficiency": ì˜ì‚¬ì†Œí†µíš¨ìœ¨ì„±ì ìˆ˜(1-10),
    "professionalism": ì „ë¬¸ì„±ì ìˆ˜(1-10),
    "scenario_appropriateness": ì‹œë‚˜ë¦¬ì˜¤ì í•©ì„±ì ìˆ˜(1-10),
    "overall_quality_score": ì „ì²´í’ˆì§ˆì ìˆ˜(1-10),
    "quality_strengths": ["í’ˆì§ˆ ë©´ì—ì„œ ìš°ìˆ˜í•œ ì ë“¤"],
    "quality_improvements": ["í’ˆì§ˆ ë©´ì—ì„œ ê°œì„ ì´ í•„ìš”í•œ ì ë“¤"],
    "detailed_analysis": {{
        "medical_accuracy_detail": "ì˜í•™ì  ì •í™•ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„",
        "communication_detail": "ì˜ì‚¬ì†Œí†µì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„",
        "professionalism_detail": "ì „ë¬¸ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„",
        "scenario_fit_detail": "ì‹œë‚˜ë¦¬ì˜¤ ì í•©ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„"
    }}
}}
"""
        
        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                HumanMessage(content=quality_assessment_prompt)
            ]
            
            response = self.llm.invoke(messages)
            # responseê°€ dictì¸ ê²½ìš°ì™€ ê°ì²´ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            if hasattr(response, 'content'):
                result_text = response.content.strip()
            elif isinstance(response, dict) and 'content' in response:
                result_text = response['content'].strip()
            else:
                result_text = str(response).strip()
            
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                quality_assessment = json.loads(json_match.group())
                
                print(f"âœ… [{state['user_id']}] 2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - ì¢…í•© ì ìˆ˜: {quality_assessment.get('overall_quality_score', 0)}")
                
                return {
                    **state,
                    "quality_evaluation": quality_assessment,
                    "messages": state["messages"] + [HumanMessage(content="2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì™„ë£Œ")]
                }
            else:
                raise ValueError(f"í’ˆì§ˆ í‰ê°€ì—ì„œ JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ: {result_text}")
        except Exception as e:
            print(f"âŒ [{state['user_id']}] í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise e

    def _generate_comprehensive_results(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """3ë‹¨ê³„: ì¢…í•© í‰ê°€ ë° ìµœì¢… ê²°ê³¼ ìƒì„±"""
        print(f"ğŸ¯ [{state['user_id']}] 3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì‹œì‘")
        
        # 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ ê²°ê³¼ ìˆ˜ì§‘
        rag_completeness = state.get("completeness_assessment", {})
        quality_assessment = state.get("quality_evaluation", {})
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜: ì™„ì„±ë„ 60%, í’ˆì§ˆ 40%)
        completeness_score = rag_completeness.get("overall_completeness", 0.5) * 10  # 0-10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        quality_score = quality_assessment.get("overall_quality_score", 6)
        
        # ê°€ì¤‘ì¹˜ ì ìš©: ì™„ì„±ë„ 60%, í’ˆì§ˆ 40%
        final_score = (completeness_score * 0.6) + (quality_score * 0.4)
        final_score = min(10, max(0, final_score))  # 0-10 ë²”ìœ„ë¡œ ì œí•œ
        
        # ì¢…í•© í”¼ë“œë°± ìƒì„±
        strengths = []
        improvements = []
        
        # 1ë‹¨ê³„ RAG í‰ê°€ì—ì„œ ê°•ì /ê°œì„ ì  ìˆ˜ì§‘
        for area_name, area_data in rag_completeness.get("areas_evaluation", {}).items():
            if isinstance(area_data, dict):
                strengths.extend(area_data.get("strengths", []))
                improvements.extend(area_data.get("improvements", []))
        
        # 2ë‹¨ê³„ í’ˆì§ˆ í‰ê°€ì—ì„œ ê°•ì /ê°œì„ ì  ì¶”ê°€
        strengths.extend(quality_assessment.get("quality_strengths", []))
        improvements.extend(quality_assessment.get("quality_improvements", []))
        
        # ìƒì„¸ ë¶„ì„ ìƒì„±
        detailed_analysis_parts = []
        detailed_analysis_parts.append(f"ã€ì™„ì„±ë„ í‰ê°€ã€‘ RAG ê¸°ë°˜ í‰ê°€ ê²°ê³¼ {rag_completeness.get('overall_completeness', 0):.1%} ì™„ì„±")
        detailed_analysis_parts.append(f"ã€í’ˆì§ˆ í‰ê°€ã€‘ 4ê°€ì§€ í’ˆì§ˆ ê¸°ì¤€ í‰ê·  {quality_score:.1f}ì ")
        
        if rag_completeness.get("total_completed_items", 0) > 0:
            detailed_analysis_parts.append(f"ì´ {rag_completeness.get('total_completed_items', 0)}ê°œ í•­ëª© ì™„ë£Œ")
        
        if rag_completeness.get("total_missing_items", 0) > 0:
            detailed_analysis_parts.append(f"{rag_completeness.get('total_missing_items', 0)}ê°œ í•­ëª© ëˆ„ë½")
        
        comprehensive_result = {
            "final_score": round(final_score, 1),
            "grade": self._calculate_grade(final_score * 10),  # 100ì  ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            "score_breakdown": {
                "completeness_score": round(completeness_score, 1),
            "quality_score": round(quality_score, 1),
                "weighted_completeness": round(completeness_score * 0.6, 1),
                "weighted_quality": round(quality_score * 0.4, 1)
            },
            "detailed_feedback": {
                "strengths": list(set(strengths))[:5] if strengths else ["í‰ê°€ë¥¼ ì„±ì‹¤íˆ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤"],
                "improvements": list(set(improvements))[:5] if improvements else ["ì§€ì†ì ì¸ í•™ìŠµê³¼ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤"],
                "overall_analysis": " | ".join(detailed_analysis_parts)
            },
            "evaluation_summary": {
                "method": "3ë‹¨ê³„ RAG+í’ˆì§ˆ í‰ê°€",
                "steps_completed": 3,
                "completeness_rate": rag_completeness.get("overall_completeness", 0),
                "quality_details": quality_assessment.get("detailed_analysis", {}),
                "total_items_evaluated": rag_completeness.get("total_completed_items", 0) + rag_completeness.get("total_missing_items", 0)
            }
        }
        
        print(f"âœ… [{state['user_id']}] 3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {final_score:.1f}/10 ({comprehensive_result['grade']})")
        
        return {
            **state,
            "comprehensive_evaluation": comprehensive_result,
            "final_scores": {
                "total_score": round(final_score * 10, 1),  # 100ì  ìŠ¤ì¼€ì¼
                "completion_rate": rag_completeness.get("overall_completeness", 0.5),
                "quality_score": quality_score,
                "grade": comprehensive_result["grade"]
            },
            "feedback": comprehensive_result["detailed_feedback"],
            "messages": state["messages"] + [HumanMessage(content=f"3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì™„ë£Œ - {final_score:.1f}ì  ({comprehensive_result['grade']})")]
        }

    def _calculate_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ê³„ì‚°"""
        if score >= 90:
            return "A+"
        elif score >= 85:
            return "A"
        elif score >= 80:
            return "B+"
        elif score >= 75:
            return "B"
        elif score >= 70:
            return "C+"
        elif score >= 65:
            return "C"
        else:
            return "F"

    async def _update_cpx_database_after_evaluation(self, session_id: str, evaluation_result: dict):
        """í‰ê°€ ì™„ë£Œ í›„ CPX Detailsë§Œ ì—…ë°ì´íŠ¸"""
        try:
            session = self.session_data[session_id]
            result_id = session["result_id"]
            user_id = session["user_id"]
            
            # CPX Detailsë§Œ ì—…ë°ì´íŠ¸ (ì‹œìŠ¤í…œ í‰ê°€ ë°ì´í„°)
            async for db in get_db():
                cpx_service = CpxService(db)
                
                await cpx_service.update_cpx_details(
                    result_id=result_id,
                    user_id=int(user_id),
                    system_evaluation_data=evaluation_result
                )
                
                print(f"âœ… CPX Details ì—…ë°ì´íŠ¸ ì™„ë£Œ: result_id={result_id}, session_id={session_id}")
                break
                
        except Exception as e:
            print(f"âŒ CPX Details ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def _cleanup_audio_files(self, audio_file_path: str):
        """í‰ê°€ ì™„ë£Œ í›„ ì„ì‹œ WAV íŒŒì¼ë“¤ë§Œ ì‚­ì œ (TTS ìºì‹œ íŒŒì¼ì€ ë³´ì¡´)"""

        try:
            file_path_obj = Path(audio_file_path)
            # TTS ìºì‹œ íŒŒì¼ì€ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            if "cache/tts" in str(file_path_obj):
                print(f"ğŸ”’ TTS ìºì‹œ íŒŒì¼ ë³´ì¡´: {audio_file_path}")
                return
                
            if file_path_obj.exists() and file_path_obj.suffix == '.wav':
                file_path_obj.unlink()  # WAV íŒŒì¼ë§Œ ì‚­ì œ
                print(f"ğŸ—‘ï¸ ì„ì‹œ WAV íŒŒì¼ ì‚­ì œ: {audio_file_path}")
                    
        except Exception as e:
            print(f"âŒ WAV íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({audio_file_path}): {e}")