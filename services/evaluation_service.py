"""
CPX í‰ê°€ ì„œë¹„ìŠ¤ - ì •ë¦¬ëœ ë²„ì „
ì˜ë£Œ ì‹œë®¬ë ˆì´ì…˜ ëŒ€í™” í‰ê°€ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤
"""

from typing import Dict, List, Optional, TypedDict, Annotated
from datetime import datetime
from pathlib import Path
import json
import logging
import os
import re
import time

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage as AnyMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

from services.cpx_service import CpxService
from core.config import get_db

from RAG.guideline_retriever import GuidelineRetriever

logger = logging.getLogger(__name__)


class CPXEvaluationState(TypedDict):
    """CPX í‰ê°€ ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    user_id: str
    scenario_id: str
    conversation_log: List[Dict]
    
    # í‰ê°€ ê²°ê³¼ë“¤
    completeness_assessment: Optional[Dict]
    quality_evaluation: Optional[Dict]
    
    # ì¢…í•© í‰ê°€ ê²°ê³¼
    comprehensive_evaluation: Optional[Dict]
    
    # ìµœì¢… ê²°ê³¼
    final_scores: Optional[Dict]
    feedback: Optional[Dict]
    markdown_feedback: Optional[str]  # ë§ˆí¬ë‹¤ìš´ í”¼ë“œë°± ì¶”ê°€
    
    # ë©”íƒ€ë°ì´í„°
    evaluation_metadata: Optional[Dict]
    
    # ë©”ì‹œì§€ ì¶”ì 
    messages: Annotated[List[AnyMessage], add_messages]


class EvaluationService:
    """CPX í‰ê°€ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """CPX í‰ê°€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.session_data = {}
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬
        self.evaluation_dir = Path("evaluation_results")
        self.evaluation_dir.mkdir(parents=True, exist_ok=True)
        
        # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê´€ë ¨
        self.llm = None
        self.workflow = None
        self._initialize_langgraph_components()
        
        # RAG ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.guideline_retriever = None
        self._initialize_guideline_retriever()

    # ================================
    # 1. ì´ˆê¸°í™” ê´€ë ¨ ë©”ì„œë“œë“¤
    # ================================
    
    def _initialize_langgraph_components(self):
        """LangGraph ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # OpenAI API ì„¤ì •
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.llm = ChatOpenAI(
                    openai_api_key=api_key,
                    model_name="gpt-5-nano",
                    max_tokens=4000
                )
                
                # ì›Œí¬í”Œë¡œìš° ìƒì„±
                self.workflow = self._create_evaluation_workflow()
                print("âœ… LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ í‰ê°€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            print(f"âŒ LangGraph ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm = None
            self.workflow = None

    def _initialize_guideline_retriever(self):
        """RAG ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        try:
            # ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
            rag_path = Path(__file__).parent.parent / "RAG"
            index_path = rag_path / "faiss_guideline_index"
            self.guideline_retriever = GuidelineRetriever(index_path=str(index_path))
            
            if self.guideline_retriever.vectorstore:
                print("âœ… RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸ RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
                self.guideline_retriever = None
                
        except Exception as e:
            print(f"âŒ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.guideline_retriever = None



    # ================================
    # 2. ì„¸ì…˜ ê´€ë¦¬ ë©”ì„œë“œë“¤ (ì™¸ë¶€ API)
    # ================================
    
    async def start_evaluation_session(self, user_id: str, scenario_id: str, result_id: Optional[int] = None) -> str:
        """í‰ê°€ ì„¸ì…˜ ì‹œì‘"""
        session_id = f"{user_id}_{scenario_id}_{int(datetime.now().timestamp())}"
        
        self.session_data[session_id] = {
            "user_id": user_id,
            "scenario_id": scenario_id,
            "result_id": result_id,  # CPX result_id ì €ì¥
            "start_time": datetime.now(),
            "conversation_entries": [],  # ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„°
            "status": "active"
        }
        
        return session_id

    async def add_conversation_entry(self, session_id: str, text: str, role: str, emotion_analysis: Optional[Dict] = None) -> Dict:
        """ì‹¤ì‹œê°„ ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€ (SER ê²°ê³¼ëŠ” queueì—ì„œ ì „ë‹¬ë°›ìŒ)"""
        if session_id not in self.session_data:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            timestamp = datetime.now()
            
            # SER ê²°ê³¼ ë¡œê¹… (queueì—ì„œ ì „ë‹¬ë°›ì€ ê²½ìš°)
            if emotion_analysis:
                print(f"ğŸ­ [{session_id}] ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹ : {emotion_analysis['predicted_emotion']} ({emotion_analysis['confidence']:.2f})")
            
            # ëŒ€í™” ì—”íŠ¸ë¦¬ ìƒì„±
            conversation_entry = {
                "timestamp": timestamp.isoformat(),
                "text": text,
                "emotion": emotion_analysis,
                "role": role,  # "doctor" (ì˜ì‚¬) or "patient" (í™˜ì)
            }
            
            # ì„¸ì…˜ ë°ì´í„°ì— ì¶”ê°€
            session = self.session_data[session_id]
            session["conversation_entries"].append(conversation_entry)
            
            print(f"ğŸ“ [{session_id}] ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€: {role} - {text[:50]}...")
            
            return {
                "success": True,
                "entry": conversation_entry,
                "total_entries": len(session["conversation_entries"])
            }
            
        except Exception as e:
            print(f"âŒ [{session_id}] ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def end_evaluation_session(self, session_id: str) -> Dict:
        """í‰ê°€ ì„¸ì…˜ ì¢…ë£Œ ë° ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        if session_id not in self.session_data:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        session = self.session_data[session_id]
        session["end_time"] = datetime.now()
        session["status"] = "completed"
        
        # ì¢…í•© í‰ê°€ ì‹¤í–‰
        evaluation_result = await self._comprehensive_evaluation(session_id, session)
        
        # CPX ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        await self._update_cpx_database_after_evaluation(session_id, evaluation_result)
        
        return evaluation_result



    # ================================
    # 3. LangGraph ì›Œí¬í”Œë¡œìš° ê´€ë ¨
    # ================================
    
    def _create_evaluation_workflow(self):
        """CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ìƒì„± (3ë‹¨ê³„)"""
        workflow = StateGraph(CPXEvaluationState)

        workflow.add_node("initialize", self._initialize_evaluation)
        workflow.add_node("step1_rag_completeness", self._evaluate_rag_completeness)
        workflow.add_node("step2_quality_assessment", self._evaluate_quality_assessment)
        workflow.add_node("step3_comprehensive_results", self._generate_comprehensive_results)

        workflow.set_entry_point("initialize")
        workflow.add_edge("initialize", "step1_rag_completeness")
        workflow.add_edge("step1_rag_completeness", "step2_quality_assessment")
        workflow.add_edge("step2_quality_assessment", "step3_comprehensive_results")
        workflow.add_edge("step3_comprehensive_results", END)

        return workflow.compile()

    def _initialize_evaluation(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """í‰ê°€ ì´ˆê¸°í™”"""
        print(f"ğŸ¯ [{state['user_id']}] CPX í‰ê°€ ì´ˆê¸°í™” - ì‹œë‚˜ë¦¬ì˜¤: {state['scenario_id']}")
        
        metadata = {
            "user_id": state["user_id"],
            "scenario_id": state["scenario_id"],
            "evaluation_date": datetime.now().isoformat(),
            "conversation_duration_minutes": len(state["conversation_log"]) * 0.5,
            "voice_recording_path": "s3ë¡œ ì €ì¥",
            "conversation_transcript": json.dumps(state["conversation_log"], ensure_ascii=False)
        }
        
        return {
            **state,
            "evaluation_metadata": metadata,
            "messages": [HumanMessage(content="CPX í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")]
        }

    async def _evaluate_rag_completeness(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ (ë³‘ë ¥ì²­ì·¨, ì‹ ì²´ì§„ì°°, í™˜ìêµìœ¡)"""
        print(f"ğŸ“‹ [{state['user_id']}] 1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì‹œì‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        scenario_id = state["scenario_id"]
        
        # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë“œ
        scenario_category = self._get_scenario_category(scenario_id)
        if not scenario_category:
            raise ValueError(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_id}'ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
        # 3ê°œ ì˜ì—­ë³„ ì²­í¬ ê¸°ë°˜ í‰ê°€
        areas_evaluation = {}
        
        for area_key, area_name in [("history_taking", "ë³‘ë ¥ ì²­ì·¨"), ("physical_examination", "ì‹ ì²´ ì§„ì°°"), ("patient_education", "í™˜ì êµìœ¡")]:
            # RAGì—ì„œ ê°€ì´ë“œë¼ì¸ ê°€ì ¸ì˜¤ê¸°
            criteria_data = self.guideline_retriever.get_evaluation_criteria(scenario_category, area_name)
            documents = criteria_data.get("documents", [])
            
            if not documents or not documents[0]:
                raise ValueError(f"âŒ {area_name} ê°€ì´ë“œë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # êµ¬ì¡°í™”ëœ ì„¹ì…˜ íŒŒì‹±
            structured_sections = self._parse_structured_sections(documents[0])
            
            # ê°„ë‹¨í•œ RAG ê°€ì´ë“œë¼ì¸ ë¹„êµ í‰ê°€ ì‹¤í–‰
            areas_evaluation[area_key] = await self._evaluate_area_simple(
                conversation_text, area_name, structured_sections
            )
        
        # ì „ì²´ ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
        total_guidelines = sum(area.get("total_guidelines", 0) for area in areas_evaluation.values())
        completed_guidelines = sum(area.get("completed_guidelines", 0) for area in areas_evaluation.values())
        overall_completeness = completed_guidelines / total_guidelines if total_guidelines > 0 else 0
        
        # ì „ì²´ ì™„ë£Œ/ëˆ„ë½ í•­ëª© ìˆ˜ì§‘ (ê°„ë‹¨í™”)
        all_completed_items = []
        all_missing_items = []
        for area_data in areas_evaluation.values():
            completed_count = area_data.get("completed_guidelines", 0)
            total_count = area_data.get("total_guidelines", 0)
            missing_count = total_count - completed_count
            
            if completed_count > 0:
                all_completed_items.append(f"{area_data.get('area_name', 'Unknown')}: {completed_count}ê°œ í•­ëª© ì™„ë£Œ")
            if missing_count > 0:
                all_missing_items.append(f"{area_data.get('area_name', 'Unknown')}: {missing_count}ê°œ í•­ëª© ëˆ„ë½")
        
        rag_completeness_result = {
            "category": scenario_category or scenario_id,
            "overall_completeness": round(overall_completeness, 2),
            "areas_evaluation": areas_evaluation,
            "total_completed_items": len(all_completed_items),
            "total_missing_items": len(all_missing_items),
            "evaluation_method": "rag_three_areas"
        }
        
        print(f"âœ… [{state['user_id']}] 1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ - ì™„ì„±ë„: {overall_completeness:.2%}")
        
        return {
            **state,
            "completeness_assessment": rag_completeness_result,
            "messages": state["messages"] + [HumanMessage(content=f"1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ - {overall_completeness:.1%}")]
        }

    async def _evaluate_quality_assessment(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """2ë‹¨ê³„: ëŒ€í™” í’ˆì§ˆ í‰ê°€ (ì¹œì ˆí•¨, ê³µê°, ì „ë¬¸ì„± ë“±)"""
        print(f"â­ [{state['user_id']}] 2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        scenario_id = state["scenario_id"]
        
        quality_prompt = f"""
ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ CPX ëŒ€í™”ì˜ í’ˆì§ˆì„ 4ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ã€ì˜ì‚¬-í™˜ì ëŒ€í™”ã€‘:
{conversation_text}

ã€ì‹œë‚˜ë¦¬ì˜¤ã€‘: {scenario_id}

ë‹¤ìŒ 4ê°€ì§€ í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:

ã€1. ì˜í•™ì  ì •í™•ì„±ã€‘:
- ì§ˆë¬¸ì˜ ì˜í•™ì  íƒ€ë‹¹ì„±ê³¼ ì •í™•ì„±
- ì§„ë‹¨ì  ì ‘ê·¼ì˜ ë…¼ë¦¬ì„±
- ì˜í•™ ìš©ì–´ ì‚¬ìš©ì˜ ì ì ˆì„±

ã€2. ì˜ì‚¬ì†Œí†µ íš¨ìœ¨ì„±ã€‘:
- í™˜ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
- ì§ˆë¬¸ì˜ ëª…í™•ì„±ê³¼ êµ¬ì²´ì„±
- ëŒ€í™” íë¦„ì˜ ìì—°ìŠ¤ëŸ¬ì›€

ã€3. ì „ë¬¸ì„±ã€‘:
- ì˜ë£Œì§„ë‹¤ìš´ íƒœë„ì™€ ì˜ˆì˜
- í™˜ìì— ëŒ€í•œ ê³µê°ê³¼ ë°°ë ¤
- ì²´ê³„ì ì´ê³  ë…¼ë¦¬ì ì¸ ì ‘ê·¼

ã€4. ì‹œë‚˜ë¦¬ì˜¤ ì í•©ì„±ã€‘:
- ì£¼ì–´ì§„ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ëŠ” ì ‘ê·¼
- í™˜ì ìƒí™© ê³ ë ¤
- íš¨ìœ¨ì  ì§„í–‰

ê° í•­ëª©ì„ 1-10ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

JSON ì‘ë‹µ:
{{
    "medical_accuracy": ì ìˆ˜(1-10),
    "communication_efficiency": ì ìˆ˜(1-10),
    "professionalism": ì ìˆ˜(1-10),
    "scenario_appropriateness": ì ìˆ˜(1-10),
    "overall_quality_score": ì „ì²´í’ˆì§ˆì ìˆ˜(1-10),
    "quality_strengths": ["ìš°ìˆ˜í•œ ì ë“¤"],
    "quality_improvements": ["ê°œì„ ì´ í•„ìš”í•œ ì ë“¤"]
}}"""

        try:
            messages = [SystemMessage(content=quality_prompt)]
            response = await self.llm.ainvoke(messages)
            result_text = response.content
            
            print(f"[í’ˆì§ˆ] LLM ì‘ë‹µ ì›ë¬¸:\n{result_text[:300]}...")
            
            # JSON íŒŒì‹±
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                quality_result = json.loads(json_str)
                print(f"[í’ˆì§ˆ] JSON íŒŒì‹± ì„±ê³µ")
            else:
                print(f"[í’ˆì§ˆ] JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                quality_result = {
                    "medical_accuracy": 6,
                    "communication_efficiency": 6,
                    "professionalism": 6,
                    "scenario_appropriateness": 6,
                    "overall_quality_score": 6,
                    "quality_strengths": ["RAG ê¸°ë°˜ í‰ê°€ ì™„ë£Œ"],
                    "quality_improvements": ["í’ˆì§ˆ í‰ê°€ ê°œì„  í•„ìš”"]
                }
            
            print(f"âœ… [{state['user_id']}] 2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - í‰ê· : {quality_result.get('overall_quality_score', 6):.1f}ì ")
            
            return {
                **state,
                "quality_evaluation": quality_result,
                "messages": state["messages"] + [HumanMessage(content=f"2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - {quality_result.get('overall_quality_score', 6):.1f}ì ")]
            }
            
        except Exception as e:
            print(f"âŒ [{state['user_id']}] í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                **state,
                "quality_evaluation": {
                    "medical_accuracy": 6,
                    "communication_efficiency": 6,
                    "professionalism": 6,
                    "scenario_appropriateness": 6,
                    "overall_quality_score": 6,
                    "quality_strengths": ["ê¸°ë³¸ í‰ê°€ ì™„ë£Œ"],
                    "quality_improvements": ["í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"]
                }
            }

    async def _generate_comprehensive_results(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """3ë‹¨ê³„: SER ê°ì • ë¶„ì„ + ì¢…í•© í‰ê°€ ë° ìµœì¢… ê²°ê³¼ ìƒì„±"""
        print(f"ğŸ¯ [{state['user_id']}] 3ë‹¨ê³„: SER ê°ì • ë¶„ì„ + ì¢…í•© í‰ê°€ ì‹œì‘")
        
        # 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ ê²°ê³¼ ìˆ˜ì§‘
        rag_completeness = state.get("completeness_assessment", {})
        quality_assessment = state.get("quality_evaluation", {})
        
        # SER ê°ì • ë¶„ì„ ìˆ˜í–‰
        ser_evaluation = await self._evaluate_ser_emotions(state["conversation_log"])
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜: ì™„ì„±ë„ 30%, í’ˆì§ˆ 30%, SER ê°ì • 40%)
        completeness_score = rag_completeness.get("overall_completeness", 0.5) * 10  # 0-10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        quality_score = quality_assessment.get("overall_quality_score", 6)
        ser_score = ser_evaluation.get("ser_score", 6)  # SER ê¸°ë°˜ ê°ì • ì ìˆ˜
        
        # ê°€ì¤‘ì¹˜ ì ìš©: ì™„ì„±ë„ 30%, í’ˆì§ˆ 30%, SER ê°ì • 40%
        final_score = (completeness_score * 0.3) + (quality_score * 0.3) + (ser_score * 0.4)
        final_score = min(10, max(0, final_score))  # 0-10 ë²”ìœ„ë¡œ ì œí•œ
        
        # ë“±ê¸‰ ê³„ì‚°
        grade = self._calculate_grade(final_score * 10)  # 100ì  ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        
        # ì¢…í•© í”¼ë“œë°± ìƒì„±
        strengths = []
        improvements = []
        
        # 1ë‹¨ê³„ì—ì„œ ê°•ì /ê°œì„ ì  ìˆ˜ì§‘
        for area_key, area_data in rag_completeness.get("areas_evaluation", {}).items():
            area_name = area_data.get('area_name', area_key)
            completion_rate = area_data.get("completion_rate", 0)
            if completion_rate > 0.7:
                strengths.append(f"{area_name} ì˜ì—­ ìš°ìˆ˜ ({completion_rate:.1%})")
            elif completion_rate < 0.5:
                improvements.append(f"{area_name} ì˜ì—­ ë³´ì™„ í•„ìš” ({completion_rate:.1%})")
        
        # 2ë‹¨ê³„ì—ì„œ ê°•ì /ê°œì„ ì  ì¶”ê°€
        strengths.extend(quality_assessment.get("quality_strengths", []))
        improvements.extend(quality_assessment.get("quality_improvements", []))
        
        # 3ë‹¨ê³„ SERì—ì„œ ê°•ì /ê°œì„ ì  ì¶”ê°€
        strengths.extend(ser_evaluation.get("ser_strengths", []))
        improvements.extend(ser_evaluation.get("ser_improvements", []))
        
        comprehensive_result = {
            "final_score": round(final_score, 1),
            "grade": grade,
            "detailed_feedback": {
                "strengths": strengths[:5],  # ìµœëŒ€ 5ê°œ
                "improvements": improvements[:5],  # ìµœëŒ€ 5ê°œ
                "overall_analysis": f"3ë‹¨ê³„ í†µí•© í‰ê°€ ê²°ê³¼ {final_score * 10:.1f}ì  (ì™„ì„±ë„ 30% + í’ˆì§ˆ 30% + ê°ì • 40%)"
            },
            "ser_evaluation": ser_evaluation  # SER í‰ê°€ ê²°ê³¼ í¬í•¨
        }
        
        print(f"âœ… [{state['user_id']}] 3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {final_score:.1f}/10 ({grade})")
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            **state,
            "comprehensive_evaluation": comprehensive_result,
            "final_scores": {
                "total_score": round(final_score * 10, 1),  # 100ì  ìŠ¤ì¼€ì¼
                "completion_rate": rag_completeness.get("overall_completeness", 0.5),
                "quality_score": quality_score,
                "grade": grade
            },
            "feedback": comprehensive_result["detailed_feedback"],
            "messages": state["messages"] + [HumanMessage(content=f"3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì™„ë£Œ - {final_score:.1f}ì  ({grade})")]
        }
        
        # ë§ˆí¬ë‹¤ìš´ í”¼ë“œë°± ìƒì„±
        print(f"ğŸ” DEBUG [{state['user_id']}] ë§ˆí¬ë‹¤ìš´ í”¼ë“œë°± ìƒì„± ì‹œì‘")
        try:
            # generate_evaluation_markdown í•¨ìˆ˜ê°€ ê¸°ëŒ€í•˜ëŠ” ì „ì²´ êµ¬ì¡° ìƒì„±
            evaluation_data = {
                "user_id": state['user_id'],
                "scenario_id": state['scenario_id'],
                "duration_minutes": 0,  # ì‹¤ì œ durationì´ ìˆë‹¤ë©´ ì‚¬ìš©
                "langgraph_text_analysis": {
                    "scores": final_result["final_scores"],
                    "feedback": final_result["feedback"], 
                    "detailed_analysis": {
                        "completeness": rag_completeness,
                        "quality": quality_assessment,
                        "comprehensive": comprehensive_result
                    },
                    "conversation_summary": {},
                    # ê¸°ì¡´ êµ¬ì¡°ë„ ìœ ì§€ (í˜¸í™˜ì„±)
                    "completeness_assessment": rag_completeness,
                    "quality_evaluation": quality_assessment,
                    "comprehensive_evaluation": comprehensive_result,
                    "final_scores": final_result["final_scores"]
                }
            }
            print(f"ğŸ” DEBUG [{state['user_id']}] evaluation_data êµ¬ì„± ì™„ë£Œ")
            
            markdown_feedback = self.generate_evaluation_markdown(evaluation_data)
            print(f"ğŸ” DEBUG [{state['user_id']}] generate_evaluation_markdown í˜¸ì¶œ ê²°ê³¼: type={type(markdown_feedback)}, len={len(markdown_feedback) if markdown_feedback else 0}")
            
            if markdown_feedback:
                print(f"ğŸ” DEBUG [{state['user_id']}] ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {markdown_feedback[:100]}...")
            else:
                print(f"ğŸ” DEBUG [{state['user_id']}] âš ï¸ ë§ˆí¬ë‹¤ìš´ì´ ë¹„ì–´ìˆìŒ!")
            
            print(f"âœ… [{state['user_id']}] ë§ˆí¬ë‹¤ìš´ í”¼ë“œë°± ìƒì„± ì™„ë£Œ ({len(markdown_feedback) if markdown_feedback else 0}ì)")
            
            # final_resultì— markdown_feedback ì¶”ê°€
            final_result["markdown_feedback"] = markdown_feedback
            print(f"ğŸ” DEBUG [{state['user_id']}] final_resultì— markdown_feedback ì¶”ê°€ ì™„ë£Œ")
            
            return final_result
            
        except Exception as e:
            print(f"âŒ [{state['user_id']}] ë§ˆí¬ë‹¤ìš´ í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ
            final_result["markdown_feedback"] = None
            return final_result

    # ================================
    # 4. í•µì‹¬ í‰ê°€ ë¡œì§
    # ================================
    
    async def _comprehensive_evaluation(self, session_id: str, session: Dict) -> Dict:
        """ì¢…í•©ì ì¸ ì„¸ì…˜ í‰ê°€ ìˆ˜í–‰ (SER + LangGraph í†µí•©)"""
        print(f"ğŸ” [{session_id}] ì¢…í•© í‰ê°€ ì‹œì‘...")
        
        # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ (ì§ì ‘ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰)
        langgraph_analysis = None
        if self.llm and self.workflow:
            try:
                # ìƒˆë¡œìš´ conversation_entriesë¥¼ conversation_log í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                conversation_log = []
                for entry in session.get("conversation_entries", []):
                    conversation_log.append({
                        "role": entry["role"],
                        "content": entry["text"],
                        "timestamp": entry["timestamp"],
                        "emotion": entry.get("emotion")
                    })
                
                if conversation_log:  # ëŒ€í™” ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í‰ê°€
                    # LangGraph ì›Œí¬í”Œë¡œìš° ì§ì ‘ ì‹¤í–‰
                    initial_state = CPXEvaluationState(
                        user_id=session["user_id"],
                        scenario_id=session["scenario_id"],
                        conversation_log=conversation_log,
                        completeness_assessment=None,
                        quality_evaluation=None,
                        comprehensive_evaluation=None,
                        final_scores=None,
                        feedback=None,
                        markdown_feedback=None,  # ë§ˆí¬ë‹¤ìš´ í”¼ë“œë°± ì´ˆê¸°í™”
                        evaluation_metadata=None,
                        messages=[]
                    )
                    
                    print(f"ğŸš€ [{session_id}] LangGraph ì›Œí¬í”Œë¡œìš° ì‹œì‘")
                    final_state = await self.workflow.ainvoke(initial_state)
                    
                    print(f"ğŸ” DEBUG [{session_id}] final_state keys: {list(final_state.keys()) if isinstance(final_state, dict) else 'Not dict'}")
                    print(f"ğŸ” DEBUG [{session_id}] final_state.markdown_feedback: {final_state.get('markdown_feedback')}")
                    
                    # LangGraph ë¶„ì„ ê²°ê³¼ êµ¬ì„±
                    student_questions = [msg for msg in conversation_log if msg.get("role") == "doctor"]
                    conversation_summary = {
                        "total_questions": len(student_questions),
                        "duration_minutes": (session["end_time"] - session["start_time"]).total_seconds() / 60
                    }
                    
                    markdown_from_state = final_state.get("markdown_feedback")
                    print(f"ğŸ” DEBUG [{session_id}] markdown_from_state: type={type(markdown_from_state)}, value={markdown_from_state}")
                    
                    langgraph_analysis = {
                        "evaluation_metadata": final_state.get("evaluation_metadata", {}),
                        "scores": final_state.get("final_scores", {}),
                        "feedback": final_state.get("feedback", {}),
                        "conversation_summary": conversation_summary,
                        "detailed_analysis": {
                            "completeness": final_state.get("completeness_assessment", {}),
                            "quality": final_state.get("quality_evaluation", {}),
                            "comprehensive": final_state.get("comprehensive_evaluation", {})
                        },
                        "markdown_feedback": markdown_from_state,
                        "evaluation_method": "3ë‹¨ê³„ ì˜í•™ì  ë¶„ì„",
                        "system_info": {
                            "version": "v2.0",
                            "evaluation_steps": 3
                        }
                    }
                    print(f"âœ… [{session_id}] LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ [{session_id}] ëŒ€í™” ë°ì´í„°ê°€ ì—†ì–´ LangGraph í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                
            except Exception as e:
                print(f"âŒ [{session_id}] LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
                langgraph_analysis = {"error": str(e)}
        
        # ì¢…í•© ê²°ê³¼ êµ¬ì„±
        evaluation_result = {
            "session_id": session_id,
            "user_id": session["user_id"],
            "scenario_id": session["scenario_id"],
            "start_time": session["start_time"].isoformat(),
            "end_time": session["end_time"].isoformat(),
            "duration_minutes": (session["end_time"] - session["start_time"]).total_seconds() / 60,
            
            # ìƒì„¸ ë¶„ì„ ê²°ê³¼
            "langgraph_text_analysis": langgraph_analysis,  # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼
            
            # ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„° (ê°ì • ë¶„ì„ í¬í•¨)
            "conversation_entries": [
                {
                    "timestamp": entry["timestamp"],
                    "text": entry["text"],
                    "role": entry["role"],
                    "emotion": entry.get("emotion")
                }
                for entry in session.get("conversation_entries", [])
            ]
        }
        
        # í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        try:
            timestamp = int(time.time())
            filename = f"evaluation_{session_id}_{timestamp}.json"
            file_path = self.evaluation_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(evaluation_result, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ’¾ [{session_id}] í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"âŒ [{session_id}] í‰ê°€ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print(f"âœ… [{session_id}] ì¢…í•© í‰ê°€ ì™„ë£Œ")
        return evaluation_result

    async def _evaluate_area_simple(self, conversation_text: str, area_name: str, structured_sections: dict) -> dict:
        """ë‹¨ì¼ ë‹¨ê³„ RAG ê°€ì´ë“œë¼ì¸ ë¹„êµ í‰ê°€ - GPT-4o í†µí•© í‰ê°€"""
        
        # ê°€ì´ë“œë¼ì¸ í…ìŠ¤íŠ¸ êµ¬ì„±
        detailed_guideline_text = ""
        for section_name, section_data in structured_sections.items():
            required_items = section_data.get('required_questions', []) + section_data.get('required_actions', [])
            if required_items:
                detailed_guideline_text += f"\nã€{section_name}ã€‘\n"
                detailed_guideline_text += "ì´ í•­ëª©ì—ì„œ í™•ì¸í•´ì•¼ í•  êµ¬ì²´ì  ë‚´ìš©ë“¤:\n"
                for item in required_items:
                    detailed_guideline_text += f"  â€¢ {item}\n"
                detailed_guideline_text += "\n"
        
        print(f"[í†µí•© í‰ê°€] {area_name} ì˜ì—­ í‰ê°€ ì‹œì‘...")
        
        prompt = f"""{area_name} ì˜ì—­ í‰ê°€

ì „ì²´ ëŒ€í™”:
{conversation_text}

í‰ê°€í•  í•­ëª©ë“¤:
{detailed_guideline_text}

**í‰ê°€ ë°©ë²•**:
1. ë¨¼ì € ì „ì²´ ëŒ€í™”ì—ì„œ {area_name} ì˜ì—­ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ íŒŒì•…í•˜ì„¸ìš”
2. í•´ë‹¹ ì˜ì—­ì˜ ëŒ€í™” ë‚´ìš©ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ ê° í•­ëª©ì„ í‰ê°€í•˜ì„¸ìš”
3. ë‹¤ë¥¸ ì˜ì—­ì˜ ë°œì–¸ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

**ì˜ì—­ íŒŒì•… ê°€ì´ë“œ**:
**ì¼ë°˜ì  ìˆœì„œ**: ë³‘ë ¥ì²­ì·¨(ì´ˆë°˜) â†’ ì‹ ì²´ì§„ì°°(ì¤‘ë°˜) â†’ í™˜ìêµìœ¡(í›„ë°˜)

**ì˜ì—­ë³„ íŠ¹ì§•**:
- **ë³‘ë ¥ì²­ì·¨**: ì •ë³´ ìˆ˜ì§‘ ëª©ì 
  * íŠ¹ì§•: ì§ˆë¬¸í˜• ë°œì–¸, "ì–¸ì œë¶€í„°", "ì–´ë–»ê²Œ", "ìˆìœ¼ì„¸ìš”" ë“±
  * ìœ„ì¹˜: ëŒ€í™” ì´ˆë°˜~ì¤‘ë°˜ (ì§„ì°° ì‹œì‘ ì „)
  * ëª©ì : ì¦ìƒ, ë³‘ë ¥, ê°€ì¡±ë ¥ ë“± ì •ë³´ íƒìƒ‰

- **ì‹ ì²´ì§„ì°°**: ê²€ì‚¬ ìˆ˜í–‰ ëª©ì 
  * íŠ¹ì§•: "ì§„ì°°í•˜ê² ìŠµë‹ˆë‹¤", "ê²€ì‚¬í•˜ê² ìŠµë‹ˆë‹¤" ë“± í–‰ìœ„í˜• ë°œì–¸
  * ìœ„ì¹˜: ë³‘ë ¥ì²­ì·¨ í›„ ëª…ì‹œì  ì§„ì°° êµ¬ê°„
  * ëª©ì : ë¬¼ë¦¬ì  ê²€ì‚¬ ì‹¤ì‹œ

- **í™˜ìêµìœ¡**: ì •ë³´ ì „ë‹¬ ëª©ì 
  * íŠ¹ì§•: "ê°€ëŠ¥ì„±", "ë•Œë¬¸ì—", "ì…ë‹ˆë‹¤" ë“± ì„¤ëª…í˜• ë°œì–¸
  * ìœ„ì¹˜: ì‹ ì²´ì§„ì°° í›„~ëŒ€í™” ì¢…ë£Œ
  * ëª©ì : ì§„ë‹¨ ì„¤ëª…, ì¹˜ë£Œ ê³„íš ì•ˆë‚´

**í‰ê°€ ê·œì¹™**:
- {area_name} ì˜ì—­ ë°œì–¸ë§Œ evidenceë¡œ ì‚¬ìš©
- ëŒ€í™” ì›ë¬¸ì„ ì •í™•íˆ ë³µì‚¬ (ë³€ê²½ ê¸ˆì§€)
- ê´€ë ¨ ë‚´ìš©ì´ ë‹¤ë¤„ì¡Œìœ¼ë©´ completed: true

JSON ì‘ë‹µ:
{{
{', '.join([f'    "{section_name}": {{"completed": true/false, "evidence": []}}' for section_name in structured_sections.keys()])}
}}"""
        
        result = await self._process_evaluation_response(prompt, area_name, structured_sections, stage="í†µí•©")
        
        print(f"[ê²€ì¦] evidence ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸...")
        print(f"[ê²€ì¦] ëŒ€í™” í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {conversation_text[:200]}...")
        # evidence ê²€ì¦ ë‹¨ê³„ ì¶”ê°€
        verified_result = self._verify_evidence_exists(conversation_text, result)
        
        return verified_result
        


    # ================================
    # 5. ìœ í‹¸ë¦¬í‹° ë° í—¬í¼ ë©”ì„œë“œë“¤
    # ================================
    
    def _get_scenario_category(self, scenario_id: str) -> Optional[str]:
        """ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë“œ"""
        try:
            scenario_path = Path(f"scenarios/neurology_dementia_case.json")  # í˜„ì¬ëŠ” í•˜ë‚˜ì˜ ì‹œë‚˜ë¦¬ì˜¤ë§Œ
            if not scenario_path.exists():
                return None
            
            with open(scenario_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("scenario_info", {}).get("category")
        except Exception as e:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _parse_structured_sections(self, document) -> dict:
        """ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì„¹ì…˜ íŒŒì‹± - RAG ê°€ì´ë“œë¼ì¸ JSON í˜•ì‹ ì²˜ë¦¬"""
        structured_sections = {}
        
        # ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        if hasattr(document, 'page_content'):
            document_text = document.page_content
        elif isinstance(document, dict):
            document_text = document.get('content', '') or document.get('page_content', '') or str(document)
        else:
            document_text = str(document)
        
        # ì„¹ì…˜ íŒ¨í„´: ã€ì„¹ì…˜ëª…ã€‘
        section_pattern = re.compile(r'ã€([^ã€‘]+)ã€‘')
        # í•­ëª© íŒ¨í„´: â€¢ ë˜ëŠ” - ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
        bullet_pattern = re.compile(r'^\s*[â€¢\-\*]\s+(.+)$', re.MULTILINE)
        
        sections = section_pattern.split(document_text)
        
        for i in range(1, len(sections), 2):
            if i + 1 < len(sections):
                section_name = sections[i].strip()
                section_content = sections[i + 1]
                
                # ì„¹ì…˜ ë‚´ìš©ì—ì„œ í•„ìˆ˜ í•­ëª© ì¶”ì¶œ
                required_items = bullet_pattern.findall(section_content)
                if required_items:
                    # ê° ì„¹ì…˜ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ (required_questions í‚¤ ì‚¬ìš©)
                    structured_sections[section_name] = {
                        'required_questions': required_items,
                        'required_actions': []  # ê¸°ë³¸ê°’
                    }
        
        return structured_sections

    def _build_conversation_text(self, conversation_log: List[Dict]) -> str:
        """ëŒ€í™” ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        conversation_parts = []
        for msg in conversation_log:
            speaker = "ì˜ì‚¬" if msg.get("role") == "doctor" else "í™˜ì"
            content = msg.get("content", "")
            conversation_parts.append(f"{speaker}: {content}")
        return "\n".join(conversation_parts)

    async def _process_evaluation_response(self, prompt: str, area_name: str, structured_sections: dict, stage: str = "") -> dict:
        """í‰ê°€ ì‘ë‹µ ì²˜ë¦¬ ê³µí†µ í•¨ìˆ˜"""
        try:
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm.ainvoke(messages)
            result_text = response.content
            
            print(f"[{stage}] LLM ì‘ë‹µ ì›ë¬¸:\n{result_text[:300]}...")
            
            # ê°œì„ ëœ JSON ì¶”ì¶œ ë° íŒŒì‹±
            # 1. ì½”ë“œ ë¸”ë¡ ë‚´ JSON ì°¾ê¸° ì‹œë„
            code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', result_text, re.DOTALL)
            if code_block_match:
                json_str = code_block_match.group(1)
            else:
                # 2. ì¼ë°˜ JSON íŒ¨í„´ ì°¾ê¸°
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', result_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                else:
                    error_msg = f"[{stage}] JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    print(error_msg)
                    raise ValueError(error_msg)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(json_str)
                print(f"[{stage}] JSON íŒŒì‹± ì„±ê³µ: {len(result)}ê°œ í•­ëª©")
            except json.JSONDecodeError as json_error:
                error_msg = f"[{stage}] JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}"
                print(error_msg)
                raise ValueError(error_msg)
            
            # ê²°ê³¼ ë³€í™˜ ë° ê²€ì¦
            guideline_evaluations = []
            for section_name in structured_sections.keys():
                section_result = result.get(section_name, {})
                
                # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³´ì •
                found_value = section_result.get("completed", False) or section_result.get("found", False)
                if isinstance(found_value, str):
                    found_value = found_value.lower() in ['true', 'yes', '1', 'found', 'completed']
                elif not isinstance(found_value, bool):
                    found_value = False
                
                evidence_value = section_result.get("evidence", [])
                if not isinstance(evidence_value, list):
                    # ë¬¸ìì—´ì¸ ê²½ìš° ë°°ì—´ë¡œ ë³€í™˜
                    if isinstance(evidence_value, str) and evidence_value:
                        evidence_value = [evidence_value]
                    else:
                        evidence_value = []
                
                # completedê°€ falseì¸ ê²½ìš° required_action ì¶”ê°€
                required_action = []
                if not found_value:
                    # RAG ê²°ê³¼ì—ì„œ í•´ë‹¹ í•­ëª©ì˜ required_questionsë‚˜ required_actions ê°€ì ¸ì˜¤ê¸°
                    for area_name_key, area_data in structured_sections.items():
                        if area_name_key == section_name:
                            # required_questionsê°€ ìˆìœ¼ë©´ ì¶”ê°€
                            if "required_questions" in area_data:
                                required_action.extend(area_data["required_questions"])
                            # required_actionsê°€ ìˆìœ¼ë©´ ì¶”ê°€  
                            if "required_actions" in area_data:
                                required_action.extend(area_data["required_actions"])
                            break

                guideline_evaluations.append({
                    "guideline_item": section_name,
                    "completed": found_value,
                    "evidence": evidence_value,
                    "required_action": required_action if not found_value else []
                })
            
            # í†µê³„ ê³„ì‚°
            total_guidelines = len(guideline_evaluations)
            completed_guidelines = sum(1 for item in guideline_evaluations if item["completed"])
            completion_rate = completed_guidelines / total_guidelines if total_guidelines > 0 else 0
            
            print(f"[{stage}] í‰ê°€ ì™„ë£Œ: {completed_guidelines}/{total_guidelines} ({completion_rate:.1%})")
            
            return {
                "area_name": area_name,
                "total_guidelines": total_guidelines,
                "completed_guidelines": completed_guidelines,
                "completion_rate": completion_rate,
                "guideline_evaluations": guideline_evaluations
            }
                
        except Exception as e:
            error_msg = f"[{stage}] í‰ê°€ ì‹¤íŒ¨: {e}"
            print(error_msg)
            logger.error(f"Traceback: {e}", exc_info=True)
            raise RuntimeError(error_msg)

    def _verify_evidence_exists(self, conversation_text: str, evaluation_result: dict) -> dict:
        """evidence arrayì˜ ê° í•­ëª©ì´ ì‹¤ì œ ëŒ€í™”ì— ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦"""
        
        verified_evaluations = []
        
        for item in evaluation_result['guideline_evaluations']:
            evidence_array = item['evidence']
            
            if not evidence_array:
                # evidenceê°€ ë¹„ì–´ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                verified_evaluations.append(item)
                continue
            
            # evidence arrayì˜ ê° í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ê²€ì¦
            verified_evidence = []
            
            for evidence_item in evidence_array:
                evidence_item = evidence_item.strip()
                if not evidence_item:
                    continue
                
                # "ì˜ì‚¬:" ë˜ëŠ” "í™˜ì:" ì ‘ë‘ì‚¬ ì œê±°í•˜ê³  ì‹¤ì œ contentë§Œ ì¶”ì¶œ
                content_to_check = evidence_item
                if evidence_item.startswith('ì˜ì‚¬: '):
                    content_to_check = evidence_item[4:]  # "ì˜ì‚¬: " ì œê±°
                elif evidence_item.startswith('í™˜ì: '):
                    content_to_check = evidence_item[4:]  # "í™˜ì: " ì œê±°
                
                # ì‹¤ì œ ëŒ€í™”ì—ì„œ í•´ë‹¹ contentê°€ ì •í™•íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if content_to_check and content_to_check in conversation_text:
                    verified_evidence.append(evidence_item)
                elif content_to_check:
                    print(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°œì–¸: {content_to_check[:50]}...")
                else:
                    verified_evidence.append(evidence_item)
            
            # ê²€ì¦ëœ evidenceê°€ ìˆìœ¼ë©´ completed ìœ ì§€, ì—†ìœ¼ë©´ falseë¡œ ë³€ê²½
            has_valid_evidence = len(verified_evidence) > 0
            final_completed = item["completed"] and has_valid_evidence
            
            verified_evaluations.append({
                "guideline_item": item["guideline_item"],
                "completed": final_completed,
                "evidence": verified_evidence,
                "required_action": item.get("required_action", []) if not final_completed else []
            })
            
            if not has_valid_evidence and item["completed"]:
                print(f"âŒ {item['guideline_item']}: ì˜ëª»ëœ evidence ì œê±°")
        
        # í†µê³„ ì¬ê³„ì‚°
        verified_result = evaluation_result.copy()
        verified_result['guideline_evaluations'] = verified_evaluations
        
        total_guidelines = len(verified_evaluations)
        completed_guidelines = sum(1 for item in verified_evaluations if item["completed"])
        completion_rate = completed_guidelines / total_guidelines if total_guidelines > 0 else 0
        
        verified_result.update({
            "completed_guidelines": completed_guidelines,
            "completion_rate": completion_rate
        })
        
        print(f"[ê²€ì¦ ì™„ë£Œ] {completed_guidelines}/{total_guidelines} ({completion_rate:.1%})")
        
        return verified_result

    def _calculate_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ê³„ì‚°"""
        if score >= 90:
            return "A+"
        elif score >= 85:
            return "A"
        elif score >= 80:
            return "B+"
        elif score >= 75:
            return "B"
        elif score >= 70:
            return "C+"
        elif score >= 65:
            return "C"
        else:
            return "F"

    async def _evaluate_ser_emotions(self, conversation_log: List[Dict]) -> Dict:
        """3ë‹¨ê³„: SER ê°ì • ë¶„ì„ í‰ê°€ - Kind ë†’ì•„ì•¼ í•˜ê³  ë¬¸ë§¥ì— ë§ëŠ” ê°ì •ì¸ì§€ LLMì´ íŒë‹¨"""
        print(f"ğŸ­ SER ê°ì • ë¶„ì„ í‰ê°€ ì‹œì‘")
        
        # SER ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
        emotion_analysis = self._analyze_conversation_emotions(conversation_log)
        
        if emotion_analysis["total_analyzed_utterances"] == 0:
            return {
                "ser_score": 5,  # ì¤‘ê°„ ì ìˆ˜
                "emotion_analysis": "ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "ser_strengths": [],
                "ser_improvements": ["ìŒì„± ê°ì • ë¶„ì„ ë°ì´í„° í™•ë³´ í•„ìš”"],
                "detailed_feedback": "SER ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
            }
        
        # ëŒ€í™” í…ìŠ¤íŠ¸ì™€ ê°ì • ë°ì´í„° êµ¬ì„±
        conversation_text = self._build_conversation_text(conversation_log)
        emotion_summary = self._generate_emotion_summary(emotion_analysis)
        
        # ê° ì˜ì‚¬ ë°œí™”ë³„ ê°ì • ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì •ë³´
        emotion_details = ""
        for emotion_data in emotion_analysis["doctor_emotions"]:
            emotion_details += f"ë°œí™”: \"{emotion_data['text']}\"\n"
            emotion_details += f"  â†’ ê°ì •: {emotion_data['predicted_emotion']} (ì‹ ë¢°ë„: {emotion_data['confidence']:.2f})\n"
            emotion_details += f"  â†’ ê°ì • ì ìˆ˜: {emotion_data['emotion_scores']}\n\n"
        
        ser_prompt = f"""
ë‹¹ì‹ ì€ ì˜ë£Œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜ì‚¬ì˜ ìŒì„± ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ í‰ê°€í•˜ì„¸ìš”.

ã€ì „ì²´ ëŒ€í™”ã€‘:
{conversation_text}

ã€ê°ì • ë¶„ì„ í†µê³„ã€‘:
{emotion_summary}

ã€ë°œí™”ë³„ ìƒì„¸ ê°ì • ë¶„ì„ã€‘:
{emotion_details}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì˜ì‚¬ì˜ ê°ì •ì  ì ì ˆì„±ì„ í‰ê°€í•˜ì„¸ìš”:

ã€í‰ê°€ ê¸°ì¤€ã€‘:
1. **Kind(ì¹œì ˆí•¨) ë¹„ìœ¨**: ì˜ë£Œì§„ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì¹œì ˆí•´ì•¼ í•¨ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
2. **ìƒí™©ë³„ ê°ì • ì ì ˆì„±**: 
   - í™˜ìê°€ ê±±ì •ì„ í‘œí˜„í•  ë•Œ â†’ Anxious(ê³µê°)ê°€ ì ì ˆí•  ìˆ˜ ìˆìŒ
   - ì •ë³´ ì „ë‹¬/ì„¤ëª… ì‹œ â†’ Kind(ì¹œì ˆí•¨)ê°€ ì ì ˆ
   - ì§„ì°°/ê²€ì‚¬ ì‹œ â†’ ì•½ê°„ì˜ Dry(ê±´ì¡°í•¨)ëŠ” ì „ë¬¸ì„±ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ
3. **ê°ì • ì¼ê´€ì„±**: ê¸‰ê²©í•œ ê°ì • ë³€í™”ê°€ ìˆëŠ”ì§€
4. **í™˜ì ìƒí™© ê³ ë ¤**: í™˜ìì˜ ìƒíƒœ/ë§ì— ë§ëŠ” ê°ì •ì¸ì§€

ã€ì ìˆ˜ ê¸°ì¤€ã€‘:
- Kind ë¹„ìœ¨ì´ 70% ì´ìƒ: 8-10ì 
- Kind ë¹„ìœ¨ì´ 50-70%: 6-8ì   
- Kind ë¹„ìœ¨ì´ 30-50%: 4-6ì 
- Kind ë¹„ìœ¨ì´ 30% ë¯¸ë§Œ: 1-4ì 
- ë‹¨, ìƒí™©ì— ë§ëŠ” Anxiousë‚˜ ì ì ˆí•œ DryëŠ” ê°€ì  ìš”ì†Œ

1-10ì ìœ¼ë¡œ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.

JSON ì‘ë‹µ:
{{
    "ser_score": ì ìˆ˜(1-10),
    "kind_ratio_assessment": "Kind ë¹„ìœ¨ì— ëŒ€í•œ í‰ê°€",
    "contextual_appropriateness": "ìƒí™©ë³„ ê°ì • ì ì ˆì„± í‰ê°€", 
    "emotional_consistency": "ê°ì • ì¼ê´€ì„± í‰ê°€",
    "patient_consideration": "í™˜ì ìƒí™© ê³ ë ¤ë„ í‰ê°€",
    "ser_strengths": ["ê°ì •ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì ë“¤"],
    "ser_improvements": ["ê°ì •ì ìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•œ ì ë“¤"],
    "detailed_feedback": "ì¢…í•©ì ì¸ ê°ì • í‰ê°€ í”¼ë“œë°±"
}}"""

        try:
            messages = [SystemMessage(content=ser_prompt)]
            response = await self.llm.ainvoke(messages)
            result_text = response.content
            
            print(f"[SER] LLM ì‘ë‹µ ì›ë¬¸:\n{result_text[:300]}...")
            
            # JSON íŒŒì‹±
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                ser_result = json.loads(json_str)
                print(f"[SER] JSON íŒŒì‹± ì„±ê³µ")
            else:
                print(f"[SER] JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                ser_result = {
                    "ser_score": 6,
                    "kind_ratio_assessment": "Kind ë¹„ìœ¨ ë¶„ì„ ì‹¤íŒ¨",
                    "contextual_appropriateness": "ìƒí™©ë³„ ì ì ˆì„± ë¶„ì„ ì‹¤íŒ¨",
                    "emotional_consistency": "ê°ì • ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨", 
                    "patient_consideration": "í™˜ì ê³ ë ¤ë„ ë¶„ì„ ì‹¤íŒ¨",
                    "ser_strengths": ["ê¸°ë³¸ ê°ì • ë¶„ì„ ì™„ë£Œ"],
                    "ser_improvements": ["SER í‰ê°€ ê°œì„  í•„ìš”"],
                    "detailed_feedback": "SER ë¶„ì„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                }
            
            # ê°ì • í†µê³„ ë°ì´í„° ì¶”ê°€
            ser_result["emotion_statistics"] = emotion_analysis["emotion_statistics"]
            ser_result["total_analyzed_utterances"] = emotion_analysis["total_analyzed_utterances"]
            
            print(f"âœ… SER ê°ì • ë¶„ì„ í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {ser_result.get('ser_score', 6):.1f}ì ")
            
            return ser_result
            
        except Exception as e:
            print(f"âŒ SER ê°ì • ë¶„ì„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                "ser_score": 5,
                "kind_ratio_assessment": "í‰ê°€ ì˜¤ë¥˜",
                "contextual_appropriateness": "í‰ê°€ ì˜¤ë¥˜",
                "emotional_consistency": "í‰ê°€ ì˜¤ë¥˜",
                "patient_consideration": "í™˜ì ìƒí™© ê³ ë ¤ë„ í‰ê°€", 
                "ser_strengths": ["ê¸°ë³¸ í‰ê°€ ì™„ë£Œ"],
                "ser_improvements": ["SER í‰ê°€ ì˜¤ë¥˜ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"],
                "detailed_feedback": f"SER í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "emotion_statistics": emotion_analysis.get("emotion_statistics", {}),
                "total_analyzed_utterances": emotion_analysis.get("total_analyzed_utterances", 0)
            }

    def _analyze_conversation_emotions(self, conversation_log: List[Dict]) -> Dict:
        """ëŒ€í™”ì—ì„œ ì˜ì‚¬ì˜ ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘ ë° ë¶„ì„"""
        doctor_emotions = []
        emotion_stats = {"Kind": 0, "Anxious": 0, "Dry": 0, "total_utterances": 0}
        
        for msg in conversation_log:
            if msg.get("role") == "doctor" and msg.get("emotion"):
                emotion_data = msg.get("emotion")
                predicted_emotion = emotion_data.get("predicted_emotion")
                confidence = emotion_data.get("confidence", 0)
                emotion_scores = emotion_data.get("emotion_scores", {})
                
                doctor_emotions.append({
                    "text": msg.get("content", ""),
                    "predicted_emotion": predicted_emotion,
                    "confidence": confidence,
                    "emotion_scores": emotion_scores
                })
                
                # í†µê³„ ìˆ˜ì§‘
                if predicted_emotion in emotion_stats:
                    emotion_stats[predicted_emotion] += 1
                emotion_stats["total_utterances"] += 1
        
        # ì „ì²´ ê°ì • ë¹„ìœ¨ ê³„ì‚°
        if emotion_stats["total_utterances"] > 0:
            for emotion in ["Kind", "Anxious", "Dry"]:
                emotion_stats[f"{emotion}_ratio"] = emotion_stats[emotion] / emotion_stats["total_utterances"]
        
        return {
            "doctor_emotions": doctor_emotions,
            "emotion_statistics": emotion_stats,
            "total_analyzed_utterances": len(doctor_emotions)
        }

    def _generate_emotion_summary(self, emotion_analysis: Dict) -> str:
        """ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½ í…ìŠ¤íŠ¸ë¡œ ìƒì„±"""
        stats = emotion_analysis.get("emotion_statistics", {})
        doctor_emotions = emotion_analysis.get("doctor_emotions", [])
        
        if not doctor_emotions:
            return "ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        total = stats.get("total_utterances", 0)
        if total == 0:
            return "ë¶„ì„ ê°€ëŠ¥í•œ ì˜ì‚¬ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°ì •ë³„ ë¹„ìœ¨
        kind_ratio = stats.get("Kind_ratio", 0)
        anxious_ratio = stats.get("Anxious_ratio", 0) 
        dry_ratio = stats.get("Dry_ratio", 0)
        
        # ì£¼ìš” ê°ì • íŒŒì•…
        dominant_emotion = "Kind"
        if anxious_ratio > kind_ratio and anxious_ratio > dry_ratio:
            dominant_emotion = "Anxious"
        elif dry_ratio > kind_ratio and dry_ratio > anxious_ratio:
            dominant_emotion = "Dry"
        
        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
        avg_confidence = sum(e.get("confidence", 0) for e in doctor_emotions) / len(doctor_emotions)
        
        summary = f"""ì´ {total}ê°œ ë°œí™” ë¶„ì„:
- Kind (ì¹œì ˆí•¨): {kind_ratio:.1%}
- Anxious (ë¶ˆì•ˆí•¨): {anxious_ratio:.1%} 
- Dry (ê±´ì¡°í•¨): {dry_ratio:.1%}
- ì£¼ìš” ê°ì •: {dominant_emotion}
- í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}

ê°ì •ë³„ ë°œí™” ì˜ˆì‹œ:"""
        
        # ê° ê°ì •ë³„ ëŒ€í‘œ ë°œí™” 1ê°œì”© ì¶”ê°€
        for emotion in ["Kind", "Anxious", "Dry"]:
            emotion_examples = [e for e in doctor_emotions if e.get("predicted_emotion") == emotion]
            if emotion_examples:
                best_example = max(emotion_examples, key=lambda x: x.get("confidence", 0))
                summary += f"\n- {emotion}: \"{best_example.get('text', '')[:30]}...\" (ì‹ ë¢°ë„: {best_example.get('confidence', 0):.2f})"
        
        return summary

    def generate_evaluation_markdown(self, evaluation_result: Dict) -> str:
        """í‰ê°€ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìƒì„±"""
        try:
            langgraph_analysis = evaluation_result.get("langgraph_text_analysis", {})
            scores = langgraph_analysis.get("scores", {})
            feedback = langgraph_analysis.get("feedback", {})
            detailed_analysis = langgraph_analysis.get("detailed_analysis", {})
            conversation_summary = langgraph_analysis.get("conversation_summary", {})
            
            # ê¸°ë³¸ ì •ë³´
            user_id = evaluation_result.get("user_id", "Unknown")
            scenario_id = evaluation_result.get("scenario_id", "Unknown")
            duration = evaluation_result.get("duration_minutes", 0)
            
            markdown_content = f"""# CPX ì‹¤ìŠµ í‰ê°€ ê²°ê³¼

## 1. ì¢…í•© ì ìˆ˜
- **ì´ì **: {scores.get('total_score', 0):.1f}ì  / 100ì  ({scores.get('grade', 'F')}ë“±ê¸‰)
- **í•„ìˆ˜í•­ëª© ë‹¬ì„±ë¥ **: {scores.get('completion_rate', 0):.1%}
- **ì§„ë£Œ ìˆ˜í–‰ë„**: {scores.get('quality_score', 0):.1f}ì  / 10ì 

## 2. ê° ë‹¨ê³„ë³„ ê²°ê³¼

### í•„ìˆ˜í•­ëª© ë‹¬ì„± í‰ê°€
"""
            
            # ì™„ì„±ë„ í‰ê°€ ìƒì„¸ ë‚´ìš©
            completeness = detailed_analysis.get("completeness", {})
            areas_evaluation = completeness.get("areas_evaluation", {})
            
            for area_key, area_data in areas_evaluation.items():
                area_name = area_data.get("area_name", area_key)
                completion_rate = area_data.get("completion_rate", 0)
                completed_guidelines = area_data.get("completed_guidelines", 0)
                total_guidelines = area_data.get("total_guidelines", 0)
                
                # ì™„ì„±ë„ ìƒíƒœ
                if completion_rate >= 0.8:
                    status = "ìš°ìˆ˜"
                elif completion_rate >= 0.5:
                    status = "ë³´í†µ"
                else:
                    status = "ë¯¸í¡"
                
                markdown_content += f"- **{area_name}**: {completion_rate:.1%} ({status})\n"

            # ì˜ì‚¬ì†Œí†µ í‰ê°€ ìƒì„¸ ë‚´ìš©
            quality = detailed_analysis.get("quality", {})
            markdown_content += f"""
### ì§„ë£Œ ìˆ˜í–‰ í‰ê°€
- **ì˜í•™ì  ì •í™•ì„±**: {quality.get('medical_accuracy', 0):.1f}/10ì 
- **ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥**: {quality.get('communication_efficiency', 0):.1f}/10ì 
- **ì „ë¬¸ì„±**: {quality.get('professionalism', 0):.1f}/10ì 
- **ì‹œë‚˜ë¦¬ì˜¤ ì í•©ì„±**: {quality.get('scenario_appropriateness', 0):.1f}/10ì 
"""

            # SER ê°ì • ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            ser_evaluation = detailed_analysis.get("comprehensive", {}).get("ser_evaluation", {})
            if ser_evaluation:
                markdown_content += f"""
### ê°ì • í‰ê°€
- ** ê°ì • ì ìˆ˜**: {ser_evaluation.get('ser_score', 0):.1f}/10ì 

**ìƒì„¸ í”¼ë“œë°±**: {ser_evaluation.get('detailed_feedback', 'ìŒì„± í†¤ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.')}
"""

            # ì¢…í•© ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­
            strengths = feedback.get("strengths", [])
            improvements = feedback.get("improvements", [])
            
            markdown_content += "\n## 3. ì¢…í•© ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­\n"
            
            if strengths:
                markdown_content += "\n### ìš°ìˆ˜í•œ ì \n"
                for strength in strengths[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                    markdown_content += f"- {strength}\n"
            
            if improvements:
                markdown_content += "\n### ê°œì„  í•„ìš”ì‚¬í•­\n"
                for improvement in improvements[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                    markdown_content += f"- {improvement}\n"
            
            # í•™ìŠµ ê¶Œì¥ì‚¬í•­ (ê°„ë‹¨í•˜ê²Œ)
            markdown_content += f"""
### í•™ìŠµ ê¶Œì¥ì‚¬í•­
1. ì™„ì„±ë„ê°€ ë‚®ì€ ì˜ì—­({', '.join([area for area, data in areas_evaluation.items() if data.get('completion_rate', 0) < 0.5])}) ì§‘ì¤‘ í•™ìŠµ
2. ì§ˆë¬¸ ê¸°ë²• ë° ì˜ì‚¬ì†Œí†µ ìŠ¤í‚¬ í–¥ìƒ
3. ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°˜ë³µ ì—°ìŠµ
"""
            
            return markdown_content
            
        except Exception as e:
            print(f"âŒ ë§ˆí¬ë‹¤ìš´ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"# CPX í‰ê°€ ê²°ê³¼\n\ní‰ê°€ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜ ë‚´ìš©: {str(e)}"

    # ================================
    # 6. ë°ì´í„°ë² ì´ìŠ¤ ë° íŒŒì¼ ê´€ë¦¬
    # ================================
    
    async def _update_cpx_database_after_evaluation(self, session_id: str, evaluation_result: dict):
        """í‰ê°€ ì™„ë£Œ í›„ CPX Detailsë§Œ ì—…ë°ì´íŠ¸"""
        try:
            session = self.session_data[session_id]
            result_id = session.get("result_id")
            user_id = session["user_id"]
            
            if result_id is None:
                print(f"âŒ [{session_id}] result_idê°€ Noneì…ë‹ˆë‹¤. CPX ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
                print(f"âŒ [{session_id}] ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. JSON íŒŒì¼ë§Œ ì €ì¥ë©ë‹ˆë‹¤.")
                return
            
            # CPX Details ë° Evaluations ì—…ë°ì´íŠ¸
            db_gen = get_db()
            db = await db_gen.__anext__()
            try:
                cpx_service = CpxService(db)
                
                # CPX Details ì—…ë°ì´íŠ¸ (ì‹œìŠ¤í…œ í‰ê°€ ë°ì´í„°)
                await cpx_service.update_cpx_details(
                    result_id=result_id,
                    user_id=int(user_id),
                    system_evaluation_data=evaluation_result
                )
                
                # CPX Results ìƒíƒœ ì—…ë°ì´íŠ¸ (ìë™ í‰ê°€ ì™„ë£Œ)
                await cpx_service.update_cpx_result_status(
                    result_id=result_id,
                    new_status="ì™„ë£Œ"
                )
                
                print(f"âœ… CPX Details ë° Evaluations ì—…ë°ì´íŠ¸ ì™„ë£Œ: result_id={result_id}, session_id={session_id}")
                
            finally:
                await db_gen.aclose()
                
        except Exception as e:
            print(f"âŒ CPX Details ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

