from typing import Dict, List, Optional, TypedDict, Annotated
from datetime import datetime
from pathlib import Path
import json
import asyncio
import aiofiles
import logging
import os
import sys
import re
from collections import Counter

# LangGraph ê´€ë ¨ import
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage as AnyMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# CPX ê´€ë ¨ import
from services.cpx_service import CpxService
from core.config import get_db

# CPX í‰ê°€ ìƒíƒœ ì •ì˜ (Multi-Step Reasoning ì „ìš©)
class CPXEvaluationState(TypedDict):
    """CPX í‰ê°€ ìƒíƒœ ì •ì˜ - Multi-Step Reasoning ì „ìš©"""
    # ìž…ë ¥ ë°ì´í„°
    user_id: str
    scenario_id: str
    conversation_log: List[Dict]
    
    # Multi-Step Reasoning ê²°ê³¼ë“¤ (í•µì‹¬)
    medical_context_analysis: Optional[Dict]
    question_intent_analysis: Optional[Dict]
    completeness_assessment: Optional[Dict]
    quality_evaluation: Optional[Dict]
    appropriateness_validation: Optional[Dict]
    
    # ì¢…í•© í‰ê°€ ê²°ê³¼
    comprehensive_evaluation: Optional[Dict]
    
    # ìµœì¢… ê²°ê³¼
    final_scores: Optional[Dict]
    feedback: Optional[Dict]
    
    # ë©”íƒ€ë°ì´í„°
    evaluation_metadata: Optional[Dict]
    
    # ë©”ì‹œì§€ ì¶”ì 
    messages: Annotated[List[AnyMessage], add_messages]

class EvaluationService:
    def __init__(self):
        """CPX í‰ê°€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        self.evaluation_checklists = self._load_evaluation_checklists()
        
        # ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ëŠ” ì œê±°í•˜ê³  JSON ê¸°ë°˜ìœ¼ë¡œ í†µí•©
        
        self.session_data = {}  # ì„¸ì…˜ë³„ í‰ê°€ ë°ì´í„°
        
        # í‰ê°€ ê²°ê³¼ ì €ìž¥ ë””ë ‰í„°ë¦¬
        self.evaluation_dir = Path("evaluation_results")
        self.evaluation_dir.mkdir(parents=True, exist_ok=True)
        
        # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê´€ë ¨
        self.llm = None
        self.workflow = None
        self._initialize_langgraph_components()
        
        # RAG ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.guideline_retriever = None
        self._initialize_guideline_retriever()
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì ìš© ìš”ì†Œë“¤ ì •ì˜
        self.scenario_applicable_elements = self._initialize_scenario_elements()

    def _initialize_guideline_retriever(self):
        """RAG ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        try:
            # RAG ë””ë ‰í† ë¦¬ì˜ guideline_retrieverë¥¼ import
            rag_path = Path(__file__).parent.parent / "RAG"
            sys.path.append(str(rag_path))
            
            from guideline_retriever import GuidelineRetriever
            
            # ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
            index_path = rag_path / "faiss_guideline_index"
            self.guideline_retriever = GuidelineRetriever(index_path=str(index_path))
            
            if self.guideline_retriever.vectorstore:
                print("âœ… RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸ RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
                self.guideline_retriever = None
                
        except Exception as e:
            print(f"âŒ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.guideline_retriever = None

    def _initialize_scenario_elements(self) -> Dict:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì ìš© ìš”ì†Œë“¤ ì´ˆê¸°í™” - scenarios/ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ"""
        scenario_elements = {}
        scenario_dir = Path("scenarios")
        
        if not scenario_dir.exists():
            print("âš ï¸ scenarios ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        for json_file in scenario_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                scenario_info = data.get("scenario_info", {})
                category = scenario_info.get("category", "unknown")
                
                # ì‹œë‚˜ë¦¬ì˜¤ IDë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì˜ˆ: memory_impairment)
                scenario_id = category.replace(" ", "_").lower()
                
                scenario_elements[scenario_id] = {
                    "name": category,
                    "description": scenario_info.get("case_presentation", ""),
                    "patient_name": scenario_info.get("patient_name", ""),
                    "primary_diagnosis": scenario_info.get("primary_diagnosis", ""),
                    "differential_diagnoses": scenario_info.get("differential_diagnoses", []),
                    "applicable_areas": [
                        "history_taking",
                        "physical_examination", 
                        "patient_education"
                    ]
                }
                
                print(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ: {category} ({json_file.name})")
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
            except Exception as e:
                print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
        
        return scenario_elements

    def _load_evaluation_checklists(self) -> Dict:
        """ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        checklists = {}
        checklist_dir = Path("evaluation_checklists")
        
        if not checklist_dir.exists():
            print("âš ï¸ evaluation_checklists ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return checklists
        
        for json_file in checklist_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    category = data.get("category")
                    if category:
                        checklists[category] = data
                        print(f"âœ… í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ: {category}")
                    else:
                        print(f"âš ï¸ {json_file.name}ì—ì„œ category í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({json_file.name}): {e}")
            except Exception as e:
                print(f"âŒ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì˜¤ë¥˜ ({json_file.name}): {e}")
        
        return checklists

    def get_evaluation_checklist(self, category: str) -> Optional[Dict]:
        """ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.evaluation_checklists.get(category)

    def _get_scenario_category(self, scenario_id: str) -> Optional[str]:
        """ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë“œ"""
        try:
            scenario_path = Path(f"scenarios/neurology_dementia_case.json")  # í˜„ìž¬ëŠ” í•˜ë‚˜ì˜ ì‹œë‚˜ë¦¬ì˜¤ë§Œ
            if not scenario_path.exists():
                return None
            
            with open(scenario_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("scenario_info", {}).get("category")
        except Exception as e:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_applicable_categories(self, checklist: Dict) -> List[Dict]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ì—ì„œ ì ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ë“¤ ì¶”ì¶œ"""
        categories = []
        
        for area_name, area_data in checklist.get("evaluation_areas", {}).items():
            for subcat_id, subcat_data in area_data.get("subcategories", {}).items():
                # applicableì´ Falseì¸ ê²½ìš° ì œì™¸
                if subcat_data.get("applicable", True):
                    required_elements = subcat_data.get("required_questions", subcat_data.get("required_actions", []))
                    categories.append({
                        "category_id": subcat_id,
                        "name": subcat_data["name"],
                        "required_questions": required_elements,
                        "required_elements": required_elements,  # ì¶”ê°€: LangGraphì—ì„œ ì‚¬ìš©
                        "weight": subcat_data.get("weight", 0.1),
                        "area": area_name
                    })
        
        return categories

    def _create_default_completeness_result(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """ê¸°ë³¸ ì™„ì„±ë„ ê²°ê³¼ ìƒì„± (ì˜¤ë¥˜ ì‹œ)"""
        completeness = {
            "category_completeness": {},
            "overall_completeness_score": 0.0,
            "missing_items": [],
            "medical_completeness_analysis": "ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
        }
        
        return {
            **state,
            "completeness_assessment": completeness,
            "messages": state["messages"] + [HumanMessage(content="Step 3: ê¸°ë³¸ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ")]
        }

    async def start_evaluation_session(self, user_id: str, scenario_id: str, result_id: Optional[int] = None) -> str:
        """í‰ê°€ ì„¸ì…˜ ì‹œìž‘"""
        session_id = f"{user_id}_{scenario_id}_{int(datetime.now().timestamp())}"
        
        self.session_data[session_id] = {
            "user_id": user_id,
            "scenario_id": scenario_id,
            "result_id": result_id,  # CPX result_id ì €ìž¥
            "start_time": datetime.now(),
            "conversation_entries": [],  # ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„°
            # "audio_files": [],  # ìž„ì‹œ ì €ìž¥ëœ wav íŒŒì¼ ê²½ë¡œë“¤
            "status": "active"
        }
        
        return session_id

    async def add_conversation_entry(self, session_id: str, audio_file_path: str, 
                                   text: str, speaker_role: str, emotion_analysis: Optional[Dict] = None) -> Dict:
        """ì‹¤ì‹œê°„ ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€ (SER ê²°ê³¼ëŠ” queueì—ì„œ ì „ë‹¬ë°›ìŒ)"""
        if session_id not in self.session_data:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            timestamp = datetime.now()
            
            # SER ê²°ê³¼ ë¡œê¹… (queueì—ì„œ ì „ë‹¬ë°›ì€ ê²½ìš°)
            if emotion_analysis:
                print(f"ðŸŽ­ [{session_id}] ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹ : {emotion_analysis['predicted_emotion']} ({emotion_analysis['confidence']:.2f})")
            
            # ëŒ€í™” ì—”íŠ¸ë¦¬ ìƒì„±
            conversation_entry = {
                "timestamp": timestamp.isoformat(),
                "text": text,
                "emotion": emotion_analysis,
                "speaker_role": speaker_role,  # "doctor" (ì˜ì‚¬) or "patient" (í™˜ìž)
                "audio_file_path": audio_file_path
            }
            
            # ì„¸ì…˜ ë°ì´í„°ì— ì¶”ê°€
            session = self.session_data[session_id]
            session["conversation_entries"].append(conversation_entry)
            if "audio_files" not in session:
                session["audio_files"] = []
            session["audio_files"].append(audio_file_path)
            
            print(f"ðŸ“ [{session_id}] ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€: {speaker_role} - {text[:50]}...")
            
            # í‰ê°€ ì™„ë£Œ í›„ ìž„ì‹œ WAV íŒŒì¼ë“¤ ì‚­ì œ
            try:
                await self._cleanup_audio_files(audio_file_path)
            except Exception as e:
                print(f"âŒ [{audio_file_path}] ìž„ì‹œ WAV íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "entry": conversation_entry,
                "total_entries": len(session["conversation_entries"])
            }
            
        except Exception as e:
            print(f"âŒ [{session_id}] ëŒ€í™” ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def end_evaluation_session(self, session_id: str) -> Dict:
        """í‰ê°€ ì„¸ì…˜ ì¢…ë£Œ ë° ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        if session_id not in self.session_data:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        session = self.session_data[session_id]
        session["end_time"] = datetime.now()
        session["status"] = "completed"
        
        # ì¢…í•© í‰ê°€ ì‹¤í–‰
        evaluation_result = await self._comprehensive_evaluation(session_id, session)
        
        # CPX ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        await self._update_cpx_database_after_evaluation(session_id, evaluation_result)
        
        return evaluation_result

    def get_session_summary(self, user_id: str) -> list:
        """ì‚¬ìš©ìžì˜ ì„¸ì…˜ ìš”ì•½"""
        return [
            {
                "session_id": sid,
                "scenario_id": data["scenario_id"],
                "date": data["start_time"].strftime("%Y-%m-%d %H:%M"),
                "status": data["status"]
            }
            for sid, data in self.session_data.items()
            if data["user_id"] == user_id
        ]

    async def _comprehensive_evaluation(self, session_id: str, session: Dict) -> Dict:
        """ì¢…í•©ì ì¸ ì„¸ì…˜ í‰ê°€ ìˆ˜í–‰ (SER + LangGraph í†µí•©)"""
        print(f"ðŸ” [{session_id}] ì¢…í•© í‰ê°€ ì‹œìž‘...")
        
        # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ (ìƒˆë¡œìš´ ëŒ€í™” ë°ì´í„° ì‚¬ìš©)
        langgraph_analysis = None
        if self.llm and self.workflow:
            try:
                # ìƒˆë¡œìš´ conversation_entriesë¥¼ conversation_log í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                conversation_log = []
                for entry in session.get("conversation_entries", []):
                    conversation_log.append({
                        "role": entry["speaker_role"],
                        "content": entry["text"],
                        "timestamp": entry["timestamp"],
                        "emotion": entry.get("emotion")
                    })
                
                if conversation_log:  # ëŒ€í™” ë°ì´í„°ê°€ ìžˆëŠ” ê²½ìš°ì—ë§Œ í‰ê°€
                    langgraph_analysis = await self.evaluate_conversation_with_langgraph(
                        session["user_id"], 
                        session["scenario_id"], 
                        conversation_log
                    )
                    print(f"âœ… [{session_id}] LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ [{session_id}] ëŒ€í™” ë°ì´í„°ê°€ ì—†ì–´ LangGraph í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                
            except Exception as e:
                print(f"âŒ [{session_id}] LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
                langgraph_analysis = {"error": str(e)}
        
        # ì¢…í•© ê²°ê³¼ êµ¬ì„±
        evaluation_result = {
            "session_id": session_id,
            "user_id": session["user_id"],
            "scenario_id": session["scenario_id"],
            "start_time": session["start_time"].isoformat(),
            "end_time": session["end_time"].isoformat(),
            "duration_minutes": (session["end_time"] - session["start_time"]).total_seconds() / 60,
            
            # ìƒì„¸ ë¶„ì„ ê²°ê³¼
            "langgraph_text_analysis": langgraph_analysis,  # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼
            
            # ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„° (ê°ì • ë¶„ì„ í¬í•¨)
            "conversation_entries": [
                {
                    "timestamp": entry["timestamp"],
                    "text": entry["text"],
                    "speaker_role": entry["speaker_role"],
                    "emotion": entry.get("emotion"),
                    "audio_file": entry["audio_file_path"]
                }
                for entry in session.get("conversation_entries", [])
            ]
        }
        
        print(f"âœ… [{session_id}] ì¢…í•© í‰ê°€ ì™„ë£Œ")
        return evaluation_result

    async def _save_evaluation_result(self, session_id: str, result: Dict):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ìž¥"""
        try:
            # JSON íŒŒì¼ë¡œ ì €ìž¥
            json_path = self.evaluation_dir / f"{session_id}_evaluation.json"
            
            async with aiofiles.open(json_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(result, ensure_ascii=False, indent=2))
            
            print(f"ðŸ’¾ [{session_id}] í‰ê°€ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {json_path}")
            
        except Exception as e:
            print(f"âŒ [{session_id}] í‰ê°€ ê²°ê³¼ ì €ìž¥ ì‹¤íŒ¨: {e}")

    async def get_evaluation_result(self, session_id: str) -> Dict:
        """ì €ìž¥ëœ í‰ê°€ ê²°ê³¼ ì¡°íšŒ"""
        json_path = self.evaluation_dir / f"{session_id}_evaluation.json"
        
        if not json_path.exists():
            return {"error": "í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            async with aiofiles.open(json_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                return json.loads(content)
        except Exception as e:
            return {"error": f"í‰ê°€ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}"}

    async def evaluate_with_rag_guidelines(self, conversation_log: List[Dict], category: str) -> Dict:
        """
        RAG ê°€ì´ë“œë¼ì¸ì„ ì‚¬ìš©í•œ ëŒ€í™” í‰ê°€
        
        Args:
            conversation_log: ëŒ€í™” ë¡œê·¸
            category: í‰ê°€í•  ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "ê¸°ì–µë ¥ ì €í•˜")
            
        Returns:
            RAG ê¸°ë°˜ í‰ê°€ ê²°ê³¼
        """
        if not self.guideline_retriever:
            return {
                "error": "RAG ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "category": category,
                "completeness": 0.0
            }
        
        try:
            print(f"ðŸ” [{category}] RAG ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ í‰ê°€ ì‹œìž‘...")
            
            # ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€
            evaluation_result = self.guideline_retriever.evaluate_conversation_completeness(
                conversation_log, category
            )
            
            # ì¶”ê°€ ë¶„ì„ì„ ìœ„í•œ ì„¸ë¶€ ì •ë³´
            detailed_analysis = {
                "category": category,
                "overall_completeness": evaluation_result.get("overall_completeness", 0.0),
                "area_breakdown": evaluation_result.get("area_results", {}),
                "completed_count": len(evaluation_result.get("completed_items", [])),
                "missing_count": len(evaluation_result.get("missing_items", [])),
                "total_expected_items": len(evaluation_result.get("completed_items", [])) + len(evaluation_result.get("missing_items", [])),
                "completed_items": evaluation_result.get("completed_items", []),
                "missing_items": evaluation_result.get("missing_items", []),
                "evaluation_method": "rag_guideline",
                "timestamp": datetime.now().isoformat()
            }
            
            print(f"âœ… [{category}] RAG í‰ê°€ ì™„ë£Œ - ì™„ì„±ë„: {detailed_analysis['overall_completeness']:.2%}")
            
            return detailed_analysis
            
        except Exception as e:
            print(f"âŒ [{category}] RAG ê°€ì´ë“œë¼ì¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "category": category,
                "completeness": 0.0,
                "evaluation_method": "rag_guideline_error"
            }

    # =============================================================================
    # LangGraph ê¸°ë°˜ í…ìŠ¤íŠ¸ í‰ê°€ ê¸°ëŠ¥ (í†µí•©)
    # =============================================================================
    
    def _initialize_langgraph_components(self):
        """LangGraph ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # OpenAI API ì„¤ì •
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.llm = ChatOpenAI(
                    openai_api_key=api_key,
                    model_name="gpt-3.5-turbo",
                    temperature=0.3,
                    max_tokens=2000
                )
                
                # ì›Œí¬í”Œë¡œìš° ìƒì„±
                self.workflow = self._create_evaluation_workflow()
                print("âœ… LangGraph í…ìŠ¤íŠ¸ í‰ê°€ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ í‰ê°€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            print(f"âŒ LangGraph ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm = None
            self.workflow = None



    async def evaluate_conversation(self, user_id: str, scenario_id: str, conversation_log: List[Dict]) -> Dict:
        """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•œ CPX í‰ê°€ ì‹¤í–‰"""
        # ì´ˆê¸° ìƒíƒœ êµ¬ì„± (Multi-Step ì „ìš©)
        initial_state = CPXEvaluationState(
            user_id=user_id,
            scenario_id=scenario_id,
            conversation_log=conversation_log,
            medical_context_analysis=None,
            question_intent_analysis=None,
            completeness_assessment=None,
            quality_evaluation=None,
            appropriateness_validation=None,
            comprehensive_evaluation=None,
            final_scores=None,
            feedback=None,
            evaluation_metadata=None,
            messages=[]
        )
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            print(f"ðŸš€ [{user_id}] CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ì‹œìž‘")
            final_state = self.workflow.invoke(initial_state)
            
            # ê°„ë‹¨í•œ ëŒ€í™” ìš”ì•½ ì •ë³´ ìƒì„±
            student_questions = [msg for msg in conversation_log if msg.get("role") == "student"]
            conversation_summary = {
                "total_questions": len(student_questions),
                "duration_minutes": len(conversation_log) * 0.5
            }
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "evaluation_metadata": final_state.get("evaluation_metadata", {}),
                "scores": final_state.get("final_scores", {}),
                "feedback": final_state.get("feedback", {}),
                "conversation_summary": conversation_summary,
                "detailed_analysis": {
                    "medical_context": final_state.get("medical_context_analysis", {}),
                    "question_intent": final_state.get("question_intent_analysis", {}),
                    "completeness": final_state.get("completeness_assessment", {}),
                    "quality": final_state.get("quality_evaluation", {}),
                    "appropriateness": final_state.get("appropriateness_validation", {}),
                    "comprehensive": final_state.get("comprehensive_evaluation", {})
                },
                "evaluation_method": "6ë‹¨ê³„ ì˜í•™ì  ë¶„ì„",
                "system_info": {
                    "version": "v2.0",
                    "evaluation_steps": 6
                }
            }
            
            print(f"ðŸŽ‰ [{user_id}] CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            print(f"âŒ [{user_id}] í‰ê°€ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "user_id": user_id,
                "scenario_id": scenario_id,
                "evaluation_date": datetime.now().isoformat()
            }

    # ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    async def evaluate_conversation_with_langgraph(self, user_id: str, scenario_id: str, conversation_log: List[Dict]) -> Dict:
        """LangGraphë¥¼ ì‚¬ìš©í•œ ëŒ€í™” í…ìŠ¤íŠ¸ í‰ê°€ (í˜¸í™˜ì„± ìœ ì§€)"""
        return await self.evaluate_conversation(user_id, scenario_id, conversation_log)

    def _create_evaluation_workflow(self):
        """CPX í‰ê°€ ì›Œí¬í”Œë¡œìš° ìƒì„± (3ë‹¨ê³„ ëª…í™•í™”)"""
        workflow = StateGraph(CPXEvaluationState)

        workflow.add_node("initialize", self._initialize_evaluation)
        workflow.add_node("step1_rag_completeness", self._evaluate_rag_completeness)
        workflow.add_node("step2_quality_assessment", self._evaluate_quality_assessment)
        workflow.add_node("step3_comprehensive_results", self._generate_comprehensive_results)

        workflow.set_entry_point("initialize")
        workflow.add_edge("initialize", "step1_rag_completeness")
        workflow.add_edge("step1_rag_completeness", "step2_quality_assessment")
        workflow.add_edge("step2_quality_assessment", "step3_comprehensive_results")
        workflow.add_edge("step3_comprehensive_results", END)

        return workflow.compile()

    def _initialize_evaluation(self, state: CPXEvaluationState) -> CPXEvaluationState:
        print(f"ðŸŽ¯ [{state['user_id']}] CPX í‰ê°€ ì´ˆê¸°í™” - ì‹œë‚˜ë¦¬ì˜¤: {state['scenario_id']}")
        
        metadata = {
            "user_id": state["user_id"],
            "scenario_id": state["scenario_id"],
            "evaluation_date": datetime.now().isoformat(),
            "conversation_duration_minutes": len(state["conversation_log"]) * 0.5,
            "voice_recording_path": "s3ë¡œ ì €ìž¥",
            "conversation_transcript": json.dumps(state["conversation_log"], ensure_ascii=False)
        }
        
        return {
            **state,
            "evaluation_metadata": metadata,
            "messages": [HumanMessage(content="CPX í‰ê°€ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤.")]
        }



    def _evaluate_rag_completeness(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ (ë³‘ë ¥ì²­ì·¨, ì‹ ì²´ì§„ì°°, í™˜ìžêµìœ¡)"""
        print(f"ðŸ“‹ [{state['user_id']}] 1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì‹œìž‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        scenario_id = state["scenario_id"]
        
        # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë“œ
        scenario_category = self._get_scenario_category(scenario_id)
        rag_data = {"category": scenario_category or scenario_id}
        
        # 3ê°œ ì˜ì—­ë³„ RAG ê¸°ë°˜ í‰ê°€
        areas_evaluation = {
            "history_taking": self._evaluate_single_area(conversation_text, "ë³‘ë ¥ ì²­ì·¨", rag_data),
            "physical_examination": self._evaluate_single_area(conversation_text, "ì‹ ì²´ ì§„ì°°", rag_data),
            "patient_education": self._evaluate_single_area(conversation_text, "í™˜ìž êµìœ¡", rag_data)
        }
        
        # ì „ì²´ ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
        area_scores = [area.get("area_score", 0) for area in areas_evaluation.values()]
        overall_completeness = sum(area_scores) / (len(area_scores) * 10) if area_scores else 0  # 0-1 ìŠ¤ì¼€ì¼
        
        # ì „ì²´ ì™„ë£Œ/ëˆ„ë½ í•­ëª© ìˆ˜ì§‘
        all_completed_items = []
        all_missing_items = []
        for area_data in areas_evaluation.values():
            all_completed_items.extend(area_data.get("completed_items", []))
            all_missing_items.extend(area_data.get("missing_items", []))
        
        rag_completeness_result = {
            "category": scenario_category or scenario_id,
            "overall_completeness": round(overall_completeness, 2),
            "areas_evaluation": areas_evaluation,
            "total_completed_items": len(all_completed_items),
            "total_missing_items": len(all_missing_items),
            "completed_items": all_completed_items,
            "missing_items": all_missing_items,
            "evaluation_method": "rag_three_areas"
        }
        
        print(f"âœ… [{state['user_id']}] 1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ - ì™„ì„±ë„: {overall_completeness:.2%}")
        
        return {
            **state,
            "completeness_assessment": rag_completeness_result,
            "messages": state["messages"] + [HumanMessage(content="1ë‹¨ê³„: RAG ê¸°ë°˜ ì™„ì„±ë„ í‰ê°€ ì™„ë£Œ")]
        }

    
    def _evaluate_single_area(self, conversation_text: str, area_name: str, rag_data: Dict) -> Dict:
        """RAG ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ë‹¨ì¼ ì˜ì—­ í‰ê°€"""
        
        # ì˜ì—­ëª… ë§¤í•‘
        area_mapping = {
            "ë³‘ë ¥ ì²­ì·¨": "history_taking",
            "ì‹ ì²´ ì§„ì°°": "physical_examination", 
            "í™˜ìž êµìœ¡": "patient_education"
        }
        
        area_key = area_mapping.get(area_name, area_name)
        
        # RAGì—ì„œ í•´ë‹¹ ì˜ì—­ì˜ êµ¬ì²´ì  í‰ê°€ ê¸°ì¤€ ê°€ì ¸ì˜¤ê¸°
        if self.guideline_retriever and hasattr(self.guideline_retriever, 'vectorstore'):
            try:
                # ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                scenario_category = rag_data.get("category", "ê¸°ì–µë ¥ ì €í•˜")
                
                # í•´ë‹¹ ì˜ì—­ì˜ ì„¸ë¶€ ê¸°ì¤€ ê²€ìƒ‰
                area_query = f"{scenario_category} {area_name} í‰ê°€ ê¸°ì¤€"
                docs = self.guideline_retriever.vectorstore.similarity_search(area_query, k=3)
                
                if docs:
                    # ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì˜ì—­ ì •ë³´ ì¶”ì¶œ
                    area_guidelines = ""
                    for doc in docs:
                        if area_key in doc.page_content:
                            area_guidelines += doc.page_content + "\n"
                    
                    if area_guidelines:
                        area_prompt = f"""
ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¤ìŒ RAG ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ "{area_name}" ì˜ì—­ì„ í‰ê°€í•˜ì„¸ìš”.

ã€í•™ìƒ-í™˜ìž ëŒ€í™”ã€‘: {conversation_text}

ã€RAG ê°€ì´ë“œë¼ì¸ - {area_name} í‰ê°€ ê¸°ì¤€ã€‘:
{area_guidelines}

ìœ„ ê°€ì´ë“œë¼ì¸ì˜ êµ¬ì²´ì  í•­ëª©ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒì„ í‰ê°€í•˜ì„¸ìš”:
1. í•„ìˆ˜ ì§ˆë¬¸/í–‰ë™ë“¤ì„ ì–¼ë§ˆë‚˜ ìˆ˜í–‰í–ˆëŠ”ê°€
2. ê°€ì´ë“œë¼ì¸ì— ëª…ì‹œëœ ì„¸ë¶€ ì‚¬í•­ë“¤ì„ ë‹¤ë¤˜ëŠ”ê°€
3. ì˜í•™ì  ì •í™•ì„±ê³¼ ì²´ê³„ì„±ì„ ë³´ì˜€ëŠ”ê°€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "area_score": ì ìˆ˜(1-10),
    "completeness_level": "excellent/good/fair/poor",
    "completed_items": ["ê°€ì´ë“œë¼ì¸ ê¸°ì¤€ìœ¼ë¡œ ì™„ë£Œëœ í•­ëª©ë“¤"],
    "missing_items": ["ê°€ì´ë“œë¼ì¸ ê¸°ì¤€ìœ¼ë¡œ ëˆ„ë½ëœ í•­ëª©ë“¤"],
    "strengths": ["RAG ê¸°ì¤€ìœ¼ë¡œ ìž˜í•œ ì ë“¤"],
    "improvements": ["RAG ê¸°ì¤€ìœ¼ë¡œ ê°œì„  í•„ìš”í•œ ì ë“¤"],
    "guideline_compliance": "ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ë„ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„"
}}
"""
        
                        try:
                            messages = [
                                SystemMessage(content="ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."),
                                HumanMessage(content=area_prompt)
                                                ]
        
                            response = self.llm(messages)
                            result_text = response.content.strip()
                            
                            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                            if json_match:
                                result = json.loads(json_match.group())
                                result["evaluation_method"] = "rag_guideline_based"
                                return result
                            else:
                                raise ValueError(f"RAG ê¸°ë°˜ {area_name} í‰ê°€ì—ì„œ JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ: {result_text}")
                        except Exception as e:
                            print(f"âŒ RAG ê¸°ë°˜ {area_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                            raise e
            
            except Exception as e:
                print(f"âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨ ({area_name}): {e}")
                raise e
        
        # RAG ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‰ê°€
        basic_prompt = f"""
ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™”ì—ì„œ "{area_name}" ì˜ì—­ì˜ ìˆ˜í–‰ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.

ã€ëŒ€í™” ë‚´ìš©ã€‘: {conversation_text}

ã€{area_name} ì¼ë°˜ í‰ê°€ ê¸°ì¤€ã€‘:
- ì™„ì„±ë„: í•„ìš”í•œ í•­ëª©ë“¤ì„ ì–¼ë§ˆë‚˜ ë‹¤ë¤˜ëŠ”ê°€
- ì •í™•ì„±: ì˜í•™ì ìœ¼ë¡œ ì •í™•í•œ ì ‘ê·¼ì¸ê°€  
- ì²´ê³„ì„±: ë…¼ë¦¬ì  ìˆœì„œë¡œ ì§„í–‰í–ˆëŠ”ê°€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "area_score": ì ìˆ˜(1-10),
    "completeness_level": "excellent/good/fair/poor",
    "completed_items": ["ì™„ë£Œëœ í•­ëª©ë“¤"],
    "missing_items": ["ëˆ„ë½ëœ í•­ëª©ë“¤"],
    "strengths": ["ê°•ì ë“¤"],
    "improvements": ["ê°œì„  í•„ìš”ì ë“¤"],
    "guideline_compliance": "ì¼ë°˜ì  ê¸°ì¤€ ê¸°ë°˜ ë¶„ì„"
}}
"""
        
        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."),
                HumanMessage(content=basic_prompt)
            ]
            
            response = self.llm(messages)
            result_text = response.content.strip()
            
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                result["evaluation_method"] = "basic_fallback"
                return result
            else:
                raise ValueError(f"ê¸°ë³¸ {area_name} í‰ê°€ì—ì„œ JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ: {result_text}")
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ {area_name} ì˜ì—­ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise e

    def _build_conversation_text(self, conversation_log: List[Dict]) -> str:
        """ëŒ€í™” ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        conversation_parts = []
        for msg in conversation_log:
            speaker = "í•™ìƒ" if msg.get("role") == "student" else "í™˜ìž"
            content = msg.get("content", "")
            conversation_parts.append(f"{speaker}: {content}")
        return "\n".join(conversation_parts)

    def _evaluate_quality_assessment(self, state: CPXEvaluationState) -> CPXEvaluationState:
        print(f"â­ [{state['user_id']}] 2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì‹œìž‘")
        
        conversation_text = self._build_conversation_text(state["conversation_log"])
        scenario_id = state["scenario_id"]
        scenario_info = self.scenario_applicable_elements.get(scenario_id, {})
        
        quality_assessment_prompt = f"""
ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¤ìŒ CPX ëŒ€í™”ì˜ í’ˆì§ˆì„ 4ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ã€í•™ìƒ-í™˜ìž ëŒ€í™”ã€‘: {conversation_text}
ã€ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ã€‘: {scenario_info.get('name', scenario_id)}

ë‹¤ìŒ 4ê°€ì§€ í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:

ã€1. ì˜í•™ì  ì •í™•ì„± (Medical Accuracy)ã€‘:
- ì§ˆë¬¸ì˜ ì˜í•™ì  íƒ€ë‹¹ì„±ê³¼ ì •í™•ì„±
- ì§„ë‹¨ì  ì ‘ê·¼ì˜ ë…¼ë¦¬ì„±
- ì˜í•™ ìš©ì–´ ì‚¬ìš©ì˜ ì ì ˆì„±
- ìž„ìƒì  íŒë‹¨ì˜ í•©ë¦¬ì„±

ã€2. ì˜ì‚¬ì†Œí†µ íš¨ìœ¨ì„± (Communication Efficiency)ã€‘:
- í™˜ìžê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
- ì§ˆë¬¸ì˜ ëª…í™•ì„±ê³¼ êµ¬ì²´ì„±
- í™˜ìž ë°˜ì‘ì— ëŒ€í•œ ì ì ˆí•œ í›„ì† ì§ˆë¬¸
- ëŒ€í™” íë¦„ì˜ ìžì—°ìŠ¤ëŸ¬ì›€

ã€3. ì „ë¬¸ì„± (Professionalism)ã€‘:
- ì˜ë£Œì§„ë‹¤ìš´ íƒœë„ì™€ ì˜ˆì˜
- í™˜ìžì— ëŒ€í•œ ê³µê°ê³¼ ë°°ë ¤
- ì²´ê³„ì ì´ê³  ë…¼ë¦¬ì ì¸ ì ‘ê·¼
- ìžì‹ ê° ìžˆëŠ” ì§„ë£Œ íƒœë„

ã€4. ì‹œë‚˜ë¦¬ì˜¤ ì í•©ì„± (Scenario Appropriateness)ã€‘:
- ì£¼ì–´ì§„ ì‹œë‚˜ë¦¬ì˜¤ì— ë§žëŠ” ì ‘ê·¼
- í™˜ìž ì—°ë ¹/ì„±ë³„/ìƒí™© ê³ ë ¤
- ì‹œê°„ ì œì•½ ë‚´ íš¨ìœ¨ì  ì§„í–‰
- ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì²´ê³„ì  ì ‘ê·¼

ê° í•­ëª©ì„ 1-10ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ì „ì²´ í’ˆì§ˆ ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "medical_accuracy": ì˜í•™ì ì •í™•ì„±ì ìˆ˜(1-10),
    "communication_efficiency": ì˜ì‚¬ì†Œí†µíš¨ìœ¨ì„±ì ìˆ˜(1-10),
    "professionalism": ì „ë¬¸ì„±ì ìˆ˜(1-10),
    "scenario_appropriateness": ì‹œë‚˜ë¦¬ì˜¤ì í•©ì„±ì ìˆ˜(1-10),
    "overall_quality_score": ì „ì²´í’ˆì§ˆì ìˆ˜(1-10),
    "quality_strengths": ["í’ˆì§ˆ ë©´ì—ì„œ ìš°ìˆ˜í•œ ì ë“¤"],
    "quality_improvements": ["í’ˆì§ˆ ë©´ì—ì„œ ê°œì„ ì´ í•„ìš”í•œ ì ë“¤"],
    "detailed_analysis": {{
        "medical_accuracy_detail": "ì˜í•™ì  ì •í™•ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„",
        "communication_detail": "ì˜ì‚¬ì†Œí†µì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„",
        "professionalism_detail": "ì „ë¬¸ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„",
        "scenario_fit_detail": "ì‹œë‚˜ë¦¬ì˜¤ ì í•©ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„"
    }}
}}
"""
        
        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì˜í•™êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."),
                HumanMessage(content=quality_assessment_prompt)
            ]
            
            response = self.llm(messages)
            result_text = response.content.strip()
            
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                quality_assessment = json.loads(json_match.group())
                
                print(f"âœ… [{state['user_id']}] 2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì™„ë£Œ - ì¢…í•© ì ìˆ˜: {quality_assessment.get('overall_quality_score', 0)}")
                
                return {
                    **state,
                    "quality_evaluation": quality_assessment,
                    "messages": state["messages"] + [HumanMessage(content="2ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì™„ë£Œ")]
                }
            else:
                raise ValueError(f"í’ˆì§ˆ í‰ê°€ì—ì„œ JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ: {result_text}")
        except Exception as e:
            print(f"âŒ [{state['user_id']}] í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise e

    def _generate_comprehensive_results(self, state: CPXEvaluationState) -> CPXEvaluationState:
        """3ë‹¨ê³„: ì¢…í•© í‰ê°€ ë° ìµœì¢… ê²°ê³¼ ìƒì„±"""
        print(f"ðŸŽ¯ [{state['user_id']}] 3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì‹œìž‘")
        
        # 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ ê²°ê³¼ ìˆ˜ì§‘
        rag_completeness = state.get("completeness_assessment", {})
        quality_assessment = state.get("quality_evaluation", {})
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜: ì™„ì„±ë„ 60%, í’ˆì§ˆ 40%)
        completeness_score = rag_completeness.get("overall_completeness", 0.5) * 10  # 0-10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        quality_score = quality_assessment.get("overall_quality_score", 6)
        
        # ê°€ì¤‘ì¹˜ ì ìš©: ì™„ì„±ë„ 60%, í’ˆì§ˆ 40%
        final_score = (completeness_score * 0.6) + (quality_score * 0.4)
        final_score = min(10, max(0, final_score))  # 0-10 ë²”ìœ„ë¡œ ì œí•œ
        
        # ì¢…í•© í”¼ë“œë°± ìƒì„±
        strengths = []
        improvements = []
        
        # 1ë‹¨ê³„ RAG í‰ê°€ì—ì„œ ê°•ì /ê°œì„ ì  ìˆ˜ì§‘
        for area_name, area_data in rag_completeness.get("areas_evaluation", {}).items():
            if isinstance(area_data, dict):
                strengths.extend(area_data.get("strengths", []))
                improvements.extend(area_data.get("improvements", []))
        
        # 2ë‹¨ê³„ í’ˆì§ˆ í‰ê°€ì—ì„œ ê°•ì /ê°œì„ ì  ì¶”ê°€
        strengths.extend(quality_assessment.get("quality_strengths", []))
        improvements.extend(quality_assessment.get("quality_improvements", []))
        
        # ìƒì„¸ ë¶„ì„ ìƒì„±
        detailed_analysis_parts = []
        detailed_analysis_parts.append(f"ã€ì™„ì„±ë„ í‰ê°€ã€‘ RAG ê¸°ë°˜ í‰ê°€ ê²°ê³¼ {rag_completeness.get('overall_completeness', 0):.1%} ì™„ì„±")
        detailed_analysis_parts.append(f"ã€í’ˆì§ˆ í‰ê°€ã€‘ 4ê°€ì§€ í’ˆì§ˆ ê¸°ì¤€ í‰ê·  {quality_score:.1f}ì ")
        
        if rag_completeness.get("total_completed_items", 0) > 0:
            detailed_analysis_parts.append(f"ì´ {rag_completeness.get('total_completed_items', 0)}ê°œ í•­ëª© ì™„ë£Œ")
        
        if rag_completeness.get("total_missing_items", 0) > 0:
            detailed_analysis_parts.append(f"{rag_completeness.get('total_missing_items', 0)}ê°œ í•­ëª© ëˆ„ë½")
        
        comprehensive_result = {
            "final_score": round(final_score, 1),
            "grade": self._calculate_grade(final_score * 10),  # 100ì  ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            "score_breakdown": {
                "completeness_score": round(completeness_score, 1),
            "quality_score": round(quality_score, 1),
                "weighted_completeness": round(completeness_score * 0.6, 1),
                "weighted_quality": round(quality_score * 0.4, 1)
            },
            "detailed_feedback": {
                "strengths": list(set(strengths))[:5] if strengths else ["í‰ê°€ë¥¼ ì„±ì‹¤ížˆ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤"],
                "improvements": list(set(improvements))[:5] if improvements else ["ì§€ì†ì ì¸ í•™ìŠµê³¼ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤"],
                "overall_analysis": " | ".join(detailed_analysis_parts)
            },
            "evaluation_summary": {
                "method": "3ë‹¨ê³„ RAG+í’ˆì§ˆ í‰ê°€",
                "steps_completed": 3,
                "completeness_rate": rag_completeness.get("overall_completeness", 0),
                "quality_details": quality_assessment.get("detailed_analysis", {}),
                "total_items_evaluated": rag_completeness.get("total_completed_items", 0) + rag_completeness.get("total_missing_items", 0)
            }
        }
        
        print(f"âœ… [{state['user_id']}] 3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {final_score:.1f}/10 ({comprehensive_result['grade']})")
        
        return {
            **state,
            "comprehensive_evaluation": comprehensive_result,
            "final_scores": {
                "total_score": round(final_score * 10, 1),  # 100ì  ìŠ¤ì¼€ì¼
                "completion_rate": rag_completeness.get("overall_completeness", 0.5),
                "quality_score": quality_score,
                "grade": comprehensive_result["grade"]
            },
            "feedback": comprehensive_result["detailed_feedback"],
            "messages": state["messages"] + [HumanMessage(content=f"3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì™„ë£Œ - {final_score:.1f}ì  ({comprehensive_result['grade']})")]
        }



    def _calculate_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ê³„ì‚°"""
        if score >= 90:
            return "A+"
        elif score >= 85:
            return "A"
        elif score >= 80:
            return "B+"
        elif score >= 75:
            return "B"
        elif score >= 70:
            return "C+"
        elif score >= 65:
            return "C"
        else:
            return "F"

    # =============================================================================
    # ìƒˆë¡œìš´ ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„° ë¶„ì„ ë©”ì„œë“œë“¤
    # =============================================================================
    
    async def _analyze_conversation_entries(self, session: Dict) -> Dict:
        """ìƒˆë¡œìš´ ì‹¤ì‹œê°„ ëŒ€í™” ë°ì´í„° ë¶„ì„"""
        conversation_entries = session.get("conversation_entries", [])
        
        if not conversation_entries:
            return {"error": "ë¶„ì„í•  ëŒ€í™” ì—”íŠ¸ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì—­í• ë³„ ë¶„ë¦¬
        doctor_entries = [entry for entry in conversation_entries if entry["speaker_role"] == "doctor"]
        patient_entries = [entry for entry in conversation_entries if entry["speaker_role"] == "patient"]
        
        # ê°ì • ë¶„ì„ í†µê³„ (ì˜ì‚¬ ë°œì–¸ë§Œ)
        emotion_stats = {}
        if doctor_entries:
            emotions = [entry.get("emotion", {}).get("predicted_emotion") for entry in doctor_entries if entry.get("emotion")]
            emotions = [e for e in emotions if e]  # None ì œê±°
            
            if emotions:
                emotion_counts = Counter(emotions)
                total_emotional_entries = len(emotions)
                
                emotion_stats = {
                    "dominant_emotion": emotion_counts.most_common(1)[0][0] if emotion_counts else None,
                    "emotion_distribution": {
                        emotion: count / total_emotional_entries 
                        for emotion, count in emotion_counts.items()
                    },
                    "total_emotional_entries": total_emotional_entries,
                    "emotional_consistency": max(emotion_counts.values()) / total_emotional_entries if total_emotional_entries > 0 else 0
                }
        
        # ëŒ€í™” íŒ¨í„´ ë¶„ì„
        conversation_pattern = {
            "total_entries": len(conversation_entries),
            "doctor_utterances": len(doctor_entries),
            "patient_utterances": len(patient_entries),
            "conversation_balance": len(patient_entries) / len(doctor_entries) if len(doctor_entries) > 0 else 0,
            "avg_doctor_utterance_length": sum(len(entry["text"]) for entry in doctor_entries) / len(doctor_entries) if doctor_entries else 0,
            "avg_patient_utterance_length": sum(len(entry["text"]) for entry in patient_entries) / len(patient_entries) if patient_entries else 0
        }
        
        # ì‹œê°„ ë¶„ì„
        if len(conversation_entries) >= 2:
            first_time = datetime.fromisoformat(conversation_entries[0]["timestamp"])
            last_time = datetime.fromisoformat(conversation_entries[-1]["timestamp"])
            duration_seconds = (last_time - first_time).total_seconds()
            
            time_analysis = {
                "conversation_duration_seconds": duration_seconds,
                "conversation_duration_minutes": duration_seconds / 60,
                "entries_per_minute": len(conversation_entries) / (duration_seconds / 60) if duration_seconds > 0 else 0
            }
        else:
            time_analysis = {
                "conversation_duration_seconds": 0,
                "conversation_duration_minutes": 0,
                "entries_per_minute": 0
            }
        
        return {
            "emotion_analysis": emotion_stats,
            "conversation_pattern": conversation_pattern,
            "time_analysis": time_analysis,
            "quality_indicators": {
                "has_emotional_data": len([e for e in doctor_entries if e.get("emotion")]) > 0,
                "conversation_completeness": len(conversation_entries) >= 10,  # ìµœì†Œ 10ê°œ ë°œì–¸
                "balanced_interaction": 0.3 <= conversation_pattern["conversation_balance"] <= 3.0
            }
        }

    async def _update_cpx_database_after_evaluation(self, session_id: str, evaluation_result: dict):
        """í‰ê°€ ì™„ë£Œ í›„ CPX Detailsë§Œ ì—…ë°ì´íŠ¸"""
        try:
            session = self.session_data[session_id]
            result_id = session["result_id"]
            user_id = session["user_id"]
            
            # CPX Detailsë§Œ ì—…ë°ì´íŠ¸ (ì‹œìŠ¤í…œ í‰ê°€ ë°ì´í„°)
            async for db in get_db():
                cpx_service = CpxService(db)
                
                await cpx_service.update_cpx_details(
                    result_id=result_id,
                    user_id=int(user_id),
                    system_evaluation_data=evaluation_result
                )
                
                print(f"âœ… CPX Details ì—…ë°ì´íŠ¸ ì™„ë£Œ: result_id={result_id}, session_id={session_id}")
                break
                
        except Exception as e:
            print(f"âŒ CPX Details ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    async def _cleanup_audio_files(self, audio_file_path: str):
        """í‰ê°€ ì™„ë£Œ í›„ ìž„ì‹œ WAV íŒŒì¼ë“¤ë§Œ ì‚­ì œ (TTS ìºì‹œ íŒŒì¼ì€ ë³´ì¡´)"""

        try:
            file_path_obj = Path(audio_file_path)
            # TTS ìºì‹œ íŒŒì¼ì€ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            if "cache/tts" in str(file_path_obj):
                print(f"ðŸ”’ TTS ìºì‹œ íŒŒì¼ ë³´ì¡´: {audio_file_path}")
                return
                
            if file_path_obj.exists() and file_path_obj.suffix == '.wav':
                file_path_obj.unlink()  # WAV íŒŒì¼ë§Œ ì‚­ì œ
                print(f"ðŸ—‘ï¸ ìž„ì‹œ WAV íŒŒì¼ ì‚­ì œ: {audio_file_path}")
                    
        except Exception as e:
            print(f"âŒ WAV íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({audio_file_path}): {e}")