"""
SER (Speech Emotion Recognition) ì„œë¹„ìŠ¤
ìŒì„± ê°ì • ë¶„ì„ ì „ë‹´ ì„œë¹„ìŠ¤
"""

from typing import Dict, Optional
from pathlib import Path
import torch
import numpy as np
import librosa
import logging

logger = logging.getLogger(__name__)

class SERService:
    def __init__(self):
        """SER ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.emotion_model = None
        self.emotion_processor = None
        self.emotion_labels = ["Anxious", "Dry", "Kind"]
        self.ser_model_path = Path("SER/results_quick_test/adversary_model_augment_v1_epoch_5")
        
        print("ğŸ­ SER ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def load_model(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self.emotion_model is not None:
            return True
        
        try:
            print("ğŸ­ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ì»¤ìŠ¤í…€ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if self.ser_model_path.exists():
                from transformers import Wav2Vec2Processor
                from SER.finetune_direct import custom_Wav2Vec2ForEmotionClassification
                
                self.emotion_model = custom_Wav2Vec2ForEmotionClassification.from_pretrained(
                    str(self.ser_model_path)
                )
                self.emotion_processor = Wav2Vec2Processor.from_pretrained(
                    str(self.ser_model_path)
                )
                
                print("âœ… ì»¤ìŠ¤í…€ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
                from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor, Wav2Vec2Config
                
                model_name = "kresnik/wav2vec2-large-xlsr-korean"
                label2id = {label: i for i, label in enumerate(self.emotion_labels)}
                id2label = {i: label for i, label in enumerate(self.emotion_labels)}
                
                config = Wav2Vec2Config.from_pretrained(
                    model_name,
                    num_labels=len(self.emotion_labels),
                    label2id=label2id,
                    id2label=id2label,
                    finetuning_task="emotion_classification"
                )
                
                self.emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(
                    model_name,
                    config=config,
                    ignore_mismatched_sizes=True
                )
                self.emotion_processor = Wav2Vec2Processor.from_pretrained(model_name)
                
                print("âœ… ê¸°ë³¸ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.emotion_model.eval()
            return True
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.emotion_model = None
            self.emotion_processor = None
            return False
    
    async def analyze_emotion_from_buffer(self, audio_buffer: bytearray) -> Dict:
        """
        ì˜¤ë””ì˜¤ ë²„í¼ì—ì„œ ì§ì ‘ ê°ì • ë¶„ì„
        
        Args:
            audio_buffer: ë¶„ì„í•  ì˜¤ë””ì˜¤ ë²„í¼ (16-bit PCM)
            
        Returns:
            ê°ì • ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.emotion_model is None:
            model_loaded = await self.load_model()
            if not model_loaded:
                return {"error": "ê°ì • ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            if not audio_buffer or len(audio_buffer) == 0:
                return {"error": "ì˜¤ë””ì˜¤ ë²„í¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
            
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ë²„í¼ì—ì„œ ì§ì ‘)
            audio_data = await self._preprocess_audio_from_buffer(audio_buffer)
            if audio_data is None:
                return {"error": "ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨"}
            
            # ê°ì • ë¶„ì„ ìˆ˜í–‰
            with torch.no_grad():
                inputs = {
                    "input_values": audio_data,
                    "attention_mask": None
                }
                
                outputs = self.emotion_model(**inputs)
                logits = outputs['emotion_logits']
                
                # ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°
                probabilities = torch.nn.functional.softmax(logits, dim=-1)
                predicted_id = torch.argmax(logits, dim=-1).item()
                confidence = probabilities[0][predicted_id].item()
                
                # ì˜ˆì¸¡ ê°ì • ê²°ê³¼
                predicted_emotion = self.emotion_labels[predicted_id]
                
                # ëª¨ë“  ê°ì •ë³„ í™•ë¥ 
                emotion_scores = {
                    emotion: probabilities[0][i].item() 
                    for i, emotion in enumerate(self.emotion_labels)
                }
                
                return {
                    "success": True,
                    "predicted_emotion": predicted_emotion,
                    "confidence": confidence,
                    "emotion_scores": emotion_scores,
                    "source": "audio_buffer"
                }
                
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    
    async def analyze_emotion(self, audio_file_path: str) -> Dict:
        """
        ìŒì„± íŒŒì¼ì˜ ê°ì • ë¶„ì„ (í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ ë©”ì„œë“œ)
        
        Args:
            audio_file_path: ë¶„ì„í•  ìŒì„± íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê°ì • ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.emotion_model is None:
            model_loaded = await self.load_model()
            if not model_loaded:
                return {"error": "ê°ì • ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
            audio_path = Path(audio_file_path)
            if not audio_path.exists():
                return {"error": f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_file_path}"}
            
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            audio_data = await self._preprocess_audio(str(audio_path))
            if audio_data is None:
                return {"error": "ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨"}
            
            # ê°ì • ë¶„ì„ ìˆ˜í–‰
            with torch.no_grad():
                inputs = {
                    "input_values": audio_data,
                    "attention_mask": None
                }
                
                outputs = self.emotion_model(**inputs)
                logits = outputs['emotion_logits']
                
                # ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°
                probabilities = torch.nn.functional.softmax(logits, dim=-1)
                predicted_id = torch.argmax(logits, dim=-1).item()
                confidence = probabilities[0][predicted_id].item()
                
                # ì˜ˆì¸¡ ê°ì • ê²°ê³¼
                predicted_emotion = self.emotion_labels[predicted_id]
                
                # ëª¨ë“  ê°ì •ë³„ í™•ë¥ 
                emotion_scores = {
                    emotion: probabilities[0][i].item() 
                    for i, emotion in enumerate(self.emotion_labels)
                }
                
                return {
                    "success": True,
                    "predicted_emotion": predicted_emotion,
                    "confidence": confidence,
                    "emotion_scores": emotion_scores,
                    "file_path": str(audio_path)
                }
                
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"error": f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    
    async def _preprocess_audio_from_buffer(self, audio_buffer: bytearray) -> Optional[torch.Tensor]:
        """ì˜¤ë””ì˜¤ ë²„í¼ ì „ì²˜ë¦¬ (Wav2Vec2ìš©)"""
        try:
            # 16-bit PCM ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            audio_data = np.frombuffer(audio_buffer, dtype=np.int16)
            
            # float32ë¡œ ì •ê·œí™” (-1.0 ~ 1.0)
            audio = audio_data.astype(np.float32) / 32768.0
            
            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 15ì´ˆ)
            max_duration = 15.0
            target_length = int(16000 * max_duration)
            
            if len(audio) > target_length:
                # ê°€ìš´ë° ë¶€ë¶„ ì‚¬ìš©
                start_idx = np.random.randint(0, len(audio) - target_length + 1)
                audio = audio[start_idx:start_idx + target_length]
            elif len(audio) < target_length:
                # íŒ¨ë”© ì¶”ê°€
                pad_length = target_length - len(audio)
                audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)
            
            # Wav2Vec2 processorë¡œ ë³€í™˜
            inputs = self.emotion_processor(
                audio,
                sampling_rate=16000,
                return_tensors="pt",
                padding=True
            )
            
            return inputs.input_values.squeeze(0)
            
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ ë²„í¼ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    async def _preprocess_audio(self, file_path: str) -> Optional[torch.Tensor]:
        """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (Wav2Vec2ìš©)"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ (16kHzë¡œ ë¦¬ìƒ˜í”Œë§)
            audio, sr = librosa.load(file_path, sr=16000, res_type='kaiser_fast')
            
            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 15ì´ˆ)
            max_duration = 15.0
            target_length = int(16000 * max_duration)
            
            if len(audio) > target_length:
                # ê°€ìš´ë° ë¶€ë¶„ ì‚¬ìš©
                start_idx = np.random.randint(0, len(audio) - target_length + 1)
                audio = audio[start_idx:start_idx + target_length]
            elif len(audio) < target_length:
                # íŒ¨ë”© ì¶”ê°€
                pad_length = target_length - len(audio)
                audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)
            
            # Wav2Vec2 processorë¡œ ë³€í™˜
            inputs = self.emotion_processor(
                audio,
                sampling_rate=16000,
                return_tensors="pt",
                padding=True
            )
            
            return inputs.input_values.squeeze(0)
            
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def is_model_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return self.emotion_model is not None and self.emotion_processor is not None
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_loaded": self.is_model_loaded(),
            "model_path": str(self.ser_model_path),
            "emotion_labels": self.emotion_labels,
            "model_exists": self.ser_model_path.exists()
        }


