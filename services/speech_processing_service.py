import asyncio
from typing import Dict
from pathlib import Path
import wave
import os


class SpeechProcessingService:
    def __init__(self, whisper_model, llm_service, tts_service, evaluation_service):
        """ìŒì„± ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.whisper_model = whisper_model
        self.llm_service = llm_service
        self.tts_service = tts_service
        self.evaluation_service = evaluation_service
        self.user_buffers = {}

        # temp_audio ë””ë ‰í† ë¦¬ ìƒì„±
        Path("temp_audio").mkdir(exist_ok=True)

    async def process_audio_chunk(self, user_id: str, audio_chunk: bytes, is_silent: bool):
        """ìŒì„± ì²­í¬ ì²˜ë¦¬"""
        if user_id not in self.user_buffers:
            self.user_buffers[user_id] = {"audio": bytearray(), "speaking": False, "silence": 0}

        buffer = self.user_buffers[user_id]
        buffer["audio"].extend(audio_chunk)

        if not is_silent:
            buffer["speaking"] = True
            buffer["silence"] = 0
        else:
            buffer["silence"] += 1
            if buffer["speaking"] and self.should_process_speech(user_id):
                await self.process_speech(user_id)

    def should_process_speech(self, user_id: str) -> bool:
        """ì‹¤ì‹œê°„ ë²„í¼ ë¶„ì„ìœ¼ë¡œ ì ì‘ì  ì²˜ë¦¬"""
        buffer = self.user_buffers[user_id]
        silence_time = buffer["silence"]
        audio_length = len(buffer["audio"])

        # ë„ˆë¬´ ì§§ì€ ê±´ ë¬´ì‹œ (0.3ì´ˆ)
        if silence_time < 3:
            return False

        # ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ë„ ë¬´ì‹œ (0.5ì´ˆ)
        if audio_length < 8000:  # 16kHz * 0.5ì´ˆ
            return False

        # ğŸ¯ í•µì‹¬: ì‹¤ì‹œê°„ ë¶€ë¶„ ì „ì‚¬ë¡œ ì˜ë„ íŒŒì•…
        if audio_length > 16000:  # 1ì´ˆ ì´ìƒ ìŒì„± ìˆìœ¼ë©´
            partial_text = self.get_partial_transcript(user_id)
            if partial_text:
                # ì§ˆë¬¸ì´ë©´ ë¹ ë¥´ê²Œ ì²˜ë¦¬ (0.5ì´ˆ)
                if self.is_question(partial_text):
                    return silence_time >= 5

                # ë§ì„¤ì„ì´ë©´ ì¶©ë¶„íˆ ê¸°ë‹¤ë¦¬ê¸° (2.0ì´ˆ)
                if self.is_hesitation(partial_text):
                    return silence_time >= 20

                # ì¼ë°˜ ë¬¸ì¥ì´ë©´ ì¤‘ê°„ ì†ë„ (0.7ì´ˆ)
                return silence_time >= 7

        # ê¸°ë³¸ ê·œì¹™: 0.5ì´ˆ
        return silence_time >= 5

    async def process_speech(self, user_id: str):
        """ì™„ì„±ëœ ìŒì„± ì²˜ë¦¬"""
        buffer = self.user_buffers[user_id]

        if not buffer["audio"]:
            return

            # 1. STT ì²˜ë¦¬
        audio_path = f"temp_audio/speech_{user_id}.wav"
        self.save_audio(buffer["audio"], audio_path)

        result = self.whisper_model.transcribe(audio_path)
        user_text = result["text"].strip()

        if not user_text:
            return

        # ì…ë ¥ ë¡œê¹…
        print(f"\nğŸ¤ ì‚¬ìš©ì ì…ë ¥: '{user_text}'")

        # 2. LLM ì‘ë‹µ ìƒì„± (ê³ ì •ëœ ì‹œë‚˜ë¦¬ì˜¤ ì‚¬ìš©)
        ai_response = await self.llm_service.generate_response(user_text, user_id)

        # ì¶œë ¥ ë¡œê¹…
        print(f"ğŸ¤– AI ì‘ë‹µ: '{ai_response}'")

        # 3. TTS ìƒì„±
        audio_path = await self.tts_service.generate_speech(ai_response)

        # 4. í‰ê°€ ê¸°ë¡
        await self.evaluation_service.record_interaction(user_id, user_text, ai_response)

        # 5. ë²„í¼ ì´ˆê¸°í™”
        buffer["audio"].clear()
        buffer["speaking"] = False
        buffer["silence"] = 0

        # 6. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(f"temp_audio/speech_{user_id}.wav"):
            os.remove(f"temp_audio/speech_{user_id}.wav")

        return {
            "user_text": user_text,
            "ai_text": ai_response,
            "audio_url": f"/static/audio/{Path(audio_path).name}" if audio_path else None
        }

    def get_partial_transcript(self, user_id: str) -> str:
        """ì‹¤ì‹œê°„ ë¶€ë¶„ ì „ì‚¬ (ë²„í¼ ë¶„ì„ìš©)"""
        try:
            buffer = self.user_buffers[user_id]
            temp_path = f"temp_audio/partial_{user_id}.wav"
            self.save_audio(buffer["audio"], temp_path)

            result = self.whisper_model.transcribe(temp_path)
            partial_text = result["text"].strip()

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(temp_path):
                os.remove(temp_path)

            return partial_text
        except:
            return ""  # ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¬¸ìì—´

    def is_question(self, text: str) -> bool:
        """ì§ˆë¬¸ì¸ì§€ íŒë‹¨ - ë¹ ë¥¸ ì²˜ë¦¬"""
        text = text.strip().lower()

        # ëª…í™•í•œ ì§ˆë¬¸ íŒ¨í„´ë“¤
        if text.endswith(('?', 'ìš”?', 'ì„¸ìš”?', 'ê¹Œìš”?', 'ë‚˜ìš”?')):
            return True

        # ì§ˆë¬¸ ì‹œì‘ ë‹¨ì–´ë“¤
        question_starts = ['ì–¸ì œ', 'ì–´ë””', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë­', 'ë¬´ì—‡', 'ì–´ë–¤', 'ëª‡', 'í˜¹ì‹œ']
        if any(text.startswith(word) for word in question_starts):
            return True

        # ì˜ë£Œ ì§ˆë¬¸ íŒ¨í„´ë“¤
        medical_questions = ['ì•„í”„', 'í†µì¦', 'ì¦ìƒ', 'ì–¸ì œë¶€í„°', 'ì–¼ë§ˆë‚˜']
        question_endings = ['ì„¸ìš”', 'ë‚˜ìš”', 'ì–´ìš”', 'ê¹Œìš”']

        for med in medical_questions:
            if med in text:
                for ending in question_endings:
                    if text.endswith(ending):
                        return True

        return False

    def is_hesitation(self, text: str) -> bool:
        """ë§ì„¤ì„ì¸ì§€ íŒë‹¨ - ë” ê¸°ë‹¤ë¦¬ê¸°"""
        text = text.strip().lower()

        # ë§ì„¤ì„ íŒ¨í„´ë“¤
        hesitation_patterns = [
            'ìŒ', 'ì–´', 'ê·¸', 'ê·¸ëŸ°', 'ì•„', 'ì–´ë–»ê²Œ', 'ì ê¹', 'ì ì‹œ',
            'ì–´ë–»ê²Œ ë§í•˜ì§€', 'ê·¸ê²Œ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ì•„ë‹ˆ', 'ì–´ë””ë³´ì'
        ]

        # ì§§ê³  ë§ì„¤ì´ëŠ” í‘œí˜„ë“¤
        if len(text) < 10 and any(pattern in text for pattern in hesitation_patterns):
            return True

        # ë¯¸ì™„ì„± ë¬¸ì¥ë“¤
        incomplete_endings = ['ê·¸ëŸ°', 'ê·¸ê²Œ', 'ì–´ë–»ê²Œ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ìŒ']
        if any(text.endswith(pattern) for pattern in incomplete_endings):
            return True

        return False

    def save_audio(self, audio_buffer: bytearray, file_path: str):
        """ì˜¤ë””ì˜¤ ë²„í¼ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥"""
        with wave.open(file_path, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(16000)
            wav_file.writeframes(bytes(audio_buffer))

    def clear_user_buffer(self, user_id: str):
        """ì‚¬ìš©ì ë²„í¼ ì´ˆê¸°í™”"""
        if user_id in self.user_buffers:
            del self.user_buffers[user_id]
