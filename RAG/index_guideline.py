"""
ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ RAG ì‹œìŠ¤í…œì— ì¸ë±ì‹±í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import os
from pathlib import Path
from typing import Dict, List
import torch
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

def load_device():
    return "cuda" if torch.cuda.is_available() else "cpu"

def create_documents_from_guideline(guideline_path: str) -> List[Document]:
    """ê°€ì´ë“œë¼ì¸ JSON íŒŒì¼ì„ Document ê°ì²´ë“¤ë¡œ ë³€í™˜"""
    
    with open(guideline_path, 'r', encoding='utf-8') as f:
        guideline = json.load(f)
    
    documents = []
    
    # 1. ì „ì²´ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ
    full_content = f"""
ì¹´í…Œê³ ë¦¬: {guideline['category']}
ì„¤ëª…: {guideline['description']}

ì´ê²ƒì€ {guideline['category']} CPX í‰ê°€ ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤.
ì´ {guideline['metadata']['total_questions']}ê°œì˜ ì§ˆë¬¸ê³¼ í–‰ë™ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""
    
    documents.append(Document(
        page_content=full_content,
        metadata={
            "source": "guideline",
            "type": "overview",
            "category": guideline['category'],
            "total_questions": guideline['metadata']['total_questions']
        }
    ))
    
    # 2. ê° í‰ê°€ ì˜ì—­ë³„ë¡œ ë¬¸ì„œ ìƒì„±
    for area_key, area_data in guideline['evaluation_areas'].items():
        area_content = f"""
í‰ê°€ ì˜ì—­: {area_data['name']}
ì¹´í…Œê³ ë¦¬: {guideline['category']}

"""
        
        # ê° í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ë“¤ì„ í¬í•¨
        for subcat_key, subcat_data in area_data.get('subcategories', {}).items():
            if not subcat_data.get('applicable', True):
                continue  # applicableì´ falseì¸ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°
                
            area_content += f"\n{subcat_data['name']}:\n"
            
            # ì§ˆë¬¸ë“¤ ì¶”ê°€
            questions = subcat_data.get('required_questions', [])
            for question in questions:
                area_content += f"- {question}\n"
            
            # í–‰ë™ë“¤ ì¶”ê°€
            actions = subcat_data.get('required_actions', [])
            for action in actions:
                area_content += f"- {action}\n"
        
        documents.append(Document(
            page_content=area_content,
            metadata={
                "source": "guideline",
                "type": "evaluation_area",
                "category": guideline['category'],
                "area": area_data['name'],
                "area_key": area_key
            }
        ))
    
    # 3. ê° í•˜ìœ„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì„¸ë¶€ ë¬¸ì„œ ìƒì„±
    for area_key, area_data in guideline['evaluation_areas'].items():
        for subcat_key, subcat_data in area_data.get('subcategories', {}).items():
            if not subcat_data.get('applicable', True):
                continue
                
            subcat_content = f"""
ì¹´í…Œê³ ë¦¬: {guideline['category']}
í‰ê°€ ì˜ì—­: {area_data['name']}
í•˜ìœ„ ì¹´í…Œê³ ë¦¬: {subcat_data['name']}

"""
            
            questions = subcat_data.get('required_questions', [])
            actions = subcat_data.get('required_actions', [])
            
            if questions:
                subcat_content += "í•„ìˆ˜ ì§ˆë¬¸ë“¤:\n"
                for question in questions:
                    subcat_content += f"- {question}\n"
            
            if actions:
                subcat_content += "\ní•„ìˆ˜ í–‰ë™ë“¤:\n"
                for action in actions:
                    subcat_content += f"- {action}\n"
            
            documents.append(Document(
                page_content=subcat_content,
                metadata={
                    "source": "guideline",
                    "type": "subcategory",
                    "category": guideline['category'],
                    "area": area_data['name'],
                    "area_key": area_key,
                    "subcategory": subcat_data['name'],
                    "subcategory_key": subcat_key,
                    "question_count": len(questions),
                    "action_count": len(actions)
                }
            ))
    
    return documents

def index_guideline_to_faiss(guideline_path: str, index_path: str, model_name: str = "intfloat/multilingual-e5-large"):
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€"""
    
    device = load_device()
    print(f"ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name
    )
    
    # ê°€ì´ë“œë¼ì¸ì„ ë¬¸ì„œë¡œ ë³€í™˜
    documents = create_documents_from_guideline(guideline_path)
    print(f"ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if os.path.exists(index_path):
        print(f"ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {index_path}")
        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
        
        # ìƒˆë¡œìš´ ë¬¸ì„œë“¤ ì¶”ê°€
        vectorstore.add_documents(documents)
        print(f"ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ {len(documents)}ê°œ ì¶”ê°€ ì™„ë£Œ")
    else:
        print(f"ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±: {index_path}")
        vectorstore = FAISS.from_documents(documents, embeddings)
        print(f"ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ {len(documents)}ê°œë¡œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    # ì¸ë±ìŠ¤ ì €ì¥
    vectorstore.save_local(index_path)
    print(f"FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_path}")
    
    return vectorstore

def search_guideline(index_path: str, query: str, k: int = 5, model_name: str = "intfloat/multilingual-e5-large"):
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
    
    device = load_device()
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name
    )
    
    # ì¸ë±ìŠ¤ ë¡œë“œ
    vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    results = vectorstore.similarity_search(query, k=k)
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¥ ê°€ì´ë“œë¼ì¸ RAG ì¸ë±ì‹± ì‹œì‘")
    
    # ê²½ë¡œ ì„¤ì •
    guideline_path = "memory_loss_guideline_rag.json"
    index_path = "faiss_guideline_index"
    
    if not os.path.exists(guideline_path):
        print(f"âŒ ê°€ì´ë“œë¼ì¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {guideline_path}")
        print("ë¨¼ì € extract_memory_loss.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ê°€ì´ë“œë¼ì¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ê°€ì´ë“œë¼ì¸ ì¸ë±ì‹±
    try:
        vectorstore = index_guideline_to_faiss(guideline_path, index_path)
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(vectorstore.index_to_docstore_id.values())}")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰:")
        test_queries = [
            "ê¸°ì–µë ¥ ì €í•˜ ë³‘ë ¥ì²­ì·¨ ì§ˆë¬¸",
            "MMSE ê²€ì‚¬ ë°©ë²•",
            "í™˜ì êµìœ¡ ë‚´ìš©"
        ]
        
        for query in test_queries:
            print(f"\nì¿¼ë¦¬: {query}")
            results = search_guideline(index_path, query, k=2)
            for i, doc in enumerate(results):
                print(f"  {i+1}. [{doc.metadata.get('type', 'unknown')}] {doc.page_content[:100]}...")
        
    except Exception as e:
        print(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()
