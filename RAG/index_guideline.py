"""
ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ RAG ì‹œìŠ¤í…œì— ì¸ë±ì‹±í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import os
from pathlib import Path
from typing import Dict, List
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API ì‚¬ìš©ìœ¼ë¡œ device í•¨ìˆ˜ ë¶ˆí•„ìš”

def create_documents_from_guideline(guideline_path: str) -> List[Document]:
    """ê°€ì´ë“œë¼ì¸ JSON íŒŒì¼ì„ Document ê°ì²´ë“¤ë¡œ ë³€í™˜"""
    
    with open(guideline_path, 'r', encoding='utf-8') as f:
        guideline = json.load(f)
    
    documents = []
    category = guideline['category']
    
    # ê° í‰ê°€ ì˜ì—­ë³„ë¡œ ì¹´í…Œê³ ë¦¬ í¬í•¨í•œ ë¬¸ì„œ ìƒì„±
    for area_key, area_data in guideline['evaluation_areas'].items():
        area_name = area_data['name']
        
        # ë¬¸ì„œ ì œëª©ì— ì¹´í…Œê³ ë¦¬ + ì˜ì—­ëª… í¬í•¨ (í•œê¸€ name ì‚¬ìš©)
        area_content = f"""
ì œëª©: {category} {area_name} í‰ê°€ ê°€ì´ë“œë¼ì¸
ì¹´í…Œê³ ë¦¬: {category}
í‰ê°€ ì˜ì—­: {area_name}

=== {category} {area_name} í•„ìˆ˜ í•­ëª©ë“¤ ===

"""
        
        # ëª¨ë“  í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸/í–‰ë™ì„ í•œ ë¬¸ì„œì— í†µí•©
        all_questions = []
        all_actions = []
        
        for subcat_key, subcat_data in area_data.get('subcategories', {}).items():
            if not subcat_data.get('applicable', True):
                continue  # applicableì´ falseì¸ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°
                
            subcat_name = subcat_data['name']
            area_content += f"\nã€{subcat_name}ã€‘\n"
            
            # ì§ˆë¬¸ë“¤ ì¶”ê°€
            questions = subcat_data.get('required_questions', [])
            if questions:
                area_content += "í•„ìˆ˜ ì§ˆë¬¸:\n"
                for question in questions:
                    area_content += f"  â€¢ {question}\n"
                    all_questions.append(question)
            
            # í–‰ë™ë“¤ ì¶”ê°€
            actions = subcat_data.get('required_actions', [])
            if actions:
                area_content += "í•„ìˆ˜ í–‰ë™:\n"
                for action in actions:
                    area_content += f"  â€¢ {action}\n"
                    all_actions.append(action)
            
            area_content += "\n"
        
        # ìš”ì•½ ì •ë³´ ì¶”ê°€
        area_content += f"""
=== ìš”ì•½ ===
ì´ í•„ìˆ˜ ì§ˆë¬¸: {len(all_questions)}ê°œ
ì´ í•„ìˆ˜ í–‰ë™: {len(all_actions)}ê°œ
"""
    
        documents.append(Document(
            page_content=area_content,
            metadata={
                "source": "cpx_textbook",
                "type": "guideline",
                "category": category,
                "area": area_name,
                "total_questions": len(all_questions),
                "total_actions": len(all_actions)
            }
        ))
    
    return documents

def index_guideline_to_faiss(guideline_path: str, index_path: str, model_name: str = "text-embedding-3-small"):
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€"""
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” - OpenAI API ì‚¬ìš©
    embeddings = OpenAIEmbeddings(
        model=model_name,
        openai_api_key=api_key
    )
    
    # ê°€ì´ë“œë¼ì¸ì„ ë¬¸ì„œë¡œ ë³€í™˜
    documents = create_documents_from_guideline(guideline_path)
    print(f"ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if os.path.exists(index_path):
        print(f"ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {index_path}")
        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
        
        # ìƒˆë¡œìš´ ë¬¸ì„œë“¤ ì¶”ê°€
        vectorstore.add_documents(documents)
        print(f"ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ {len(documents)}ê°œ ì¶”ê°€ ì™„ë£Œ")
    else:
        print(f"ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±: {index_path}")
        vectorstore = FAISS.from_documents(documents, embeddings)
        print(f"ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ {len(documents)}ê°œë¡œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    # ì¸ë±ìŠ¤ ì €ì¥
    vectorstore.save_local(index_path)
    print(f"FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_path}")
    
    return vectorstore

def search_guideline(index_path: str, query: str, k: int = 5, model_name: str = "text-embedding-3-small"):
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    embeddings = OpenAIEmbeddings(
        model=model_name,
        openai_api_key=api_key
    )
    
    # ì¸ë±ìŠ¤ ë¡œë“œ
    vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    results = vectorstore.similarity_search(query, k=k)
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¥ ê°€ì´ë“œë¼ì¸ RAG ì¸ë±ì‹± ì‹œì‘")
    
    # ê²½ë¡œ ì„¤ì •
    guideline_path = "memory_loss_guideline_rag.json"
    index_path = "faiss_guideline_index"
    
    if not os.path.exists(guideline_path):
        print(f"âŒ ê°€ì´ë“œë¼ì¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {guideline_path}")
        print("ë¨¼ì € extract_memory_loss.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ê°€ì´ë“œë¼ì¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ê°€ì´ë“œë¼ì¸ ì¸ë±ì‹±
    try:
        vectorstore = index_guideline_to_faiss(guideline_path, index_path)
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(vectorstore.index_to_docstore_id.values())}")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (ì¹´í…Œê³ ë¦¬ í¬í•¨)
        print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰:")
        test_queries = [
            "ê¸°ì–µë ¥ ì €í•˜ ë³‘ë ¥ ì²­ì·¨",
            "ê¸°ì–µë ¥ ì €í•˜ ì‹ ì²´ ì§„ì°°", 
            "ê¸°ì–µë ¥ ì €í•˜ í™˜ì êµìœ¡"
        ]
        
        for query in test_queries:
            print(f"\nì¿¼ë¦¬: {query}")
            results = search_guideline(index_path, query, k=2)
            for i, doc in enumerate(results):
                print(f"  {i+1}. [{doc.metadata.get('type', 'unknown')}] {doc.page_content[:100]}...")
        
    except Exception as e:
        print(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()
