import asyncio
import json
import logging


from typing import Dict, Any

import numpy as np
from datetime import datetime
import base64

from fastapi import APIRouter, WebSocket, WebSocketDisconnect


from core.startup import service_manager
from core.config import settings
from infra.inmemory_queue import (
    enqueue_user_utterance,
    enqueue_ai_utterance,
    enqueue_conversation_ended,
    start_worker_once,
)
from services.cpx_service import CpxService
from core.config import get_db
        
logger = logging.getLogger(__name__)

# WebSocket ë¼ìš°í„° ìƒì„±
router = APIRouter()

class AudioProcessor:
    """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ì‚¬ìš©ìë³„ ì„¸ì…˜ ê´€ë¦¬
        self.user_sessions: Dict[str, Dict] = {}
        # í‰ê°€ ì„¸ì…˜ ID ê´€ë¦¬
        self.user_evaluation_sessions: Dict[str, str] = {}  # user_id -> session_id
    
    def get_user_session(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        if user_id not in self.user_sessions:
            self.user_sessions[user_id] = {
                "audio_buffer": bytearray(),
                "is_speaking": False,
                "silence_duration": 0,
                "min_speech_duration": 1.0,
                "max_silence_duration": 1.0,  # ë¹ ë¥¸ ì‘ë‹µ
                "is_processing": False,  # STT ì²˜ë¦¬ ì¤‘ í”Œë˜ê·¸
                "should_cancel": False,  # ì²˜ë¦¬ ì·¨ì†Œ í”Œë˜ê·¸
                "conversation_ended": False,  # ëŒ€í™” ì¢…ë£Œ í”Œë˜ê·¸
                "scenario_id": None,  # ì„ íƒëœ ì‹œë‚˜ë¦¬ì˜¤
                "session_start_time": None,  # ì„¸ì…˜ ì‹œì‘ ì‹œê°„
            }
        return self.user_sessions[user_id]
    
    def clear_user_session(self, user_id: str):
        """ì‚¬ìš©ì ì„¸ì…˜ ì •ë¦¬"""
        if user_id in self.user_sessions:
            del self.user_sessions[user_id]
        if user_id in self.user_evaluation_sessions:
            del self.user_evaluation_sessions[user_id]
    
    async def detect_voice_activity(self, audio_chunk: bytes) -> bool:
        """ìŒì„± í™œë™ ê°ì§€ (VAD)"""
        try:
            if len(audio_chunk) == 0:
                return False
            
            # 16-bit PCMìœ¼ë¡œ ë³€í™˜
            audio_data = np.frombuffer(audio_chunk, dtype=np.int16)
            
            if len(audio_data) == 0:
                return False
            
            # RMS ê³„ì‚°ìœ¼ë¡œ ìŒì„± ë ˆë²¨ ì¸¡ì •
            rms = np.sqrt(np.mean(audio_data.astype(np.float32) ** 2))
            
            # ë™ì  ì„ê³„ê°’ (í™˜ê²½ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
            voice_threshold = 1000  # ë” ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            
            return rms > voice_threshold
            
        except Exception as e:
            logger.error(f"VAD ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _convert_buffer_to_numpy(self, audio_buffer: bytearray) -> np.ndarray:
        """ì˜¤ë””ì˜¤ ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # 16-bit PCMìœ¼ë¡œ ë³€í™˜
            audio_data = np.frombuffer(audio_buffer, dtype=np.int16)
            
            # float32ë¡œ ì •ê·œí™” (-1.0 ~ 1.0)
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            return audio_float
            
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ ë²„í¼ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return np.array([])
    
    async def _perform_stt_from_buffer(self, audio_numpy: np.ndarray) -> str:
        """numpy ë°°ì—´ì—ì„œ ì§ì ‘ STT ì²˜ë¦¬ (Google Cloud Speech)"""
        try:
            if len(audio_numpy) == 0:
                return ""
            
            # STT ì„œë¹„ìŠ¤ë¡œ ì²˜ë¦¬
            result = await service_manager.stt_service.transcribe_from_buffer(audio_numpy)
            return result
            
        except Exception as e:
            logger.error(f"STT ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return ""
    
    async def process_complete_utterance(self, websocket: WebSocket, user_id: str, audio_buffer: bytearray, session: dict):
        """ì™„ì „í•œ ë°œí™” ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë²„í¼ ì§ì ‘ ì‚¬ìš©)"""
        if len(audio_buffer) == 0:
            return
        
        try:
            logger.info(f"[{user_id}] ë°œí™” ì™„ë£Œ, STT ì²˜ë¦¬ ì‹œì‘")
            
            # ì·¨ì†Œ í™•ì¸
            if session["should_cancel"]:
                logger.info(f"[{user_id}] â¹ï¸  ì²˜ë¦¬ ì·¨ì†Œë¨ (ìƒˆ ë°œí™” ê°ì§€)")
                return
            
            # ì²˜ë¦¬ ì¤‘ ìƒíƒœ ì „ì†¡
            await websocket.send_text(json.dumps({
                "type": "processing",
                "message": "ìŒì„±ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "avatar_action": "thinking"
            }, ensure_ascii=False))
            
            # ì·¨ì†Œ í™•ì¸
            if session["should_cancel"]:
                logger.info(f"[{user_id}] â¹ï¸  ì²˜ë¦¬ ì·¨ì†Œë¨ (ìƒˆ ë°œí™” ê°ì§€)")
                return
            
            # ì˜¤ë””ì˜¤ ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (íŒŒì¼ ì €ì¥ ì—†ì´)
            audio_numpy = self._convert_buffer_to_numpy(audio_buffer)
            
            # ì·¨ì†Œ í™•ì¸
            if session["should_cancel"]:
                logger.info(f"[{user_id}] â¹ï¸  ì²˜ë¦¬ ì·¨ì†Œë¨ (ìƒˆ ë°œí™” ê°ì§€)")
                return
            
            # STT ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë²„í¼ ì§ì ‘ ì‚¬ìš©)
            user_text = await self._perform_stt_from_buffer(audio_numpy)
            
            # ì·¨ì†Œ í™•ì¸
            if session["should_cancel"]:
                logger.info(f"[{user_id}] â¹ï¸  ì²˜ë¦¬ ì·¨ì†Œë¨ (ìƒˆ ë°œí™” ê°ì§€)")
                return
            
            if user_text:
                logger.info(f"[{user_id}] STT ê²°ê³¼: {user_text}")
                
                # ì‚¬ìš©ì ë°œí™” í ì ì¬ (ì˜¤ë””ì˜¤ ë²„í¼ ì§ì ‘ ì „ë‹¬)
                if user_id in self.user_evaluation_sessions:
                    session_id = self.user_evaluation_sessions[user_id]
                    session.setdefault("seq", 0)
                    session["seq"] += 1
                    asyncio.create_task(
                        self._enqueue_with_retry(
                            enqueue_user_utterance,
                            session_id,
                            user_id,
                            session["seq"],
                            audio_buffer,  # ì˜¤ë””ì˜¤ ë²„í¼ ì§ì ‘ ì „ë‹¬
                            user_text,
                        )
                    )
                
                # AI ì‘ë‹µ ìƒì„± (ì˜¤ë””ì˜¤ ë²„í¼ ì§ì ‘ ì „ë‹¬)
                response_data = await self._generate_ai_response(user_id, user_text, audio_buffer)
                
            # ì·¨ì†Œ í™•ì¸ (ë§ˆì§€ë§‰ ì²´í¬)
                if session["should_cancel"]:
                    logger.info(f"[{user_id}] â¹ï¸  ì²˜ë¦¬ ì·¨ì†Œë¨ (ìƒˆ ë°œí™” ê°ì§€)")
                    return
                
                # ëŒ€í™” ì¢…ë£Œ í™•ì¸ ë° ì„¸ì…˜ì— í”Œë˜ê·¸ ì„¤ì •
                if response_data.get("conversation_ended", False):
                    session["conversation_ended"] = True
                    logger.info(f"ğŸ [{user_id}] ì„¸ì…˜ì— ëŒ€í™” ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì • - ì´í›„ ìŒì„± ì²˜ë¦¬ ì°¨ë‹¨")
                
                # WebSocketìœ¼ë¡œ ì‘ë‹µ ì „ì†¡
                await websocket.send_text(json.dumps(response_data, ensure_ascii=False))

                # ë°±ê·¸ë¼ìš´ë“œ í ì ì¬ (AI ë°œí™” / ì¢…ë£Œ)
                if user_id in self.user_evaluation_sessions:
                    session_id = self.user_evaluation_sessions[user_id]
                    if not response_data.get("conversation_ended", False):
                        # AI ë°œí™” í ì ì¬ (ë¹„ë™ê¸°, ë°±ì˜¤í”„ ì¬ì‹œë„)
                        session.setdefault("seq", 0)
                        session["seq"] += 1
                        asyncio.create_task(
                            self._enqueue_with_retry(
                                enqueue_ai_utterance,
                                session_id,
                                user_id,
                                session["seq"],
                                response_data.get("tts_audio_buffer"),  # TTS ë²„í¼ ì§ì ‘ ì „ë‹¬
                                response_data.get("ai_text", ""),
                            )
                        )
                    else:
                        # ì¢…ë£Œ ì‹ í˜¸ í ì ì¬ í›„ ì†Œì¼“ ì¢…ë£Œ (ë¹„ë™ê¸°, ë°±ì˜¤í”„ ì¬ì‹œë„)
                        session.setdefault("seq", 0)
                        session["seq"] += 1
                        asyncio.create_task(
                            self._enqueue_with_retry(
                                enqueue_conversation_ended,
                                session_id,
                                user_id,
                                session["seq"],
                            )
                        )
                        try:
                            await websocket.close(code=1000)
                        except Exception:
                            pass
            else:
                # ìŒì„± ì¸ì‹ ì‹¤íŒ¨
                if not session["should_cancel"]:
                    await websocket.send_text(json.dumps({
                        "type": "no_speech",
                        "message": "ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                        "avatar_action": "listening"
                    }, ensure_ascii=False))
                
        except Exception as e:
            logger.error(f"ë°œí™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            if not session["should_cancel"]:
                await websocket.send_text(json.dumps({
                    "type": "error",
                    "message": "ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "avatar_action": "error"
                }, ensure_ascii=False))
    
    def _correct_medical_terms(self, text: str) -> str:
        """í•œêµ­ì–´ ì˜ë£Œ ìš©ì–´ ì˜¤ì¸ì‹ êµì •"""
        medical_corrections = {
            "ìœ„ì—¼": ["ìœ„ì—½", "ìœ„ì—´"],
            "ë³µí†µ": ["ë³µí‰", "ë³µíŠ¼", "ë³µí†¤"],
            "ë‘í†µ": ["ë‘íŠ¼", "ë‘í†¤", "íˆ¬í†µ"],
            "ì–´ì§€ëŸ¼": ["ì–´ì§€ëŸ¼ì¦", "ì–´ì§€ëŸ¬ì›€"],
            "êµ¬í† ": ["êµ´í† ", "ì¿ í† "],
            "ì„¤ì‚¬": ["ì…œì‚¬", "ì„¤ì‹¸"]
        }
        
        for correct, variants in medical_corrections.items():
            for variant in variants:
                if variant in text and variant != correct:
                    text = text.replace(variant, correct)
                    logger.info(f"ì˜ë£Œ ìš©ì–´ êµì •: {variant} â†’ {correct}")
        
        return text
    
    async def _generate_ai_response(self, user_id: str, user_text: str, audio_buffer: bytearray = None) -> Dict[str, Any]:
        """AI ì‘ë‹µ ìƒì„± (ë©”ëª¨ë¦¬ ë²„í¼ ê¸°ë°˜)"""
        try:
            # ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = self.get_user_session(user_id)
            
            # ì…ë ¥ ë¡œê¹…
            print(f"\nğŸ¤ ì‚¬ìš©ì ì…ë ¥: '{user_text}'")
            
            # LLM ì‘ë‹µ ìƒì„± (ê³ ì •ëœ ì‹œë‚˜ë¦¬ì˜¤ ì‚¬ìš©)
            llm_response = await service_manager.llm_service.generate_response(user_text, user_id)
            response_text = llm_response["text"]
            conversation_ended = llm_response["conversation_ended"]
            
            # ì¶œë ¥ ë¡œê¹…
            print(f"ğŸ¤– AI ì‘ë‹µ: '{response_text}'")
            
            # TTS ìƒì„± (ë©”ëª¨ë¦¬ ë²„í¼ë¡œ ë°˜í™˜)
            tts_audio_buffer = await service_manager.tts_service.generate_speech(response_text)
            logger.info(f"ğŸ”Š TTS ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ (ë©”ëª¨ë¦¬ ë²„í¼)")
            
            # TTS ì˜¤ë””ì˜¤ ë²„í¼ë¥¼ Base64ë¡œ ì¸ì½”ë”©
            tts_audio_base64 = None
            if tts_audio_buffer:
                tts_audio_base64 = base64.b64encode(tts_audio_buffer).decode('utf-8')
                logger.info(f"ğŸ”Š TTS ì˜¤ë””ì˜¤ Base64 ì¸ì½”ë”© ì™„ë£Œ ({len(tts_audio_base64)} ë¬¸ì)")
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
            response_data = {
                "type": "voice_response",
                "user_text": user_text,
                "ai_text": response_text,
                "tts_audio_base64": tts_audio_base64,  # Base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤
                "avatar_action": "talking",
                "processing_time": "ì‹¤ì‹œê°„",
                "conversation_ended": conversation_ended,
            }
            
            # ëŒ€í™” ì¢…ë£Œ ì‹œ íŠ¹ë³„ ì²˜ë¦¬(ì‘ë‹µ êµ¬ì„±ë§Œ ë³€ê²½). í‰ê°€ëŠ” ì›Œì»¤ê°€ ì¢…ë£Œ ì´ë²¤íŠ¸ + pendingìœ¼ë¡œ íŠ¸ë¦¬ê±°
            if conversation_ended:
                response_data["type"] = "conversation_ended"
                response_data["avatar_action"] = "goodbye"
                response_data["message"] = "ì§„ë£Œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í‰ê°€ ê²°ê³¼ëŠ” ê³§ ì €ì¥ë©ë‹ˆë‹¤."
            
            return response_data
            
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "type": "error",
                "message": "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "avatar_action": "error",
                "conversation_ended": False
            }

    async def _enqueue_with_retry(self, func, *args, **kwargs):
        """ë¹„ë™ê¸° í ì ì¬ì— ëŒ€í•œ ê°„ë‹¨í•œ ë°±ì˜¤í”„ ì¬ì‹œë„ ë˜í¼
        - func: enqueue_user_utterance/enqueue_ai_utterance/enqueue_conversation_ended
        - args: í•´ë‹¹ í•¨ìˆ˜ ì¸ì ê·¸ëŒ€ë¡œ ì „ë‹¬
        ì •ì±…: 3íšŒ ì¬ì‹œë„, 0.1s, 0.3s, 0.9s ì§€ìˆ˜ ë°±ì˜¤í”„
        ì‹¤íŒ¨ ì‹œ ë¡œê¹…ë§Œ í•˜ê³  ë“œë¡­(ìš”êµ¬ì‚¬í•­ìƒ ìœ ì‹¤ í—ˆìš©)
        """
        delays = [0.1, 0.3, 0.9]
        last_exc = None
        for i, delay in enumerate([0.0] + delays):
            if delay > 0:
                await asyncio.sleep(delay)
            try:
                await func(*args, **kwargs)
                return True
            except Exception as e:
                last_exc = e
                logger.warning(
                    f"enqueue retry {i}/{len(delays)} failed: func={getattr(func, '__name__', str(func))}, err={e}"
                )
        logger.error(
            f"enqueue failed after retries: func={getattr(func, '__name__', str(func))}, args={args}, kwargs={kwargs}, err={last_exc}"
        )
        return False

# ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
audio_processor = AudioProcessor()

@router.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    """WebSocket ì‹¤ì‹œê°„ ìŒì„± ìŠ¤íŠ¸ë¦¬ë°"""
    await websocket.accept()
    logger.info(f"ğŸ”— ì‹¤ì‹œê°„ ìŒì„± ì—°ê²°: {user_id}")
    
    # ì‚¬ìš©ì ì„¸ì…˜ ì´ˆê¸°í™”
    session = audio_processor.get_user_session(user_id)
    
    try:
        # ì›Œì»¤ ì‹œì‘ (ìµœì´ˆ 1íšŒ)
        start_worker_once()


        # ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤(ì¹˜ë§¤) ì„¤ì • ë° í‰ê°€ ì„¸ì…˜ ì‹œì‘
        default_scenario_id = "1"  # ì¹˜ë§¤ ì‹œë‚˜ë¦¬ì˜¤
        session["scenario_id"] = default_scenario_id
        session["session_start_time"] = datetime.now().isoformat()
        
        # CPX ê²°ê³¼ ìƒì„± (í‰ê°€ ì‹œì‘)
        cpx_result_id = None
        async for db in get_db():
            cpx_service = CpxService(db)
            cpx_result = await cpx_service.create_cpx_result(
                student_id=int(user_id),
                patient_name="AI í™˜ì",
                evaluation_status="ì§„í–‰ì¤‘"
            )
            cpx_result_id = cpx_result.result_id
            break
        
        # ì„¸ì…˜ì— CPX result_idì™€ user_id ì €ì¥
        session["result_id"] = cpx_result_id
        session["user_id"] = user_id
        
        # LLM ì„œë¹„ìŠ¤ì— ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
        service_manager.llm_service.select_scenario(default_scenario_id, user_id)
        
        # í‰ê°€ ì„¸ì…˜ ì‹œì‘ (result_id ì „ë‹¬)
        eval_session_id = await service_manager.evaluation_service.start_evaluation_session(
            user_id, default_scenario_id, cpx_result_id
        )
        audio_processor.user_evaluation_sessions[user_id] = eval_session_id
        
        print(f"ğŸ­ [{user_id}] ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤({default_scenario_id}) ì„¤ì • ë° í‰ê°€ ì„¸ì…˜ ì‹œì‘: {eval_session_id}")
        
        # ì‹œì‘ ë©”ì‹œì§€ ì „ì†¡
        await websocket.send_text(json.dumps({
            "type": "session_started",
            "message": f"ğŸ¥ CPX ì‹œìŠ¤í…œì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤! ({user_id})\n\n ê¸°ì–µë ¥ ì €í•˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\nì§€ê¸ˆë¶€í„° í™˜ìì—ê²Œ ë§ì„ ê±¸ì–´ë³´ì„¸ìš”.",
            "scenario_id": default_scenario_id,
            "result_id": cpx_result_id,
            "avatar_action": "ready"
        }, ensure_ascii=False))
        
        while True:
            # ì‹¤ì‹œê°„ ë©”ì‹œì§€ ìˆ˜ì‹ 
            message = await websocket.receive()
            
            if message["type"] == "websocket.receive":
                if "bytes" in message:
                    # ìŒì„± ì²­í¬ ì²˜ë¦¬
                    await handle_audio_chunk(websocket, user_id, message["bytes"], session)
                    
    except WebSocketDisconnect:
        logger.info(f"ğŸ”Œ ì—°ê²° í•´ì œ: {user_id}")
    except Exception as e:
        logger.error(f"WebSocket ì˜¤ë¥˜: {e}")
    finally:
        # í‰ê°€ ì„¸ì…˜ ë³´í˜¸: í‰ê°€ ì§„í–‰ ì¤‘ì´ë©´ ì •ë¦¬ ì—°ê¸°
        if user_id in audio_processor.user_evaluation_sessions:
            session_id = audio_processor.user_evaluation_sessions[user_id]
            print(f"âš ï¸ [{user_id}] í‰ê°€ ì„¸ì…˜ ë³´í˜¸ - ë°±ê·¸ë¼ìš´ë“œ í‰ê°€ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°: {session_id}")
            # í‰ê°€ ì„¸ì…˜ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì •ë¦¬í•˜ë„ë¡ í•¨
        
        # WebSocket ì„¸ì…˜ë§Œ ì •ë¦¬ (í‰ê°€ ì„¸ì…˜ì€ ë³´ì¡´)
        if user_id in audio_processor.user_sessions:
            del audio_processor.user_sessions[user_id]
        
        # LLM ì„œë¹„ìŠ¤ì˜ ì‚¬ìš©ì ìƒíƒœë„ ì •ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        service_manager.llm_service.clear_user_memory(user_id)
        logger.info(f"ğŸ§¹ [{user_id}] WebSocket ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ (í‰ê°€ ì„¸ì…˜ ë³´ì¡´)")

async def handle_audio_chunk(websocket: WebSocket, user_id: str, audio_chunk: bytes, session: Dict):
    """ìŒì„± ì²­í¬ ì²˜ë¦¬"""
    try:
        # ëŒ€í™” ì¢…ë£Œ í™•ì¸ - ì¢…ë£Œëœ ê²½ìš° ìŒì„± ì²˜ë¦¬ ì°¨ë‹¨
        if session.get("conversation_ended", False):
            logger.info(f"ğŸ”’ [{user_id}] ëŒ€í™” ì¢…ë£Œë¨ - ìŒì„± ì²˜ë¦¬ ì°¨ë‹¨")
            return
        
        # ì˜¤ë””ì˜¤ ë²„í¼ì— ì¶”ê°€
        session["audio_buffer"].extend(audio_chunk)
        
        # ìŒì„± í™œë™ ê°ì§€
        has_voice = await audio_processor.detect_voice_activity(audio_chunk)
        
        if has_voice:
            if not session["is_speaking"]:
                # ìƒˆë¡œìš´ ë°œí™” ì‹œì‘ - ê¸°ì¡´ ì²˜ë¦¬ ì¤‘ì´ë©´ ì·¨ì†Œ
                if session["is_processing"]:
                    logger.info(f"[{user_id}] ğŸ”„ ìƒˆ ë°œí™” ê°ì§€ - ê¸°ì¡´ ì²˜ë¦¬ ì·¨ì†Œ")
                    session["should_cancel"] = True
                    session["is_processing"] = False
                    # ê¸°ì¡´ ë²„í¼ ì´ˆê¸°í™”
                    session["audio_buffer"].clear()
                
                logger.info(f"[{user_id}] ğŸ¤ ë°œí™” ì‹œì‘")
                session["is_speaking"] = True
                session["silence_duration"] = 0
                session["should_cancel"] = False
                
                # ë¦¬ìŠ¤ë‹ ìƒíƒœ ì „ì†¡
                await websocket.send_text(json.dumps({
                    "type": "listening",
                    "message": "ìŒì„±ì„ ë“£ê³  ìˆìŠµë‹ˆë‹¤...",
                    "avatar_action": "listening"
                }, ensure_ascii=False))
            
            session["silence_duration"] = 0
            
            # ì§§ì€ ë°œí™”ëŠ” í˜¸ì‘ì–´ì¼ ê°€ëŠ¥ì„± - ë” ë¹ ë¥´ê²Œ ì²˜ë¦¬
            if len(session["audio_buffer"]) > 0:
                buffer_duration = len(session["audio_buffer"]) / (16000 * 2)  # ì´ˆ ë‹¨ìœ„
                if buffer_duration > 0.3 and buffer_duration < 1.5:  # 0.3ì´ˆ~1.5ì´ˆ ì§§ì€ ë°œí™”
                    session["max_silence_duration"] = 0.7  # í˜¸ì‘ì–´ëŠ” ë” ë¹ ë¥´ê²Œ ì²˜ë¦¬
                else:
                    session["max_silence_duration"] = 1.0  # ì¼ë°˜ ë°œí™”
        else:
            # ì¹¨ë¬µ êµ¬ê°„
            if session["is_speaking"]:
                session["silence_duration"] += 0.1  # 100ms ë‹¨ìœ„
                
                # ì¶©ë¶„í•œ ì¹¨ë¬µì‹œ ë°œí™” ì¢…ë£Œ ì²˜ë¦¬
                if session["silence_duration"] >= session["max_silence_duration"]:
                    # STT ì²˜ë¦¬ ì‹œì‘ í‘œì‹œ
                    session["is_processing"] = True
                    
                    await audio_processor.process_complete_utterance(
                        websocket, user_id, session["audio_buffer"], session
                    )
                    
                    # ì·¨ì†Œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì„¸ì…˜ ì´ˆê¸°í™”
                    if not session["should_cancel"]:
                        session["audio_buffer"].clear()
                        session["is_speaking"] = False
                        session["silence_duration"] = 0
                        session["is_processing"] = False
                    
    except Exception as e:
        logger.error(f"ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
