"""
Wav2Vec2 ê¸°ë°˜ ìŒì„± ê°ì • ë¶„ì„ ëª¨ë¸
"""

import torch
import torch.nn as nn
from transformers import (
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2Config,
    Wav2Vec2Processor
)
from typing import Dict, Any, Optional
from .config import model_config

class SpeechEmotionClassifier(nn.Module):
    """ìŒì„± ê°ì • ë¶„ì„ì„ ìœ„í•œ Wav2Vec2 ëª¨ë¸ (kresnik/wav2vec2-large-xlsr-korean ê¸°ë°˜)"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__()
        
        if config is None:
            config = {}
            
        self.model_name = config.get('model_name', model_config.model_name)
        self.num_labels = config.get('num_labels', model_config.num_labels)
        
        print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {self.model_name}")
        print(f"ğŸ“Š ê°ì • í´ë˜ìŠ¤ ìˆ˜: {self.num_labels}")
        
        # Wav2Vec2 ì„¤ì • ë¡œë“œ ë° ìˆ˜ì •
        self.wav2vec2_config = Wav2Vec2Config.from_pretrained(
            self.model_name,
            num_labels=self.num_labels,
            label2id=model_config.label2id,
            id2label=model_config.id2label,
            finetuning_task="emotion_classification",
            # kresnik/wav2vec2-large-xlsr-korean ëª¨ë¸ì— ìµœì í™”ëœ ì„¤ì •
            attention_dropout=0.1,
            hidden_dropout=0.1,
            feat_proj_dropout=0.0,
            mask_time_prob=0.05,
            layerdrop=0.1,
        )
        
        # ëª¨ë¸ ë¡œë“œ - ASR ëª¨ë¸ì„ ê°ì • ë¶„ì„ìš©ìœ¼ë¡œ ë³€ê²½
        try:
            self.model = Wav2Vec2ForSequenceClassification.from_pretrained(
                self.model_name,
                config=self.wav2vec2_config,
                ignore_mismatched_sizes=True
            )
            print("âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ ì‚¬ì „ í›ˆë ¨ëœ ë¶„ë¥˜ í—¤ë“œ ë¡œë”© ì‹¤íŒ¨, ìƒˆë¡œìš´ í—¤ë“œ ìƒì„±: {e}")
            # ASR ëª¨ë¸ì—ì„œ íŠ¹ì„± ì¶”ì¶œê¸°ë§Œ ë¡œë“œí•˜ê³  ìƒˆë¡œìš´ ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€
            from transformers import Wav2Vec2Model
            base_model = Wav2Vec2Model.from_pretrained(self.model_name)
            self.model = Wav2Vec2ForSequenceClassification(self.wav2vec2_config)
            self.model.wav2vec2 = base_model
        
        # Processor ë¡œë“œ
        self.processor = Wav2Vec2Processor.from_pretrained(self.model_name)
        
        # Feature extractor ë™ê²° (ì„ íƒì )
        self._freeze_feature_extractor()
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
        self._print_model_info()
    
    def _freeze_feature_extractor(self):
        """Feature extractor ê°€ì¤‘ì¹˜ ë™ê²° (í•œêµ­ì–´ ASR ê°€ì¤‘ì¹˜ ë³´ì¡´)"""
        self.model.wav2vec2.feature_extractor._freeze_parameters()
        print("ğŸ”’ Feature extractor ê°€ì¤‘ì¹˜ ë™ê²° ì™„ë£Œ")
    
    def _print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ“ˆ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´:")
        print(f"  ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"  í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        print(f"  ë™ê²°ëœ íŒŒë¼ë¯¸í„°: {total_params - trainable_params:,}")
        print(f"  í›ˆë ¨ ê°€ëŠ¥ ë¹„ìœ¨: {100 * trainable_params / total_params:.1f}%")
    
    def forward(self, input_values, attention_mask=None, labels=None):
        """ìˆœì „íŒŒ"""
        outputs = self.model(
            input_values=input_values,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs
    
    def predict(self, audio_input, return_probabilities=False):
        """ë‹¨ì¼ ì˜¤ë””ì˜¤ì— ëŒ€í•œ ì˜ˆì¸¡ (kresnik/wav2vec2-large-xlsr-korean ìµœì í™”)"""
        self.eval()
        with torch.no_grad():
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            if isinstance(audio_input, str):
                # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° ì˜¤ë””ì˜¤ ë¡œë“œ
                from .preprocessing import preprocessor
                audio_array = preprocessor.preprocess(audio_input, apply_augmentation=False)
                if audio_array is None:
                    raise ValueError(f"ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {audio_input}")
                audio_input = audio_array
            
            if isinstance(audio_input, torch.Tensor):
                input_values = audio_input.unsqueeze(0) if audio_input.dim() == 1 else audio_input
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                input_values = input_values.to(next(self.model.parameters()).device)
            else:
                # numpy arrayì¸ ê²½ìš° - kresnik ëª¨ë¸ì— ìµœì í™”
                inputs = self.processor(
                    audio_input,
                    sampling_rate=model_config.sampling_rate,
                    return_tensors="pt",
                    padding=True,
                    max_length=model_config.sampling_rate * int(model_config.max_duration),
                    truncation=True
                )
                input_values = inputs.input_values.to(next(self.model.parameters()).device)
            
            # ì˜ˆì¸¡
            outputs = self.forward(input_values)
            logits = outputs.logits
            
            if return_probabilities:
                probabilities = torch.nn.functional.softmax(logits, dim=-1)
                return probabilities.cpu().numpy()
            else:
                predicted_ids = torch.argmax(logits, dim=-1)
                predicted_labels = [model_config.id2label[pred_id.item()] 
                                  for pred_id in predicted_ids]
                return predicted_labels[0] if len(predicted_labels) == 1 else predicted_labels
    
    def get_embeddings(self, input_values, attention_mask=None):
        """ì¤‘ê°„ í‘œí˜„ ì¶”ì¶œ"""
        with torch.no_grad():
            outputs = self.model.wav2vec2(
                input_values=input_values,
                attention_mask=attention_mask
            )
            # Last hidden stateì˜ í‰ê· ì„ ì‚¬ìš©
            embeddings = outputs.last_hidden_state.mean(dim=1)
            return embeddings
    
    def save_model(self, save_path: str):
        """ëª¨ë¸ ì €ì¥"""
        self.model.save_pretrained(save_path)
        self.processor.save_pretrained(save_path)
    
    @classmethod
    def from_pretrained(cls, model_path: str):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        instance = cls()
        instance.model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)
        instance.processor = Wav2Vec2Processor.from_pretrained(model_path)
        return instance

class EmotionHead(nn.Module):
    """ì»¤ìŠ¤í…€ ê°ì • ë¶„ë¥˜ í—¤ë“œ"""
    
    def __init__(self, input_dim: int, num_emotions: int, dropout_rate: float = 0.1):
        super().__init__()
        
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(input_dim // 2, num_emotions)
        )
    
    def forward(self, hidden_states):
        # Global average pooling
        pooled_output = hidden_states.mean(dim=1)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

def create_model(config: Optional[Dict[str, Any]] = None) -> SpeechEmotionClassifier:
    """ëª¨ë¸ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return SpeechEmotionClassifier(config)