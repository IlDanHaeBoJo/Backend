#!/usr/bin/env python3
"""
ì§ì ‘ í›ˆë ¨ ë£¨í”„ êµ¬í˜„ ë²„ì „ (Trainer ì‚¬ìš© ì•ˆí•¨)
+ ì ëŒ€ì  í•™ìŠµ ì¶”ê°€
"""

import os
import sys
import json
import re
import random
from collections import Counter
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import librosa
from audiomentations import Compose, PitchShift, TimeStretch, AddGaussianNoise, Shift, Gain, RoomSimulator, HighPassFilter, LowPassFilter
from typing import List, Tuple, Optional, Dict, Any, Literal
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

from tqdm import tqdm
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, f1_score, classification_report

from transformers import (
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2Processor,
    Wav2Vec2Config
)
# ì ëŒ€ì  í•™ìŠµì„ ìœ„í•œ custom model ì„í¬íŠ¸
from Wav2Vec2_seq_clf import custom_Wav2Vec2ForEmotionClassification
from data_utils import *
from datasets import EmotionDataset, collate_fn
from config import Config


# ê°ì • ë¼ë²¨ ì •ì˜ -> config.pyë¡œ ì˜®ê¹€
"""
"Anxious" : 0
"Kind" : 1
"Dry" : 2
"""
EMOTION_LABELS = ["Anxious", "Dry", "Kind"]
LABEL2ID = {label: i for i, label in enumerate(EMOTION_LABELS)}
ID2LABEL = {i: label for i, label in enumerate(EMOTION_LABELS)}

MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
SAMPLE_RATE = 16000
MAX_DURATION = 10.0


# Character Vocabulary ë¡œë“œ
try:
    with open('char_to_id.json', 'r', encoding='utf-8') as f:
        CHAR2ID = json.load(f)
    CHAR_VOCAB = list(CHAR2ID.keys())
    ID2CHAR = {i: char for char, i in CHAR2ID.items()}
    print(f"âœ… Character Vocabulary ë¡œë“œ ì™„ë£Œ ({len(CHAR_VOCAB)}ê°œ)")
except FileNotFoundError:
    print("âŒ 'char_to_id.json'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vocab.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)



def build_augment(epoch, total_epochs):
    scale = 0.6 if epoch < 0.2 * total_epochs else 1.0

    return Compose([
        RoomSimulator(p=0.20 * scale),
        HighPassFilter(min_cutoff_freq=60, max_cutoff_freq=120, p=0.15 * scale),
        LowPassFilter(min_cutoff_freq=3500, max_cutoff_freq=6000, p=0.15 * scale),

        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.006 if scale<1 else 0.008, p=0.35 * scale),
        Gain(min_gain_in_db=-2.0 if scale<1 else -3.0,
             max_gain_in_db= 2.0 if scale<1 else  3.0, p=0.35 * scale),

        Shift(min_fraction=-0.03 if scale<1 else -0.05,
              max_fraction= 0.03 if scale<1 else  0.05, p=0.35 * scale),

        PitchShift(min_semitones=-1, max_semitones=1, p=0.20 * scale),
        TimeStretch(min_rate=0.98 if scale<1 else 0.97,
                    max_rate=1.02 if scale<1 else 1.03, p=0.15 * scale),
    ])





def create_model_and_processor(freeze_base_model: bool = True, num_speakers: int = 500):
    """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {MODEL_NAME}")
    
    # ì„¤ì • ìˆ˜ì •
    config = Wav2Vec2Config.from_pretrained(
        MODEL_NAME,
        num_labels=len(EMOTION_LABELS),
        label2id=LABEL2ID,
        id2label=ID2LABEL,
        finetuning_task="emotion_classification"
    )
    config.char_vocab_size = len(CHAR_VOCAB) # ì ëŒ€ì ëª¨ë¸ì„ ìœ„í•œ ì„¤ì •
    config.num_speakers = num_speakers
    
    model = custom_Wav2Vec2ForEmotionClassification.from_pretrained(
        MODEL_NAME,
        config=config,
        ignore_mismatched_sizes=True
    )
    
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    
    if freeze_base_model:
        for param in model.wav2vec2.parameters():
            param.requires_grad = False

    return model, processor

def enable_last_k_blocks(model, last_k: int = 4):
    for p in model.wav2vec2.parameters():
        p.requires_grad = False
    
    layers = model.wav2vec2.encoder.layers
    num_layers = len(layers)
    for i in range(num_layers - last_k, num_layers):
        for p in layers[i].parameters():
            p.requires_grad = True
    
    for name, module in model.named_modules():
        if any(k in name for k in ["classifier", "adversary", "speaker_adversary", "pooler", "stats_projector", "projector"]):
            for p in module.parameters():
                p.requires_grad = True

def evaluate_model(model, dataloader, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="í‰ê°€ ì¤‘"):
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.0, # í‰ê°€ ì‹œì—ëŠ” ì ëŒ€ì  ì†ì‹¤ ë°˜ì˜ ì•ˆí•¨,
                speaker_ids = None,
            )
            
            loss = outputs['loss']
            
            # í›ˆë ¨ ì¤‘ì´ ì•„ë‹ ë•Œë„ lossê°€ ê³„ì‚°ë˜ë„ë¡ adv_lambda=0.0ìœ¼ë¡œ í˜¸ì¶œ
            # ë§Œì•½ lossê°€ Noneì´ë©´ (test set ë“±ì—ì„œ content_labelì´ ì—†ì„ ê²½ìš°), ê°ì • ì†ì‹¤ë§Œ ê³„ì‚°
            if loss is None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(outputs['emotion_logits'].view(-1, model.config.num_labels), batch['labels'].to(device).view(-1))

            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(batch['labels'].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'f1': f1,
        'predictions': predictions,
        'true_labels': true_labels
    }

def train_model(model, train_loader, val_loader, device, num_epochs=3, learning_rate=3e-5):
    """ì§ì ‘ í›ˆë ¨ ë£¨í”„"""
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)
    print("ğŸš€ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)")
    backbone_params = []
    head_params = []

    for n, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if n.startswith("wav2vec2."):
            backbone_params.append(p)
        else:
            # pooler, stats_projector, projector(ë¶€ëª¨), classifier, adversaries ë“±
            head_params.append(p)

    for n, p in model.adversary.named_parameters():
        p.requires_grad = False
    model.adversary.eval()

    optimizer = optim.AdamW(
        [
            {"params": backbone_params, "lr": 5e-6, "weight_decay": 0.01},
            {"params": head_params,     "lr": 1e-4, "weight_decay": 0.01},
        ]
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * num_epochs
    accumulation_steps = 16
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)
    
    best_f1 = 0
    best_model_state = None
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
    print(f"   ì´ ì—í¬í¬: {num_epochs}")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    max_adv = 0.05
    warmup_epochs = 1.0
    class_weights = torch.tensor([2.0, 1.5, 0.7], dtype=torch.float32).to(device)


    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
        
        # í›ˆë ¨
        model.train()
        train_loss = 0
        optimizer.zero_grad()

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training")
        for step, batch in enumerate(progress_bar):
            current_step = epoch * len(train_loader) + step
            total_warmup_steps = warmup_epochs * len(train_loader)
            progress = min(1.0, current_step / total_warmup_steps)
            current_adv_lambda = max_adv * progress
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=current_adv_lambda,
                speaker_ids=batch['speaker_ids'].to(device),
                class_weights = class_weights
            )
            
            loss = outputs['loss']
            if loss is None or torch.isnan(loss) or torch.isinf(loss):
                continue

            loss /= accumulation_steps
            loss.backward()

            if (step + 1) % accumulation_steps == 0:
              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
              optimizer.step()
              scheduler.step()
              optimizer.zero_grad()
            
            train_loss += loss.item()
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr_backbone': f'{scheduler.get_last_lr()[0]:.6f}',
                'lr_head': f'{scheduler.get_last_lr()[1]:.6f}',
                "adv": f'{adv_lambda:.3f}'
            })
        
        train_loss /= len(train_loader)
        
        # ê²€ì¦
        print("ğŸ“Š ê²€ì¦ ì¤‘...")
        val_results = evaluate_model(model, val_loader, device)
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, F1: {val_results['f1']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_results['f1'] > best_f1:
            best_f1 = val_results['f1']
            best_model_state = model.state_dict().copy()
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’« ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1:.4f})")
    
    return model

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ§ª wav2vec2-large-xlsr-korean ì§ì ‘ í›ˆë ¨ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    print("="*70)
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    
    data_dir = "/data/ghdrnjs/SER/small/"
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {data_dir}")
    index = build_corpus_index(data_dir, require_emotion=True)
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
        split_speaker_and_content(
            index,
            val_content_ratio=0.2,
            test_content_ratio=0.2,
            val_speaker_ratio=0.2,
            test_speaker_ratio=0.2,
            seed=42,
            # ì˜ˆì‹œ) íŠ¹ì • ëŒ€í™” IDë¥¼ ê³ ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì£¼ì„ í•´ì œ
            # fixed_val_content_ids=[22, 35],
            # fixed_test_content_ids=[27, 41],
        )
    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ")
    print(f"  Test: {len(test_paths)}ê°œ")

    train_speakers = sorted({extract_speaker_id(p, data_dir) for p in train_paths})
    num_speakers = len(train_speakers)
    print(f"ğŸ” í™”ì ìˆ˜: {num_speakers}")

    # audio_paths, labels = load_dataset_subset(data_dir, max_per_class=3000)
    # print(f"âœ… ì´ {len(audio_paths)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    
    # ë°ì´í„° ë¶„í•  (íŒŒì¼ëª… ë§ˆì§€ë§‰ ìˆ«ì ê¸°ì¤€)
    
    # (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = split_data_by_last_digit(audio_paths, labels)
    
    # ê° ì„¸íŠ¸ì˜ ê°ì •ë³„ ë¶„í¬ í™•ì¸
    from collections import Counter
    train_emotion_dist = Counter(train_labels)
    val_emotion_dist = Counter(val_labels)
    test_emotion_dist = Counter(test_labels)
    
    print(f"\nğŸ“ˆ ê°ì •ë³„ ë¶„í¬:")
    print(f"  Train: {dict(train_emotion_dist)}")
    print(f"  Validation: {dict(val_emotion_dist)}")
    print(f"  Test: {dict(test_emotion_dist)}")
    
    # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±
    model, processor = create_model_and_processor(num_speakers=num_speakers)
    enable_last_k_blocks(model, last_k=4)
    model.to(device)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ”„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    train_dataset = EmotionDataset(train_paths, train_labels, processor)
    val_dataset = EmotionDataset(val_paths, val_labels, processor, is_training=False)
    test_dataset = EmotionDataset(test_paths, test_labels, processor, is_training=False)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    batch_size = 2
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    try:
        # í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1ì—í¬í¬)
        model = train_model(model, train_loader, val_loader, device, num_epochs=5, learning_rate=3e-5)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€...")
        test_results = evaluate_model(model, test_loader, device)
        
        print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"   - F1 ìŠ¤ì½”ì–´: {test_results['f1']:.4f}")
        print(f"   - ì†ì‹¤: {test_results['loss']:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report = classification_report(
            test_results['true_labels'], 
            test_results['predictions'],
            target_names=EMOTION_LABELS,
            digits=4
        )
        print(report)
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        output_dir = "./results_quick_test"
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥ (transformers ë°©ì‹)
        model_save_path = os.path.join(output_dir, "adversary_no_content_speaker_model_augment_v3_epoch_5_last_k_4")
        os.makedirs(model_save_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(model_save_path)
        processor.save_pretrained(model_save_path)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"\nğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print(f"   ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸: python test_my_voice.py your_audio.wav --model_path {model_save_path}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
