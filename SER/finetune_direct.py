#!/usr/bin/env python3
"""
ì§ì ‘ í›ˆë ¨ ë£¨í”„ êµ¬í˜„ ë²„ì „ (Trainer ì‚¬ìš© ì•ˆí•¨)
+ ì ëŒ€ì  í•™ìŠµ ì¶”ê°€
"""

import os
import sys
import json
import re
import random
from collections import Counter
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import librosa
from audiomentations import Compose, PitchShift, TimeStretch, AddGaussianNoise, Shift, Gain, ClippingDistortion
from typing import List, Tuple, Optional, Dict, Any, Literal
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

from tqdm import tqdm
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, f1_score, classification_report

from transformers import (
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2Processor,
    Wav2Vec2Config
)
# ì ëŒ€ì  í•™ìŠµì„ ìœ„í•œ custom model ì„í¬íŠ¸
from Wav2Vec2_seq_clf import custom_Wav2Vec2ForEmotionClassification

# ê°ì • ë¼ë²¨ ì •ì˜
"""
"Anxious" : 0
"Kind" : 1
"Dry" : 2
"""
EMOTION_LABELS = ["Anxious", "Dry", "Kind"]
LABEL2ID = {label: i for i, label in enumerate(EMOTION_LABELS)}
ID2LABEL = {i: label for i, label in enumerate(EMOTION_LABELS)}

MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
SAMPLE_RATE = 16000
MAX_DURATION = 10.0

AUGMENTATION = Compose([
    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),
    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
    Gain(min_gain_in_db=-6.0, max_gain_in_db=6.0, p=0.5),
    ClippingDistortion(0, 2.0, p=0.3)

])

# Character Vocabulary ë¡œë“œ
try:
    with open('char_to_id.json', 'r', encoding='utf-8') as f:
        CHAR2ID = json.load(f)
    CHAR_VOCAB = list(CHAR2ID.keys())
    ID2CHAR = {i: char for char, i in CHAR2ID.items()}
    print(f"âœ… Character Vocabulary ë¡œë“œ ì™„ë£Œ ({len(CHAR_VOCAB)}ê°œ)")
except FileNotFoundError:
    print("âŒ 'char_to_id.json'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vocab.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def extract_number_from_filename(filename: str, type: Literal['content', 'emotion'] = 'emotion') -> Optional[int]:
    try:
        if type == "content":
            # íŒŒì¼ëª…ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì ê·¸ë£¹ ì „ì²´ë¥¼ ì¶”ì¶œ (ì˜ˆ: F2001_000123.wav -> 123)
            match = re.search(r'_(\d+)\.wav$', os.path.basename(filename))
            if match:
                return int(match.group(1))
            return None
        else:
            # F..._...xxxD.wav ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì Dë¥¼ ì¶”ì¶œ
            match = re.search(r'_(\d+)\.wav$', os.path.basename(filename))
            if match:
                # ìˆ«ìê°€ ì—¬ëŸ¬ ìë¦¬ì¼ ê²½ìš° ë§ˆì§€ë§‰ í•œ ìë¦¬ë§Œ ì‚¬ìš©
                return int(match.group(1)) % 10
            return None
    except (ValueError, AttributeError):
        return None

def split_data_by_last_digit(audio_paths: List[str], labels: List[str]) -> Tuple[
    Tuple[List[str], List[str]], 
    Tuple[List[str], List[str]], 
    Tuple[List[str], List[str]]
]:
    """íŒŒì¼ëª…ì˜ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ train/val/test ë¶„í• 
    
    Args:
        audio_paths: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        labels: í•´ë‹¹í•˜ëŠ” ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ((train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels))
        - Train: ë§ˆì§€ë§‰ ìˆ«ìê°€ 1,2,3,4,5,6
        - Validation: ë§ˆì§€ë§‰ ìˆ«ìê°€ 7,8
        - Test: ë§ˆì§€ë§‰ ìˆ«ìê°€ 9,0
    """
    train_paths, train_labels = [], []
    val_paths, val_labels = [], []
    test_paths, test_labels = [], []
    
    for path, label in zip(audio_paths, labels):
        last_digit = extract_number_from_filename(path, type="emotion")
        
        if last_digit is None:
            print(f"âš ï¸ íŒŒì¼ëª…ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            continue
            
        if last_digit in [1, 2, 3, 4, 5, 6]:
            train_paths.append(path)
            train_labels.append(label)
        elif last_digit in [7, 8]:
            val_paths.append(path)
            val_labels.append(label)
        elif last_digit in [9, 0]:
            test_paths.append(path)
            test_labels.append(label)
    
    return (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels)

def get_emotion_from_filename(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°ì • ë¼ë²¨ ë°˜í™˜"""
    file_num = extract_number_from_filename(filename, type="content")
    if file_num is None:
        return None
        
    if 21 <= file_num <= 30:
        return "Anxious"
    elif 31 <= file_num <= 40:
        return "Kind"
    elif 141 <= file_num <= 150:
        return "Dry"
    else:
        return None

def load_dataset_subset(data_dir: str, max_per_class: int) -> Tuple[List[str], List[str]]:
    audio_paths = []
    labels = []
    emotion_counts = {label: 0 for label in EMOTION_LABELS}
    
    person_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])
    print(f"ğŸ“ ë°œê²¬ëœ person í´ë”: {len(person_folders)}ê°œ")
    
    for person_folder in tqdm(person_folders, desc="ë°ì´í„°ì…‹ ë¡œë”© ì¤‘"):
        wav_path = os.path.join(data_dir, person_folder, "wav_48000")
        if not os.path.exists(wav_path):
            continue
        
        for audio_file in os.listdir(wav_path):
            if not audio_file.lower().endswith('.wav'):
                continue
            
            emotion_label = get_emotion_from_filename(audio_file)
            if emotion_label and emotion_counts[emotion_label] < max_per_class:
                audio_paths.append(os.path.join(wav_path, audio_file))
                labels.append(emotion_label)
                emotion_counts[emotion_label] += 1
        
        if all(count >= max_per_class for count in emotion_counts.values()):
            break

    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¶„í¬ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©):")
    for emotion, count in emotion_counts.items():
        percentage = (count / len(audio_paths) * 100) if len(audio_paths) > 0 else 0
        print(f"  {emotion}: {count}ê°œ ({percentage:.1f}%)")
            
    return audio_paths, labels

def preprocess_audio(file_path: str, processor: Wav2Vec2Processor, is_training: bool = False) -> Optional[torch.Tensor]:
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        if is_training:
            audio = AUGMENTATION(samples=audio, sample_rate=sr)
       
        # ê¸¸ì´ ì¡°ì •
        target_length = int(SAMPLE_RATE * MAX_DURATION)
        if len(audio) > target_length:
            start_idx = np.random.randint(0, len(audio) - target_length + 1)
            audio = audio[start_idx:start_idx + target_length]
        elif len(audio) < target_length:
            pad_length = target_length - len(audio)
            audio = np.pad(audio, (0, pad_length), mode='constant')
        
        inputs = processor(audio, sampling_rate=SAMPLE_RATE, return_tensors="pt", padding=True)
        return inputs.input_values.squeeze(0)
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {file_path}, {e}")
        return None

# (í•„ìˆ˜) í™”ì ID ì¶”ì¶œ: data_dir ë°”ë¡œ ì•„ë˜ 1ë‹¨ê³„ í´ë”ëª…ì´ í™”ì
def extract_speaker_id(audio_path: str, data_dir: str) -> str:
    rel = os.path.relpath(audio_path, data_dir)
    spk = rel.split(os.sep)[0]
    return spk

def build_speaker_mapping(train_paths, data_dir):
    train_speakers = sorted({extract_speaker_id(p, data_dir) for p in train_paths})
    spk2id = {spk: i for i, spk in enumerate(train_speakers)}
    return spk2id

# (ì„ íƒ) ê²½ë¡œì—ì„œ ê°ì • ë¼ë²¨ ì¶”ë¡  (í´ë”ëª…ì— Anxious/Kind/Dryê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©)
def infer_emotion_from_path(audio_path: str) -> Optional[str]:
    parts = os.path.normpath(audio_path).split(os.sep)
    for p in reversed(parts):
        if p in EMOTION_LABELS:
            return p
    # í´ë”ëª…ì— ì—†ìœ¼ë©´ íŒŒì¼ëª… ê·œì¹™ìœ¼ë¡œ ì¶”ë¡  (ê¸°ì¡´ í•¨ìˆ˜)
    return get_emotion_from_filename(os.path.basename(audio_path))

# ë°ì´í„° ì „ì²´ë¥¼ ìŠ¤ìº”í•´ì„œ (ê²½ë¡œ, ê°ì •, í™”ì, ìŠ¤í¬ë¦½íŠ¸ID) ì¸ë±ìŠ¤ ìƒì„±
def build_corpus_index(data_dir: str,
                       accept_exts={'.wav', '.flac'},
                       require_emotion=True) -> List[Dict[str, Any]]:
    """
    return: [{"path": p, "emotion": e, "speaker": s, "content_id": c}, ...]
    """
    index = []
    speakers = sorted([d for d in os.listdir(data_dir)
                       if os.path.isdir(os.path.join(data_dir, d))])
    print(f"ğŸ“ í™”ì í´ë” ìˆ˜: {len(speakers)}")

    for spk in tqdm(speakers, desc="ì¸ë±ìŠ¤ êµ¬ì¶•"):
        spk_dir = os.path.join(data_dir, spk)
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰ (ê°ì •ë³„ í´ë”/ë‹¨ì¼ í´ë” ë‘˜ ë‹¤ ëŒ€ì‘)
        for root, _, files in os.walk(spk_dir):
            for fn in files:
                ext = os.path.splitext(fn)[1].lower()
                if ext not in accept_exts:
                    continue
                path = os.path.join(root, fn)

                # ê°ì • ë¼ë²¨
                emo = infer_emotion_from_path(path)
                if require_emotion and emo not in EMOTION_LABELS:
                    # ê°ì • ë¯¸ë§¤ì¹­ ìƒ˜í”Œì€ ì œì™¸
                    continue

                # ìŠ¤í¬ë¦½íŠ¸(ëŒ€í™”) ID: íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ (ê¸°ì¡´ ê·œì¹™ ê·¸ëŒ€ë¡œ)
                cid = extract_number_from_filename(fn, type="content")
                if cid is None:
                    # ìŠ¤í¬ë¦½íŠ¸ ID ì—†ìœ¼ë©´ ì œì™¸(ë¶ˆêµì°¨ ì¡°ê±´ì„ ë³´ì¥í•˜ê¸° ìœ„í•´)
                    continue

                index.append({
                    "path": path,
                    "emotion": emo,
                    "speaker": spk,
                    "content_id": cid
                })
    print(f"âœ… ì¸ë±ìŠ¤ ìƒ˜í”Œ ìˆ˜: {len(index)}")
    return index

def split_speaker_and_content(
    index: List[Dict[str, Any]],
    val_content_ratio: float = 0.2,
    test_content_ratio: float = 0.2,
    val_speaker_ratio: float = 0.2,
    test_speaker_ratio: float = 0.2,
    seed: int = 42,
    fixed_val_content_ids: Optional[List[int]] = None,
    fixed_test_content_ids: Optional[List[int]] = None,
) -> Tuple[Tuple[List[str], List[str]],
           Tuple[List[str], List[str]],
           Tuple[List[str], List[str]]]:
    """
    index: build_corpus_index() ë°˜í™˜ ë¦¬ìŠ¤íŠ¸
    ë°˜í™˜: ((train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels))
    """
    rng = random.Random(seed)

    # ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ ID, í™”ì ëª©ë¡
    all_contents = sorted(set([it["content_id"] for it in index]))
    all_speakers = sorted(set([it["speaker"] for it in index]))

    # --- 2-1) ìŠ¤í¬ë¦½íŠ¸(ëŒ€í™”) ë¶ˆêµì°¨ ì„¸íŠ¸ ë§Œë“¤ê¸°
    if fixed_val_content_ids is not None and fixed_test_content_ids is not None:
        val_contents = set(fixed_val_content_ids)
        test_contents = set(fixed_test_content_ids)
        train_contents = set(all_contents) - val_contents - test_contents
    else:
        contents = all_contents[:]
        rng.shuffle(contents)
        n_val = max(1, int(len(contents) * val_content_ratio))
        n_test = max(1, int(len(contents) * test_content_ratio))
        val_contents = set(contents[:n_val])
        test_contents = set(contents[n_val:n_val+n_test])
        train_contents = set(contents[n_val+n_test:])

    # --- 2-2) í™”ì ë¶ˆêµì°¨ ì„¸íŠ¸ ë§Œë“¤ê¸°
    speakers = all_speakers[:]
    rng.shuffle(speakers)
    n_val_spk = max(1, int(len(speakers) * val_speaker_ratio))
    n_test_spk = max(1, int(len(speakers) * test_speaker_ratio))
    val_speakers = set(speakers[:n_val_spk])
    test_speakers = set(speakers[n_val_spk:n_val_spk+n_test_spk])
    train_speakers = set(speakers[n_val_spk+n_test_spk:])

    # --- 2-3) êµì§‘í•© ì œê±°: ë‘ ì¡°ê±´(í™”ì ì„¸íŠ¸, ìŠ¤í¬ë¦½íŠ¸ ì„¸íŠ¸)ì„ ë™ì‹œì— ë§Œì¡±í•˜ëŠ” ìƒ˜í”Œë§Œ ì±„íƒ
    train_items = [it for it in index
                   if it["speaker"] in train_speakers and it["content_id"] in train_contents]
    val_items   = [it for it in index
                   if it["speaker"] in val_speakers and it["content_id"] in val_contents]
    test_items  = [it for it in index
                   if it["speaker"] in test_speakers and it["content_id"] in test_contents]

    # --- 2-4) ì ê²€ ì¶œë ¥
    def summarize(name, items):
        spks = sorted(set([it["speaker"] for it in items]))
        cids = sorted(set([it["content_id"] for it in items]))
        emo_cnt = Counter([it["emotion"] for it in items])
        print(f"\n[{name}] ìƒ˜í”Œ: {len(items)}, í™”ì: {len(spks)}, ìŠ¤í¬ë¦½íŠ¸ID: {len(cids)}")
        print(f"  ê°ì •ë¶„í¬: {dict(emo_cnt)}")
        print(f"  ì˜ˆì‹œ í™”ì(ìµœëŒ€ 10): {spks[:10]}")
        print(f"  ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ID(ìµœëŒ€ 20): {cids[:20]}")

    summarize("TRAIN", train_items)
    summarize("VAL",   val_items)
    summarize("TEST",  test_items)

    # --- 2-5) êµì°¨ ê²€ì¦: í™”ì/ìŠ¤í¬ë¦½íŠ¸ ë¶ˆêµì°¨ ì—¬ë¶€ í™•ì¸
    assert set([it["speaker"] for it in train_items]).isdisjoint(set([it["speaker"] for it in val_items + test_items])), \
        "Train í™”ìê°€ Val/Testì™€ ê²¹ì¹©ë‹ˆë‹¤."
    assert set([it["speaker"] for it in val_items]).isdisjoint(set([it["speaker"] for it in test_items])), \
        "Val í™”ìê°€ Testì™€ ê²¹ì¹©ë‹ˆë‹¤."
    assert set([it["content_id"] for it in train_items]).isdisjoint(set([it["content_id"] for it in val_items + test_items])), \
        "Train ìŠ¤í¬ë¦½íŠ¸IDê°€ Val/Testì™€ ê²¹ì¹©ë‹ˆë‹¤."
    assert set([it["content_id"] for it in val_items]).isdisjoint(set([it["content_id"] for it in test_items])), \
        "Val ìŠ¤í¬ë¦½íŠ¸IDê°€ Testì™€ ê²¹ì¹©ë‹ˆë‹¤."

    # --- 2-6) ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    def to_xy(items):
        return [it["path"] for it in items], [it["emotion"] for it in items]

    return to_xy(train_items), to_xy(val_items), to_xy(test_items)


class EmotionDataset(Dataset):
    def __init__(self, audio_paths: List[str], labels: List[str], processor: Wav2Vec2Processor, is_training: bool = True):
        self.data_dir = "/data/ghdrnjs/SER/small/"
        self.audio_paths = audio_paths
        self.labels = labels
        self.processor = processor
        self.encoded_labels = [LABEL2ID[label] for label in labels]
        self.is_training = is_training

        with open("script.json", "r", encoding="utf-8") as f:
            self.text_json = json.load(f)

        self.spk2id = build_speaker_mapping(audio_paths, self.data_dir)        
    def __len__(self):
        return len(self.audio_paths)
    
    def __getitem__(self, idx):
        audio_path = self.audio_paths[idx]
        emotion_label = self.encoded_labels[idx]
        file_number = extract_number_from_filename(audio_path, type="content")
        
        content_text = ""
        if file_number is not None and str(file_number) in self.text_json:
            content_text = self.text_json[str(file_number)]
        
        input_values = preprocess_audio(audio_path, self.processor, self.is_training)
        if input_values is None:
            input_values = torch.zeros(int(SAMPLE_RATE * MAX_DURATION))
        
        spk_idx_tensor = None
        if self.spk2id is not None:
            spk_str = extract_speaker_id(audio_path, self.data_dir)
            if spk_str in self.spk2id:
                spk_idx = self.spk2id[spk_str]
                spk_idx_tensor = torch.tensor(spk_idx, dtype=torch.long)

        return {
            'input_values': input_values,
            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),
            'content_text': content_text,
            'speaker_id': spk_idx_tensor
        }

def collate_fn(batch: List[Dict[str, any]]) -> Dict[str, torch.Tensor]:
    input_values = [item['input_values'] for item in batch]
    emotion_labels = [item['emotion_labels'] for item in batch]
    content_texts = [item['content_text'] for item in batch]
    spk_list = [item.get('speaker_id', None) for item in batch]
    
    padded_input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)
    
    tokenized_contents = []
    content_lengths = []
    for text in content_texts:
        ids = [CHAR2ID.get(char, CHAR2ID['<unk>']) for char in text]
        tokenized_contents.append(torch.tensor(ids, dtype=torch.long))
        content_lengths.append(len(ids))

    padded_content_labels = torch.nn.utils.rnn.pad_sequence(
        tokenized_contents, 
        batch_first=True, 
        padding_value=CHAR2ID['<pad>']
    )

    # attention_mask for audio
    attention_mask = torch.ones_like(padded_input_values, dtype=torch.long)
    for i, seq in enumerate(input_values):
        attention_mask[i, len(seq):] = 0

    if all((s is not None) and isinstance(s, torch.Tensor) for s in spk_list):
        # ê° ìš”ì†Œê°€ 0-dim long tensorë¼ë©´ stack -> (B,)
        speaker_ids = torch.stack(spk_list)            # shape: (B,)
        speaker_ids = speaker_ids.view(-1).long()      # ë³´ì •
    else:
        speaker_ids = None

    return {
        'input_values': padded_input_values,
        'attention_mask': attention_mask,
        'labels': torch.stack(emotion_labels),
        'content_labels': padded_content_labels,
        'content_labels_lengths': torch.tensor(content_lengths, dtype=torch.long),
        'speaker_ids': speaker_ids,
    }

def create_model_and_processor(freeze_base_model: bool = True, num_speakers: int = 500):
    """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {MODEL_NAME}")
    
    # ì„¤ì • ìˆ˜ì •
    config = Wav2Vec2Config.from_pretrained(
        MODEL_NAME,
        num_labels=len(EMOTION_LABELS),
        label2id=LABEL2ID,
        id2label=ID2LABEL,
        finetuning_task="emotion_classification"
    )
    config.char_vocab_size = len(CHAR_VOCAB) # ì ëŒ€ì ëª¨ë¸ì„ ìœ„í•œ ì„¤ì •
    config.num_speakers = num_speakers
    
    model = custom_Wav2Vec2ForEmotionClassification.from_pretrained(
        MODEL_NAME,
        config=config,
        ignore_mismatched_sizes=True
    )
    
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    
    if freeze_base_model:
        for param in model.wav2vec2.parameters():
            param.requires_grad = False

    return model, processor

def enable_last_k_blocks(model, last_k: int = 4):
    # 1) ì „ì²´ freeze
    for p in model.wav2vec2.parameters():
        p.requires_grad = False
    # 2) ë§ˆì§€ë§‰ Kê°œ blockë§Œ unfreeze
    layers = model.wav2vec2.encoder.layers
    num_layers = len(layers)
    for i in range(num_layers - last_k, num_layers):
        for p in layers[i].parameters():
            p.requires_grad = True
    # 3) í—¤ë“œ/ì ëŒ€ì/í’€ëŸ¬/í”„ë¡œì í„°ëŠ” í•­ìƒ í•™ìŠµ
    for name, module in model.named_modules():
        if any(k in name for k in ["classifier", "adversary", "speaker_adversary", "pooler", "stats_projector", "projector"]):
            for p in module.parameters():
                p.requires_grad = True


def evaluate_model(model, dataloader, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="í‰ê°€ ì¤‘"):
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.0, # í‰ê°€ ì‹œì—ëŠ” ì ëŒ€ì  ì†ì‹¤ ë°˜ì˜ ì•ˆí•¨,
                speaker_ids = None,
            )
            
            loss = outputs['loss']
            
            # í›ˆë ¨ ì¤‘ì´ ì•„ë‹ ë•Œë„ lossê°€ ê³„ì‚°ë˜ë„ë¡ adv_lambda=0.0ìœ¼ë¡œ í˜¸ì¶œ
            # ë§Œì•½ lossê°€ Noneì´ë©´ (test set ë“±ì—ì„œ content_labelì´ ì—†ì„ ê²½ìš°), ê°ì • ì†ì‹¤ë§Œ ê³„ì‚°
            if loss is None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(outputs['emotion_logits'].view(-1, model.config.num_labels), batch['labels'].to(device).view(-1))

            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(batch['labels'].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'f1': f1,
        'predictions': predictions,
        'true_labels': true_labels
    }

def train_model(model, train_loader, val_loader, device, num_epochs=3, learning_rate=3e-5):
    """ì§ì ‘ í›ˆë ¨ ë£¨í”„"""
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)
    print("ğŸš€ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)")
    # optimizer_grouped_parameters = [
    #     {
    #         "params": [p for n, p in model.named_parameters() if "wav2vec2" in n],
    #         "lr": 1e-5,  # ì‚¬ì „ í›ˆë ¨ëœ Backboneì€ ë‚®ì€ í•™ìŠµë¥ 
    #     },
    #     {
    #         "params": [p for n, p in model.named_parameters() if "wav2vec2" not in n],
    #         "lr": 1e-4,  # ìƒˆë¡œ ì¶”ê°€ëœ Classifierì™€ AdversaryëŠ” ë†’ì€ í•™ìŠµë¥ 
    #     },
    # ]
    # optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01)

    backbone_params = []
    head_params = []

    for n, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if n.startswith("wav2vec2."):
            backbone_params.append(p)
        else:
            # pooler, stats_projector, projector(ë¶€ëª¨), classifier, adversaries ë“±
            head_params.append(p)

    optimizer = optim.AdamW(
        [
            {"params": backbone_params, "lr": 5e-6, "weight_decay": 0.01},
            {"params": head_params,     "lr": 1e-4, "weight_decay": 0.01},
        ]
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * num_epochs
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)
    
    best_f1 = 0
    best_model_state = None
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
    print(f"   ì´ ì—í¬í¬: {num_epochs}")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    max_adv = 0.1
    warmup_epochs = 1.0


    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
        
        # í›ˆë ¨
        model.train()
        train_loss = 0
        for step, batch in enumerate(train_loader):
            progress = min(1.0, (epoch + step/len(train_loader)) / warmup_epochs)
            adv_lambda = max_adv * progress
        train_predictions = []
        train_true_labels = []

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training")
        for batch in tqdm(progress_bar):
            optimizer.zero_grad()
            
            
            # speaker_ids ë„˜ê²¨ì¤˜ì•¼ í•¨
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.1,
                speaker_ids=batch['speaker_ids'].to(device)
            )
            
            loss = outputs['loss']
            if loss is None or torch.isnan(loss) or torch.isinf(loss):
                continue

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.6f}'
            })
        
        # í›ˆë ¨ ê²°ê³¼
        train_loss /= len(train_loader)
        # train_acc = accuracy_score(train_true_labels, train_predictions)
        # train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')
        
        # print(f"ğŸ¯ í›ˆë ¨ ê²°ê³¼ - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        
        # ê²€ì¦
        print("ğŸ“Š ê²€ì¦ ì¤‘...")
        val_results = evaluate_model(model, val_loader, device)
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, F1: {val_results['f1']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_results['f1'] > best_f1:
            best_f1 = val_results['f1']
            best_model_state = model.state_dict().copy()
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’« ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1:.4f})")
    
    return model

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ§ª wav2vec2-large-xlsr-korean ì§ì ‘ í›ˆë ¨ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    print("="*70)
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    
    data_dir = "/data/ghdrnjs/SER/small/"
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {data_dir}")
    index = build_corpus_index(data_dir, require_emotion=True)
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
        split_speaker_and_content(
            index,
            val_content_ratio=0.2,
            test_content_ratio=0.2,
            val_speaker_ratio=0.2,
            test_speaker_ratio=0.2,
            seed=42,
            # ì˜ˆì‹œ) íŠ¹ì • ëŒ€í™” IDë¥¼ ê³ ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì£¼ì„ í•´ì œ
            # fixed_val_content_ids=[22, 35],
            # fixed_test_content_ids=[27, 41],
        )
    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ")
    print(f"  Test: {len(test_paths)}ê°œ")

    train_speakers = sorted({extract_speaker_id(p, data_dir) for p in train_paths})
    num_speakers = len(train_speakers)
    print(f"ğŸ” í™”ì ìˆ˜: {num_speakers}")

    # audio_paths, labels = load_dataset_subset(data_dir, max_per_class=3000)
    # print(f"âœ… ì´ {len(audio_paths)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    
    # ë°ì´í„° ë¶„í•  (íŒŒì¼ëª… ë§ˆì§€ë§‰ ìˆ«ì ê¸°ì¤€)
    
    # (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = split_data_by_last_digit(audio_paths, labels)
    
    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ") 
    print(f"  Test: {len(test_paths)}ê°œ")
    
    # ê° ì„¸íŠ¸ì˜ ê°ì •ë³„ ë¶„í¬ í™•ì¸
    from collections import Counter
    train_emotion_dist = Counter(train_labels)
    val_emotion_dist = Counter(val_labels)
    test_emotion_dist = Counter(test_labels)
    
    print(f"\nğŸ“ˆ ê°ì •ë³„ ë¶„í¬:")
    print(f"  Train: {dict(train_emotion_dist)}")
    print(f"  Validation: {dict(val_emotion_dist)}")
    print(f"  Test: {dict(test_emotion_dist)}")
    
    # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±
    model, processor = create_model_and_processor(num_speakers=num_speakers)
    enable_last_k_blocks(model, last_k=4)
    model.to(device)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ”„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    train_dataset = EmotionDataset(train_paths, train_labels, processor)
    val_dataset = EmotionDataset(val_paths, val_labels, processor, is_training=False)
    test_dataset = EmotionDataset(test_paths, test_labels, processor, is_training=False)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    batch_size = 2
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    try:
        # í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1ì—í¬í¬)
        model = train_model(model, train_loader, val_loader, device, num_epochs=5, learning_rate=3e-5)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€...")
        test_results = evaluate_model(model, test_loader, device)
        
        print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"   - F1 ìŠ¤ì½”ì–´: {test_results['f1']:.4f}")
        print(f"   - ì†ì‹¤: {test_results['loss']:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report = classification_report(
            test_results['true_labels'], 
            test_results['predictions'],
            target_names=EMOTION_LABELS,
            digits=4
        )
        print(report)
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        output_dir = "./results_quick_test"
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥ (transformers ë°©ì‹)
        model_save_path = os.path.join(output_dir, "adversary_content_speaker_model_augment_v2_epoch_5_last_k_4")
        os.makedirs(model_save_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(model_save_path)
        processor.save_pretrained(model_save_path)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"\nğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print(f"   ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸: python test_my_voice.py your_audio.wav --model_path {model_save_path}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
