#!/usr/bin/env python3
"""
ì§ì ‘ í›ˆë ¨ ë£¨í”„ êµ¬í˜„ ë²„ì „ (Trainer ì‚¬ìš© ì•ˆí•¨)
+ ì ëŒ€ì  í•™ìŠµ ì¶”ê°€
"""

import os
import sys
import json
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import librosa
from audiomentations import Compose, PitchShift, TimeStretch, AddGaussianNoise, Shift
from typing import List, Tuple, Optional, Dict, Any, Literal
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

from tqdm import tqdm
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, f1_score, classification_report

from transformers import (
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2Processor,
    Wav2Vec2Config
)
# ì ëŒ€ì  í•™ìŠµì„ ìœ„í•œ custom model ì„í¬íŠ¸
from Wav2Vec2_seq_clf import custom_Wav2Vec2ForEmotionClassification

# ê°ì • ë¼ë²¨ ì •ì˜
"""
"Anxious" : 0
"Kind" : 1
"Dry" : 2
"""
EMOTION_LABELS = ["Anxious", "Dry", "Kind"]
LABEL2ID = {label: i for i, label in enumerate(EMOTION_LABELS)}
ID2LABEL = {i: label for i, label in enumerate(EMOTION_LABELS)}

MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
SAMPLE_RATE = 16000
MAX_DURATION = 10.0

AUGMENTATION = Compose([
    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),
    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
])

# Character Vocabulary ë¡œë“œ
try:
    with open('char_to_id.json', 'r', encoding='utf-8') as f:
        CHAR2ID = json.load(f)
    CHAR_VOCAB = list(CHAR2ID.keys())
    ID2CHAR = {i: char for char, i in CHAR2ID.items()}
    print(f"âœ… Character Vocabulary ë¡œë“œ ì™„ë£Œ ({len(CHAR_VOCAB)}ê°œ)")
except FileNotFoundError:
    print("âŒ 'char_to_id.json'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vocab.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def extract_number_from_filename(filename: str, type: Literal['content', 'emotion'] = 'emotion') -> Optional[int]:
    try:
        if type == "content":
            # íŒŒì¼ëª…ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì ê·¸ë£¹ ì „ì²´ë¥¼ ì¶”ì¶œ (ì˜ˆ: F2001_000123.wav -> 123)
            match = re.search(r'_(\d+)\.wav$', os.path.basename(filename))
            if match:
                return int(match.group(1))
            return None
        else:
            # F..._...xxxD.wav ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì Dë¥¼ ì¶”ì¶œ
            match = re.search(r'_(\d+)\.wav$', os.path.basename(filename))
            if match:
                # ìˆ«ìê°€ ì—¬ëŸ¬ ìë¦¬ì¼ ê²½ìš° ë§ˆì§€ë§‰ í•œ ìë¦¬ë§Œ ì‚¬ìš©
                return int(match.group(1)) % 10
            return None
    except (ValueError, AttributeError):
        return None

def split_data_by_last_digit(audio_paths: List[str], labels: List[str]) -> Tuple[
    Tuple[List[str], List[str]], 
    Tuple[List[str], List[str]], 
    Tuple[List[str], List[str]]
]:
    """íŒŒì¼ëª…ì˜ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ train/val/test ë¶„í• 
    
    Args:
        audio_paths: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        labels: í•´ë‹¹í•˜ëŠ” ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ((train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels))
        - Train: ë§ˆì§€ë§‰ ìˆ«ìê°€ 1,2,3,4,5,6
        - Validation: ë§ˆì§€ë§‰ ìˆ«ìê°€ 7,8
        - Test: ë§ˆì§€ë§‰ ìˆ«ìê°€ 9,0
    """
    train_paths, train_labels = [], []
    val_paths, val_labels = [], []
    test_paths, test_labels = [], []
    
    for path, label in zip(audio_paths, labels):
        last_digit = extract_number_from_filename(path, type="emotion")
        
        if last_digit is None:
            print(f"âš ï¸ íŒŒì¼ëª…ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            continue
            
        if last_digit in [1, 2, 3, 4, 5, 6]:
            train_paths.append(path)
            train_labels.append(label)
        elif last_digit in [7, 8]:
            val_paths.append(path)
            val_labels.append(label)
        elif last_digit in [9, 0]:
            test_paths.append(path)
            test_labels.append(label)
    
    return (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels)

def get_emotion_from_filename(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°ì • ë¼ë²¨ ë°˜í™˜"""
    file_num = extract_number_from_filename(filename, type="content")
    if file_num is None:
        return None
        
    if 21 <= file_num <= 30:
        return "Anxious"
    elif 31 <= file_num <= 40:
        return "Kind"
    elif 141 <= file_num <= 150:
        return "Dry"
    else:
        return None

def load_dataset_subset(data_dir: str, max_per_class: int) -> Tuple[List[str], List[str]]:
    audio_paths = []
    labels = []
    emotion_counts = {label: 0 for label in EMOTION_LABELS}
    
    person_folders = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])
    print(f"ğŸ“ ë°œê²¬ëœ person í´ë”: {len(person_folders)}ê°œ")
    
    for person_folder in tqdm(person_folders, desc="ë°ì´í„°ì…‹ ë¡œë”© ì¤‘"):
        wav_path = os.path.join(data_dir, person_folder, "wav_48000")
        if not os.path.exists(wav_path):
            continue
        
        for audio_file in os.listdir(wav_path):
            if not audio_file.lower().endswith('.wav'):
                continue
            
            emotion_label = get_emotion_from_filename(audio_file)
            if emotion_label and emotion_counts[emotion_label] < max_per_class:
                audio_paths.append(os.path.join(wav_path, audio_file))
                labels.append(emotion_label)
                emotion_counts[emotion_label] += 1
        
        if all(count >= max_per_class for count in emotion_counts.values()):
            break

    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¶„í¬ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©):")
    for emotion, count in emotion_counts.items():
        percentage = (count / len(audio_paths) * 100) if len(audio_paths) > 0 else 0
        print(f"  {emotion}: {count}ê°œ ({percentage:.1f}%)")
            
    return audio_paths, labels

def preprocess_audio(file_path: str, processor: Wav2Vec2Processor, is_training: bool = False) -> Optional[torch.Tensor]:
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        if is_training:
            audio = AUGMENTATION(samples=audio, sample_rate=sr)
       
        # ê¸¸ì´ ì¡°ì •
        target_length = int(SAMPLE_RATE * MAX_DURATION)
        if len(audio) > target_length:
            start_idx = np.random.randint(0, len(audio) - target_length + 1)
            audio = audio[start_idx:start_idx + target_length]
        elif len(audio) < target_length:
            pad_length = target_length - len(audio)
            audio = np.pad(audio, (0, pad_length), mode='constant')
        
        inputs = processor(audio, sampling_rate=SAMPLE_RATE, return_tensors="pt", padding=True)
        return inputs.input_values.squeeze(0)
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {file_path}, {e}")
        return None

class EmotionDataset(Dataset):
    def __init__(self, audio_paths: List[str], labels: List[str], processor: Wav2Vec2Processor, is_training: bool = True):
        self.audio_paths = audio_paths
        self.labels = labels
        self.processor = processor
        self.encoded_labels = [LABEL2ID[label] for label in labels]
        self.is_training = is_training
        
        with open("script.json", "r", encoding="utf-8") as f:
            self.text_json = json.load(f)

    def __len__(self):
        return len(self.audio_paths)
    
    def __getitem__(self, idx):
        audio_path = self.audio_paths[idx]
        emotion_label = self.encoded_labels[idx]
        file_number = extract_number_from_filename(audio_path, type="content")
        
        content_text = ""
        if file_number is not None and str(file_number) in self.text_json:
             content_text = self.text_json[str(file_number)]
        
        input_values = preprocess_audio(audio_path, self.processor)
        if input_values is None:
            input_values = torch.zeros(int(SAMPLE_RATE * MAX_DURATION))
        
        return {
            'input_values': input_values,
            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),
            'content_text': content_text
        }

def collate_fn(batch: List[Dict[str, any]]) -> Dict[str, torch.Tensor]:
    input_values = [item['input_values'] for item in batch]
    emotion_labels = [item['emotion_labels'] for item in batch]
    content_texts = [item['content_text'] for item in batch]
    
    padded_input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)
    
    tokenized_contents = []
    content_lengths = []
    for text in content_texts:
        ids = [CHAR2ID.get(char, CHAR2ID['<unk>']) for char in text]
        tokenized_contents.append(torch.tensor(ids, dtype=torch.long))
        content_lengths.append(len(ids))

    padded_content_labels = torch.nn.utils.rnn.pad_sequence(
        tokenized_contents, 
        batch_first=True, 
        padding_value=CHAR2ID['<pad>']
    )

    # attention_mask for audio
    attention_mask = torch.ones_like(padded_input_values)
    for i, seq in enumerate(input_values):
        attention_mask[i, len(seq):] = 0


    return {
        'input_values': padded_input_values,
        'attention_mask': attention_mask,
        'labels': torch.stack(emotion_labels),
        'content_labels': padded_content_labels,
        'content_labels_lengths': torch.tensor(content_lengths, dtype=torch.long)
    }

def create_model_and_processor(freeze_base_model: bool = True):
    """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {MODEL_NAME}")
    
    # ì„¤ì • ìˆ˜ì •
    config = Wav2Vec2Config.from_pretrained(
        MODEL_NAME,
        num_labels=len(EMOTION_LABELS),
        label2id=LABEL2ID,
        id2label=ID2LABEL,
        finetuning_task="emotion_classification"
    )
    config.char_vocab_size = len(CHAR_VOCAB) # ì ëŒ€ì ëª¨ë¸ì„ ìœ„í•œ ì„¤ì •
    
    model = custom_Wav2Vec2ForEmotionClassification.from_pretrained(
        MODEL_NAME,
        config=config,
        ignore_mismatched_sizes=True
    )
    
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    
    if freeze_base_model:
        for param in model.wav2vec2.parameters():
            param.requires_grad = False

    return model, processor

def evaluate_model(model, dataloader, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="í‰ê°€ ì¤‘"):
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.0  # í‰ê°€ ì‹œì—ëŠ” ì ëŒ€ì  ì†ì‹¤ ë°˜ì˜ ì•ˆí•¨
            )
            
            loss = outputs['loss']
            
            # í›ˆë ¨ ì¤‘ì´ ì•„ë‹ ë•Œë„ lossê°€ ê³„ì‚°ë˜ë„ë¡ adv_lambda=0.0ìœ¼ë¡œ í˜¸ì¶œ
            # ë§Œì•½ lossê°€ Noneì´ë©´ (test set ë“±ì—ì„œ content_labelì´ ì—†ì„ ê²½ìš°), ê°ì • ì†ì‹¤ë§Œ ê³„ì‚°
            if loss is None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(outputs['emotion_logits'].view(-1, model.config.num_labels), batch['labels'].to(device).view(-1))

            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(batch['labels'].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'f1': f1,
        'predictions': predictions,
        'true_labels': true_labels
    }

def train_model(model, train_loader, val_loader, device, num_epochs=3, learning_rate=3e-5):
    """ì§ì ‘ í›ˆë ¨ ë£¨í”„"""
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * num_epochs
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)
    
    best_f1 = 0
    best_model_state = None
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
    print(f"   ì´ ì—í¬í¬: {num_epochs}")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
        
        # í›ˆë ¨
        model.train()
        train_loss = 0
        train_predictions = []
        train_true_labels = []

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training")
        for batch in tqdm(progress_bar):
            optimizer.zero_grad()
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.1
            )
            
            loss = outputs['loss']
            if loss is None or torch.isnan(loss) or torch.isinf(loss):
                continue

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.6f}'
            })
        
        # í›ˆë ¨ ê²°ê³¼
        train_loss /= len(train_loader)
        # train_acc = accuracy_score(train_true_labels, train_predictions)
        # train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')
        
        # print(f"ğŸ¯ í›ˆë ¨ ê²°ê³¼ - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        
        # ê²€ì¦
        print("ğŸ“Š ê²€ì¦ ì¤‘...")
        val_results = evaluate_model(model, val_loader, device)
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, F1: {val_results['f1']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_results['f1'] > best_f1:
            best_f1 = val_results['f1']
            best_model_state = model.state_dict().copy()
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’« ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1:.4f})")
    
    return model

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ§ª wav2vec2-large-xlsr-korean ì§ì ‘ í›ˆë ¨ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    print("="*70)
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    
    data_dir = "/data/ghdrnjs/SER/small/"
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {data_dir}")
    audio_paths, labels = load_dataset_subset(data_dir, max_per_class=3000)
    print(f"âœ… ì´ {len(audio_paths)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    
    # ë°ì´í„° ë¶„í•  (íŒŒì¼ëª… ë§ˆì§€ë§‰ ìˆ«ì ê¸°ì¤€)
    print(f"\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘ (íŒŒì¼ëª… ë§ˆì§€ë§‰ ìˆ«ì ê¸°ì¤€)...")
    print(f"  - Train: ë§ˆì§€ë§‰ ìˆ«ì 1,2,3,4,5,6")
    print(f"  - Validation: ë§ˆì§€ë§‰ ìˆ«ì 7,8")
    print(f"  - Test: ë§ˆì§€ë§‰ ìˆ«ì 9,0")
    
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = split_data_by_last_digit(audio_paths, labels)
    
    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ") 
    print(f"  Test: {len(test_paths)}ê°œ")
    
    # ê° ì„¸íŠ¸ì˜ ê°ì •ë³„ ë¶„í¬ í™•ì¸
    from collections import Counter
    train_emotion_dist = Counter(train_labels)
    val_emotion_dist = Counter(val_labels)
    test_emotion_dist = Counter(test_labels)
    
    print(f"\nğŸ“ˆ ê°ì •ë³„ ë¶„í¬:")
    print(f"  Train: {dict(train_emotion_dist)}")
    print(f"  Validation: {dict(val_emotion_dist)}")
    print(f"  Test: {dict(test_emotion_dist)}")
    
    # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±
    model, processor = create_model_and_processor()
    model.to(device)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ”„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    train_dataset = EmotionDataset(train_paths, train_labels, processor)
    val_dataset = EmotionDataset(val_paths, val_labels, processor)
    test_dataset = EmotionDataset(test_paths, test_labels, processor)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    batch_size = 2
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    try:
        # í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1ì—í¬í¬)
        model = train_model(model, train_loader, val_loader, device, num_epochs=5, learning_rate=3e-5)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€...")
        test_results = evaluate_model(model, test_loader, device)
        
        print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"   - F1 ìŠ¤ì½”ì–´: {test_results['f1']:.4f}")
        print(f"   - ì†ì‹¤: {test_results['loss']:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report = classification_report(
            test_results['true_labels'], 
            test_results['predictions'],
            target_names=EMOTION_LABELS,
            digits=4
        )
        print(report)
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        output_dir = "./results_quick_test"
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥ (transformers ë°©ì‹)
        model_save_path = os.path.join(output_dir, "adversary_model_augment_v1_epoch_5")
        os.makedirs(model_save_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(model_save_path)
        processor.save_pretrained(model_save_path)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"\nğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print(f"   ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸: python test_my_voice.py your_audio.wav --model_path {model_save_path}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
