#!/usr/bin/env python3
"""
ì§ì ‘ í›ˆë ¨ ë£¨í”„ êµ¬í˜„ ë²„ì „ (Trainer ì‚¬ìš© ì•ˆí•¨)
ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ ë¬¸ì œ í•´ê²°
"""

import os
import sys
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import librosa
from typing import List, Tuple, Optional, Dict, Any
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ì§„í–‰ ìƒí™© í‘œì‹œ
from tqdm import tqdm

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Transformers (Trainer ì œì™¸)
from transformers import (
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2Processor,
    Wav2Vec2Config
)

# ê°ì • ë¼ë²¨ ì •ì˜
"""
"Anxious" : 0
"Kind" : 1
"Dry" : 2
"""
EMOTION_LABELS = ["Anxious", "Dry", "Kind"]
LABEL2ID = {label: i for i, label in enumerate(EMOTION_LABELS)}
ID2LABEL = {i: label for i, label in enumerate(EMOTION_LABELS)}

# ëª¨ë¸ ë° í›ˆë ¨ ì„¤ì •
MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
SAMPLE_RATE = 16000
MAX_DURATION = 10.0

def get_emotion_from_filename(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°ì • ë¼ë²¨ ë°˜í™˜"""
    try:
        match = re.search(r'_(\d{6})\.', filename)
        if not match:
            return None
        
        file_num = int(match.group(1))
        
        if 21 <= file_num <= 30:
            return "Anxious"
        elif 31 <= file_num <= 40:
            return "Kind"
        elif 91 <= file_num <= 100:
            return "Dry"
        else:
            return None
    except (ValueError, AttributeError):
        return None

def load_dataset_subset(data_dir: str, max_per_class: int = 30) -> Tuple[List[str], List[str]]:
    """ë°ì´í„°ì…‹ì˜ ì‘ì€ ì„œë¸Œì…‹ ë¡œë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)"""
    audio_paths = []
    labels = []
    
    if not os.path.exists(data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return audio_paths, labels
    
    person_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]
    print(f"ğŸ“ ë°œê²¬ëœ person í´ë”: {len(person_folders)}ê°œ")
    
    emotion_counts = {"Anxious": 0, "Kind": 0, "Dry": 0}
    
    # ì²˜ìŒ ëª‡ ê°œ í´ë”ë§Œ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    # selected_folders = sorted(person_folders)[:5]  # ì²˜ìŒ 5ê°œ í´ë”ë§Œ
    selected_folders = sorted(person_folders)
    
    for person_folder in tqdm(selected_folders, desc="í´ë” ì²˜ë¦¬ ì¤‘"):
        person_path = os.path.join(data_dir, person_folder)
        wav_path = os.path.join(person_path, "wav_48000")
        
        if not os.path.exists(wav_path):
            continue
        
        for audio_file in os.listdir(wav_path):
            if not audio_file.lower().endswith(('.wav', '.mp3', '.m4a', '.flac')):
                continue
            
            emotion_label = get_emotion_from_filename(audio_file)
            
            if emotion_label is not None and emotion_counts[emotion_label] < max_per_class:
                audio_path = os.path.join(wav_path, audio_file)
                audio_paths.append(audio_path)
                labels.append(emotion_label)
                emotion_counts[emotion_label] += 1
                
                # ëª¨ë“  í´ë˜ìŠ¤ê°€ ì¶©ë¶„íˆ ëª¨ì´ë©´ ì¤‘ë‹¨
                if all(count >= max_per_class for count in emotion_counts.values()):
                    break
        
        if all(count >= max_per_class for count in emotion_counts.values()):
            break
    
    print(f"\nğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¶„í¬ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©):")
    for emotion, count in emotion_counts.items():
        percentage = (count / len(audio_paths) * 100) if len(audio_paths) > 0 else 0
        print(f"  {emotion}: {count}ê°œ ({percentage:.1f}%)")
    
    return audio_paths, labels

def preprocess_audio(file_path: str, processor: Wav2Vec2Processor) -> Optional[torch.Tensor]:
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)
        
        # ê¸¸ì´ ì¡°ì •
        target_length = int(SAMPLE_RATE * MAX_DURATION)
        if len(audio) > target_length:
            start_idx = np.random.randint(0, len(audio) - target_length + 1)
            audio = audio[start_idx:start_idx + target_length]
        elif len(audio) < target_length:
            pad_length = target_length - len(audio)
            audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)
        
        # ì •ê·œí™”
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.8
        
        # Wav2Vec2 processorë¡œ ë³€í™˜
        inputs = processor(
            audio,
            sampling_rate=SAMPLE_RATE,
            return_tensors="pt",
            padding=True
        )
        
        return inputs.input_values.squeeze(0)
        
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {file_path}, {e}")
        return None

class EmotionDataset(Dataset):
    """ìŒì„± ê°ì • ë¶„ì„ ë°ì´í„°ì…‹"""
    
    def __init__(self, audio_paths: List[str], labels: List[str], processor: Wav2Vec2Processor):
        self.audio_paths = audio_paths
        self.labels = labels
        self.processor = processor
        self.encoded_labels = [LABEL2ID[label] for label in labels]
    
    def __len__(self):
        return len(self.audio_paths)
    
    def __getitem__(self, idx):
        audio_path = self.audio_paths[idx]
        label = self.encoded_labels[idx]
        
        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        input_values = preprocess_audio(audio_path, self.processor)
        
        if input_values is None:
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë°ì´í„°
            input_values = torch.zeros(int(SAMPLE_RATE * MAX_DURATION))
        
        return {
            'input_values': input_values,
            'labels': torch.tensor(label, dtype=torch.long)
        }

def collate_fn(batch):
    """ë°°ì¹˜ ë°ì´í„° ì •ë¦¬"""
    input_values = [item['input_values'] for item in batch]
    labels = [item['labels'] for item in batch]
    
    # íŒ¨ë”© (ê°€ì¥ ê¸´ ê²ƒì— ë§ì¶¤)
    max_len = max(x.size(0) for x in input_values)
    padded_inputs = []
    
    for x in input_values:
        if x.size(0) < max_len:
            pad_len = max_len - x.size(0)
            x = torch.cat([x, torch.zeros(pad_len)], dim=0)
        padded_inputs.append(x)
    
    input_values = torch.stack(padded_inputs)
    labels = torch.stack(labels)
    
    return {
        'input_values': input_values,
        'labels': labels
    }

def create_model_and_processor():
    """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {MODEL_NAME}")
    
    # ì„¤ì • ìˆ˜ì •
    config = Wav2Vec2Config.from_pretrained(
        MODEL_NAME,
        num_labels=len(EMOTION_LABELS),
        label2id=LABEL2ID,
        id2label=ID2LABEL,
        finetuning_task="emotion_classification"
    )
    
    # ëª¨ë¸ ë¡œë“œ
    model = Wav2Vec2ForSequenceClassification.from_pretrained(
        MODEL_NAME,
        config=config,
        ignore_mismatched_sizes=True
    )
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    
    # Feature extractor ë™ê²°
    model.wav2vec2.feature_extractor._freeze_parameters()
    
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return model, processor

def evaluate_model(model, dataloader, device, criterion):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="í‰ê°€ ì¤‘"):
            input_values = batch['input_values'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_values=input_values, labels=labels)
            loss = outputs.loss
            
            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs.logits, dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'f1': f1,
        'predictions': predictions,
        'true_labels': true_labels
    }

def train_model(model, train_loader, val_loader, device, num_epochs=3, learning_rate=3e-5):
    """ì§ì ‘ í›ˆë ¨ ë£¨í”„"""
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * num_epochs
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)
    
    # ì†ì‹¤ í•¨ìˆ˜
    criterion = nn.CrossEntropyLoss()
    
    best_f1 = 0
    best_model_state = None
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
    print(f"   ì´ ì—í¬í¬: {num_epochs}")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
        
        # í›ˆë ¨
        model.train()
        train_loss = 0
        train_predictions = []
        train_true_labels = []
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1} í›ˆë ¨")
        for batch_idx, batch in enumerate(progress_bar):
            input_values = batch['input_values'].to(device)
            labels = batch['labels'].to(device)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = model(input_values=input_values, labels=labels)
            loss = outputs.loss
            
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs.logits, dim=-1)
            train_predictions.extend(preds.cpu().numpy())
            train_true_labels.extend(labels.cpu().numpy())
            
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.6f}'
            })
        
        # í›ˆë ¨ ê²°ê³¼
        train_loss /= len(train_loader)
        train_acc = accuracy_score(train_true_labels, train_predictions)
        train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')
        
        print(f"ğŸ¯ í›ˆë ¨ ê²°ê³¼ - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        
        # ê²€ì¦
        print("ğŸ“Š ê²€ì¦ ì¤‘...")
        val_results = evaluate_model(model, val_loader, device, criterion)
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, F1: {val_results['f1']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_results['f1'] > best_f1:
            best_f1 = val_results['f1']
            best_model_state = model.state_dict().copy()
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’« ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1:.4f})")
    
    return model

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ§ª wav2vec2-large-xlsr-korean ì§ì ‘ í›ˆë ¨ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    print("="*70)
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    
    # ë°ì´í„° ë¡œë“œ (ì‘ì€ ì„œë¸Œì…‹)
    data_dir = "/data/ghdrnjs/SER/small/"
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {data_dir}")
    print(f"   ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê° í´ë˜ìŠ¤ë‹¹ 30ê°œì”©ë§Œ ì‚¬ìš©")
    
    audio_paths, labels = load_dataset_subset(data_dir, max_per_class=3000)
    
    if len(audio_paths) == 0:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì´ {len(audio_paths)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    
    # ë°ì´í„° ë¶„í• 
    print(f"\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘...")
    train_paths, temp_paths, train_labels, temp_labels = train_test_split(
        audio_paths, labels, test_size=0.4, random_state=42, stratify=labels
    )
    val_paths, test_paths, val_labels, test_labels = train_test_split(
        temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
    )
    
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ") 
    print(f"  Test: {len(test_paths)}ê°œ")
    
    # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±
    model, processor = create_model_and_processor()
    model.to(device)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ”„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    train_dataset = EmotionDataset(train_paths, train_labels, processor)
    val_dataset = EmotionDataset(val_paths, val_labels, processor)
    test_dataset = EmotionDataset(test_paths, test_labels, processor)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    batch_size = 2
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    try:
        # í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1ì—í¬í¬)
        model = train_model(model, train_loader, val_loader, device, num_epochs=1, learning_rate=3e-5)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€...")
        test_results = evaluate_model(model, test_loader, device, nn.CrossEntropyLoss())
        
        print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"   - F1 ìŠ¤ì½”ì–´: {test_results['f1']:.4f}")
        print(f"   - ì†ì‹¤: {test_results['loss']:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report = classification_report(
            test_results['true_labels'], 
            test_results['predictions'],
            target_names=EMOTION_LABELS,
            digits=4
        )
        print(report)
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        output_dir = "./results_quick_test"
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥ (transformers ë°©ì‹)
        model_save_path = os.path.join(output_dir, "final_model")
        os.makedirs(model_save_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(model_save_path)
        processor.save_pretrained(model_save_path)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"\nğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print(f"   ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸: python test_my_voice.py your_audio.wav --model_path {model_save_path}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()