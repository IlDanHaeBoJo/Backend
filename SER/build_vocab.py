# Backend/SER/build_vocab.py
import os
import json
from tqdm import tqdm
from collections import Counter

DATA_DIR = "/data/ghdrnjs/SER/small/"
OUTPUT_FILE = "char_to_id.json"

def build_vocab():
    """ë°ì´í„°ì…‹ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ Character Vocabularyë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"ğŸ” ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì „ì‚¬ íŒŒì¼(.txt)ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤: {DATA_DIR}")
    transcript_paths = os.path.join(DATA_DIR, "script.txt")
    print(transcript_paths)

    if not transcript_paths:
        print(f"âŒ '{DATA_DIR}'ì—ì„œ ì „ì‚¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    all_text = ""
    print("ğŸ“– ì „ì‚¬ íŒŒì¼ì„ ì½ëŠ” ì¤‘...")
    try:
        with open(transcript_paths, 'r', encoding='utf-8') as f:
            all_text = f.read().strip()
    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {transcript_paths} - {e}")
        return

    print("ğŸ“Š ë¬¸ì ë¹ˆë„ìˆ˜ ê³„ì‚° ì¤‘...")
    # ëª¨ë“  ê¸€ìì˜ ë¹ˆë„ ê³„ì‚°
    char_counts = Counter(all_text)
    
    # Vocabulary ìƒì„± (ë¹ˆë„ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)
    # <pad>: íŒ¨ë”©ìš©, <unk>: ëª¨ë¥´ëŠ” ê¸€ììš©
    vocab = ['<pad>', '<unk>'] + [char for char, count in char_counts.most_common()]
    
    char_to_id = {char: i for i, char in enumerate(vocab)}
    
    print(f"ğŸ‰ ì´ {len(vocab)}ê°œì˜ ê³ ìœ í•œ ë¬¸ìë¡œ Vocabulary ìƒì„± ì™„ë£Œ!")
    
    # íŒŒì¼ë¡œ ì €ì¥
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(char_to_id, f, ensure_ascii=False, indent=4)
        
    print(f"ğŸ’¾ Vocabularyê°€ '{OUTPUT_FILE}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("\n-- ì˜ˆì‹œ --")
    print(list(char_to_id.items())[:20]) # ì²˜ìŒ 20ê°œ ì˜ˆì‹œ ì¶œë ¥

if __name__ == "__main__":
    build_vocab()