import torch
import torch.nn as nn
import torch.optim as optim
from transformers import Wav2Vec2Processor, Wav2Vec2Config
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings('ignore')

from . import config
from .datasets import EmotionDataset, collate_fn, preprocess_audio
from .utils import setup_logging, CheckpointManager, validate_audio_files
from .data_utils import split_speaker_and_content, build_corpus_index, extract_speaker_id, extract_number_from_filename
from .Wav2Vec2_seq_clf import custom_Wav2Vec2ForEmotionClassification
from .model_utils import enable_last_k_blocks

from collections import Counter
from tqdm import tqdm
from sklearn.metrics import classification_report, accuracy_score, f1_score

import wandb
import random

def create_model_and_processor(freeze_base_model: bool = True):
    """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {config.model.model_name}")
    
    # ì„¤ì • ìˆ˜ì •
    model_config = Wav2Vec2Config.from_pretrained(
        config.model.model_name,
        num_labels=config.model.num_labels,
        label2id=config.model.label2id,
        id2label=config.model.id2label,
        finetuning_task="emotion_classification"
    )
    
    if config.char_vocab:
        model_config.char_vocab_size = len(config.char_vocab)
    
    model = custom_Wav2Vec2ForEmotionClassification.from_pretrained(
        config.model.model_name,
        config=model_config,
        ignore_mismatched_sizes=True
    )
    
    processor = Wav2Vec2Processor.from_pretrained(config.model.model_name)
    
    if freeze_base_model:
        for param in model.wav2vec2.parameters():
            param.requires_grad = False
        print("âœ… Base model íŒŒë¼ë¯¸í„° ê³ ì •")
    
    return model, processor

def train_model(model, train_loader, val_loader, device, num_epochs=3, learning_rate=3e-5):
    """ì§ì ‘ í›ˆë ¨ ë£¨í”„"""
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)
    print("ğŸš€ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)")
    backbone_params = []
    head_params = []

    for n, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if n.startswith("wav2vec2."):
            backbone_params.append(p)
        else:
            # pooler, stats_projector, projector(ë¶€ëª¨), classifier, adversaries ë“±
            head_params.append(p)

    optimizer = optim.AdamW(
        [
            {"params": backbone_params, "lr": 5e-6, "weight_decay": 0.01},
            {"params": head_params,     "lr": 1e-4, "weight_decay": 0.01},
        ]
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * num_epochs
    accumulation_steps = 16
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)
    
    best_f1 = 0
    best_model_state = None
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
    print(f"   ì´ ì—í¬í¬: {num_epochs}")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    max_adv = 0.1
    warmup_epochs = 1.0
    class_weights = torch.tensor([2.0, 1.5, 0.7], dtype=torch.float32).to(device)

    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
        
        # í›ˆë ¨
        model.train()
        train_loss = 0
        train_predictions = []
        train_true_labels = []
        optimizer.zero_grad()

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training")
        for step, batch in enumerate(progress_bar):
            current_step = epoch * len(train_loader) + step
            total_warmup_steps = warmup_epochs * len(train_loader)
            progress = min(1.0, current_step / total_warmup_steps)
            current_adv_lambda = max_adv * progress
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=current_adv_lambda,
                speaker_ids=batch['speaker_ids'].to(device),
                class_weights = class_weights
            )
            
            loss = outputs['loss']
            if loss is None or torch.isnan(loss) or torch.isinf(loss):
                continue

            loss /= accumulation_steps
            loss.backward()

            if (step + 1) % accumulation_steps == 0:
              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
              optimizer.step()
              scheduler.step()
              optimizer.zero_grad()
            
            train_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            train_predictions.extend(preds.cpu().numpy())
            train_true_labels.extend(batch['labels'].cpu().numpy())
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.6f}'
            })
        
        train_loss /= len(train_loader)
        
        # í›ˆë ¨ ì •í™•ë„ ë° F1 ê³„ì‚°
        train_accuracy = accuracy_score(train_true_labels, train_predictions)
        train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')

        # Weight & Biases ë¡œê¹…
        wandb.log({
            "epoch": epoch,
            "train/loss": train_loss,
            "train/accuracy": train_accuracy,
            "train/f1": train_f1
        })

        # ê²€ì¦
        print("ğŸ“Š ê²€ì¦ ì¤‘...")
        val_results = evaluate_model(model, val_loader, device, epoch=epoch, is_training=True)
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, F1: {val_results['f1']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_results['f1'] > best_f1:
            best_f1 = val_results['f1']
            best_model_state = model.state_dict().copy()
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’« ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1:.4f})")
    
    return model

def evaluate_model(model, dataloader, device, epoch=None, is_training=False):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="í‰ê°€ ì¤‘"):
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.0, # í‰ê°€ ì‹œì—ëŠ” ì ëŒ€ì  ì†ì‹¤ ë°˜ì˜ ì•ˆí•¨,
                speaker_ids = None,
            )
            
            loss = outputs['loss']
            
            # í›ˆë ¨ ì¤‘ì´ ì•„ë‹ ë•Œë„ lossê°€ ê³„ì‚°ë˜ë„ë¡ adv_lambda=0.0ìœ¼ë¡œ í˜¸ì¶œ
            # ë§Œì•½ lossê°€ Noneì´ë©´ (test set ë“±ì—ì„œ content_labelì´ ì—†ì„ ê²½ìš°), ê°ì • ì†ì‹¤ë§Œ ê³„ì‚°
            if loss is None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(outputs['emotion_logits'].view(-1, model.config.num_labels), batch['labels'].to(device).view(-1))

            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(batch['labels'].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')

    if is_training:
        wandb.log({
            "epoch" : epoch,
            "val/loss": loss,
            "val/accuracy": accuracy,
            "val/f1": f1,
        })
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'f1': f1,
        'predictions': predictions,
        'true_labels': true_labels
    }

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    logger = setup_logging("INFO", "training.log")
    logger.info("ğŸš€ SER ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    
    logger.info(f"ëª¨ë¸: {config.model.model_name}")
    logger.info(f"ê°ì • ë¼ë²¨: {config.model.emotion_labels}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {config.training.batch_size}")
    logger.info(f"í•™ìŠµë¥ : {config.training.learning_rate}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ì‚¬ìš© ì¥ì¹˜: {device}")
    model, processor = create_model_and_processor()
    enable_last_k_blocks(model, last_k=4)
    model.to(device)

    # Weight & Biases ì´ˆê¸°í™”
    wandb.init(
    project="Speech_Emotion_Recognition",
    name="3 Class Classification",

    config={
        "learning_rate": config.training.learning_rate,
        "epochs": config.training.num_epochs,
        "batch_size": config.training.batch_size,
        "architecture": "custom Wav2Vec2",
    }
    )

    
    # ë°ì´í„° ë¡œë“œ
    index = build_corpus_index(config.paths.data_dir)
    valid_paths, invalid_paths = validate_audio_files([item["path"] for item in index])
    
    if invalid_paths:
        logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ {len(invalid_paths)}ê°œ ì œì™¸")
    
    # ë°ì´í„° ë¶„í• 
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
        split_speaker_and_content(
        index,
        val_content_ratio=0.2,
        test_content_ratio=0.2,
        val_speaker_ratio=0.2,
        test_speaker_ratio=0.2,
    )
    train_speakers = sorted({extract_speaker_id(p, config.paths.data_dir) for p in train_paths})
    num_speakers = len(train_speakers)
    print(f"ğŸ” í™”ì ìˆ˜: {num_speakers}")

    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ")
    print(f"  Test: {len(test_paths)}ê°œ")

    train_emotion_dist = Counter(train_labels)
    val_emotion_dist = Counter(val_labels)
    test_emotion_dist = Counter(test_labels)
    
    print(f"\nğŸ“ˆ ê°ì •ë³„ ë¶„í¬:")
    print(f"  Train: {dict(train_emotion_dist)}")
    print(f"  Validation: {dict(val_emotion_dist)}")
    print(f"  Test: {dict(test_emotion_dist)}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = EmotionDataset(train_paths, train_labels, processor, is_training=True)
    val_dataset = EmotionDataset(val_paths, val_labels, processor, is_training=False)
    test_dataset = EmotionDataset(test_paths, test_labels, processor, is_training=False)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.training.batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.training.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config.training.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )

    # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
    checkpoint_manager = CheckpointManager()
    
    # í›ˆë ¨ ì‹¤í–‰
    try:
        # í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1ì—í¬í¬)
        model = train_model(model, 
                            train_loader, 
                            val_loader, 
                            device, 
                            num_epochs=config.training.num_epochs, 
                            learning_rate=config.training.learning_rate)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€...")
        test_results = evaluate_model(model, test_loader, device, is_training=False)
        
        print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"   - F1 ìŠ¤ì½”ì–´: {test_results['f1']:.4f}")
        print(f"   - ì†ì‹¤: {test_results['loss']:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report = classification_report(
            test_results['true_labels'], 
            test_results['predictions'],
            target_names=config.model.emotion_labels,
            digits=4
        )
        print(report)
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        output_dir = config.paths.output_dir
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥ (transformers ë°©ì‹)
        model_save_path = os.path.join(output_dir, "adversary_content_speaker_model_augment_v2_epoch_5_last_k_4")
        os.makedirs(model_save_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(model_save_path)
        processor.save_pretrained(model_save_path)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"\nğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print(f"   ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸: python test_my_voice.py your_audio.wav --model_path {model_save_path}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    logger.info("âœ… í›ˆë ¨ ì™„ë£Œ")





if __name__ == "__main__":
    main()