#!/usr/bin/env python3
"""
í•™ìŠµëœ ëª¨ë¸ë¡œ ë‚´ ìŒì„± íŒŒì¼ í…ŒìŠ¤íŠ¸í•˜ê¸°
ì‚¬ìš©ë²•: python test_my_voice.py your_audio_file.wav
"""

import os
import sys
import torch
import numpy as np
import librosa
import argparse
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor
import warnings
warnings.filterwarnings('ignore')

# ê°ì • ë¼ë²¨ ì •ì˜ (í•™ìŠµ ì‹œì™€ ë™ì¼í•´ì•¼ í•¨)
EMOTION_LABELS = ["Anxious", "Dry", "Kind"]
LABEL2ID = {label: i for i, label in enumerate(EMOTION_LABELS)}
ID2LABEL = {i: label for i, label in enumerate(EMOTION_LABELS)}

# ëª¨ë¸ ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼)
MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
SAMPLE_RATE = 16000
MAX_DURATION = 10.0

def load_trained_model(model_path=None):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    
    if model_path and os.path.exists(model_path):
        # ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ
        print(f"ğŸ¤– ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
        model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)
        processor = Wav2Vec2Processor.from_pretrained(model_path)
    else:
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (í•™ìŠµ ì „ ìƒíƒœ)
        print(f"âš ï¸  ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (í•™ìŠµ ì „): {MODEL_NAME}")
        print("   í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
        
        from transformers import Wav2Vec2Config
        
        config = Wav2Vec2Config.from_pretrained(
            MODEL_NAME,
            num_labels=len(EMOTION_LABELS),
            label2id=LABEL2ID,
            id2label=ID2LABEL,
            finetuning_task="emotion_classification"
        )
        
        model = Wav2Vec2ForSequenceClassification.from_pretrained(
            MODEL_NAME,
            config=config,
            ignore_mismatched_sizes=True
        )
        processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    
    return model, processor

def preprocess_audio(file_path, processor):
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)"""
    try:
        print(f"ğŸµ ì˜¤ë””ì˜¤ ë¡œë”©: {file_path}")
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ (ë‹¤ì–‘í•œ í¬ë§· ì§€ì›)
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, res_type='kaiser_fast')
        
        # ê¸¸ì´ ì •ë³´ ì¶œë ¥
        duration = len(audio) / SAMPLE_RATE
        print(f"   - ê¸¸ì´: {duration:.2f}ì´ˆ")
        print(f"   - ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sr}Hz")
        
        # ê¸¸ì´ ì¡°ì •
        target_length = int(SAMPLE_RATE * MAX_DURATION)
        if len(audio) > target_length:
            print(f"   - ì˜¤ë””ì˜¤ê°€ {MAX_DURATION}ì´ˆë³´ë‹¤ ê¸¸ì–´ì„œ ìë¦…ë‹ˆë‹¤.")
            # ê°€ìš´ë° ë¶€ë¶„ ì‚¬ìš©
            start_idx = (len(audio) - target_length) // 2
            audio = audio[start_idx:start_idx + target_length]
        elif len(audio) < target_length:
            print(f"   - ì˜¤ë””ì˜¤ê°€ {MAX_DURATION}ì´ˆë³´ë‹¤ ì§§ì•„ì„œ íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.")
            pad_length = target_length - len(audio)
            audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)
        
        # ì •ê·œí™”
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.8
        
        # Wav2Vec2 processorë¡œ ë³€í™˜
        inputs = processor(
            audio,
            sampling_rate=SAMPLE_RATE,
            return_tensors="pt",
            padding=True
        )
        
        return inputs.input_values.squeeze(0)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        print(f"   íŒŒì¼ ê²½ë¡œ: {file_path}")
        print(f"   íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(file_path)}")
        import traceback
        traceback.print_exc()
        return None

def predict_emotion(model, processor, audio_file, device):
    """ê°ì • ì˜ˆì¸¡"""
    
    # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
    input_values = preprocess_audio(audio_file, processor)
    
    if input_values is None:
        return None, None
    
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ
    model.eval()
    model.to(device)
    
    with torch.no_grad():
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_values = input_values.unsqueeze(0).to(device)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        outputs = model(input_values=input_values)
        logits = outputs.logits
        
        # í™•ë¥  ê³„ì‚°
        probabilities = torch.nn.functional.softmax(logits, dim=-1)
        probabilities = probabilities.cpu().numpy()[0]
        
        # ìµœê³  í™•ë¥  í´ë˜ìŠ¤
        predicted_class_id = np.argmax(probabilities)
        predicted_emotion = ID2LABEL[predicted_class_id]
        confidence = probabilities[predicted_class_id]
        
        return predicted_emotion, probabilities

def print_results(audio_file, predicted_emotion, probabilities):
    """ê²°ê³¼ ì¶œë ¥"""
    
    print(f"\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼")
    print("=" * 50)
    print(f"ğŸ“ íŒŒì¼: {os.path.basename(audio_file)}")
    print(f"ğŸ­ ì˜ˆì¸¡ ê°ì •: {predicted_emotion}")
    print(f"ğŸ² í™•ì‹ ë„: {probabilities[LABEL2ID[predicted_emotion]]:.1%}")
    
    print(f"\nğŸ“Š ìƒì„¸ í™•ë¥ :")
    for emotion in EMOTION_LABELS:
        prob = probabilities[LABEL2ID[emotion]]
        bar = "â–ˆ" * int(prob * 20)  # 0-20 ê¸¸ì´ì˜ ë°”
        print(f"  {emotion:8s}: {prob:.1%} {bar}")
    
    # ê°ì •ë³„ ì„¤ëª…
    emotion_descriptions = {
        "Anxious": "ë¶ˆì•ˆ, ì´ˆì¡°í•¨",
        "Dry": "ê±´ì¡°í•¨, ë¬´ë¯¸ê±´ì¡°í•¨", 
        "Kind": "ì¹œì ˆí•¨, ë”°ëœ»í•¨"
    }
    
    print(f"\nğŸ’­ í•´ì„: {emotion_descriptions.get(predicted_emotion, 'ì•Œ ìˆ˜ ì—†ìŒ')}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="í•™ìŠµëœ ëª¨ë¸ë¡œ ìŒì„± ê°ì • ë¶„ì„")
    parser.add_argument("audio_file", help="ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model_path", help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)")
    parser.add_argument("--gpu", type=int, default=None, help="ì‚¬ìš©í•  GPU ë²ˆí˜¸")
    
    args = parser.parse_args()
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.audio_file):
        print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.audio_file}")
        sys.exit(1)
    
    # GPU ì„¤ì •
    if args.gpu is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, processor = load_trained_model(args.model_path)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        print(f"\nğŸ”® ê°ì • ë¶„ì„ ì¤‘...")
        predicted_emotion, probabilities = predict_emotion(
            model, processor, args.audio_file, device
        )
        
        if predicted_emotion is None:
            print("âŒ ì˜ˆì¸¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # ê²°ê³¼ ì¶œë ¥
        print_results(args.audio_file, predicted_emotion, probabilities)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ì¸ì ì—†ì´ ì‹¤í–‰í•  ë•Œ)"""
    print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("ì‚¬ìš©ë²•:")
    print("  python test_my_voice.py your_audio.wav")
    print("  python test_my_voice.py your_audio.wav --model_path ./results_3class_simple/final_model")
    print("\nì§€ì› í˜•ì‹: .wav, .mp3, .m4a, .flac")

if __name__ == "__main__":
    if len(sys.argv) == 1:
        # ì¸ì ì—†ì´ ì‹¤í–‰í•˜ë©´ ì‚¬ìš©ë²• í‘œì‹œ
        quick_test()
    else:
        main()