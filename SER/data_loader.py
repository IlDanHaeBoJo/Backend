"""
ìŒì„± ê°ì • ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë¡œë”
"""

import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import Wav2Vec2Processor
import numpy as np
from typing import Dict, List, Tuple, Optional
import json
from sklearn.model_selection import train_test_split

from .preprocessing import preprocessor
from .config import model_config, data_config, training_config

class SpeechEmotionDataset(Dataset):
    """ìŒì„± ê°ì • ë¶„ì„ ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data_paths: List[str],
                 labels: List[str],
                 processor: Wav2Vec2Processor,
                 is_training: bool = True,
                 max_duration: float = 15.0):
        
        self.data_paths = data_paths
        self.labels = labels
        self.processor = processor
        self.is_training = is_training
        self.max_duration = max_duration
        self.sampling_rate = model_config.sampling_rate
        
        # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
        self.label_encoder = model_config.label2id
        self.encoded_labels = [self.label_encoder[label] for label in labels]
        
        print(f"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(self.data_paths)}ê°œ ìƒ˜í”Œ")
        
    def __len__(self):
        return len(self.data_paths)
    
    def __getitem__(self, idx):
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œì™€ ë¼ë²¨
            audio_path = self.data_paths[idx]
            label = self.encoded_labels[idx]
            
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            audio = preprocessor.preprocess(
                audio_path, 
                apply_augmentation=self.is_training
            )
            
            if audio is None:
                # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ë¹ˆ ì˜¤ë””ì˜¤ ë°˜í™˜
                audio = np.zeros(int(self.sampling_rate * self.max_duration))
            
            # Wav2Vec2 processorë¡œ ë³€í™˜
            inputs = self.processor(
                audio,
                sampling_rate=self.sampling_rate,
                return_tensors="pt",
                padding=True,
                max_length=int(self.sampling_rate * self.max_duration),
                truncation=True
            )
            
            return {
                'input_values': inputs.input_values.squeeze(0),
                'attention_mask': inputs.attention_mask.squeeze(0) if 'attention_mask' in inputs else None,
                'labels': torch.tensor(label, dtype=torch.long),
                'audio_path': audio_path
            }
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ (idx: {idx}): {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            dummy_audio = np.zeros(int(self.sampling_rate * self.max_duration))
            inputs = self.processor(
                dummy_audio,
                sampling_rate=self.sampling_rate,
                return_tensors="pt",
                padding=True
            )
            
            return {
                'input_values': inputs.input_values.squeeze(0),
                'attention_mask': inputs.attention_mask.squeeze(0) if 'attention_mask' in inputs else None,
                'labels': torch.tensor(0, dtype=torch.long),  # ê¸°ë³¸ ë¼ë²¨
                'audio_path': 'error'
            }

class DataCollator:
    """ë°°ì¹˜ ë°ì´í„° ì •ë¦¬ë¥¼ ìœ„í•œ collator"""
    
    def __init__(self, processor: Wav2Vec2Processor, padding: bool = True):
        self.processor = processor
        self.padding = padding
    
    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:
        # input_valuesì™€ labels ë¶„ë¦¬
        input_values = [feature['input_values'] for feature in features]
        labels = [feature['labels'] for feature in features]
        attention_masks = [feature.get('attention_mask') for feature in features]
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch = {}
        
        # input_values íŒ¨ë”©
        if self.padding:
            input_values = torch.nn.utils.rnn.pad_sequence(
                input_values, batch_first=True, padding_value=0.0
            )
        else:
            input_values = torch.stack(input_values)
        
        batch['input_values'] = input_values
        
        # attention_mask ì²˜ë¦¬
        if attention_masks[0] is not None:
            if self.padding:
                attention_mask = torch.nn.utils.rnn.pad_sequence(
                    attention_masks, batch_first=True, padding_value=0
                )
            else:
                attention_mask = torch.stack(attention_masks)
            batch['attention_mask'] = attention_mask
        
        # labels
        batch['labels'] = torch.stack(labels)
        
        return batch

def load_dataset_from_directory(data_dir: str, 
                              emotion_mapping: Optional[Dict[str, str]] = None) -> Tuple[List[str], List[str]]:
    """ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ
    
    ì˜ˆìƒ êµ¬ì¡°:
    data_dir/
    â”œâ”€â”€ emotion1/
    â”‚   â”œâ”€â”€ file1.wav
    â”‚   â””â”€â”€ file2.wav
    â”œâ”€â”€ emotion2/
    â”‚   â”œâ”€â”€ file3.wav
    â”‚   â””â”€â”€ file4.wav
    ...
    """
    
    audio_paths = []
    labels = []
    
    if not os.path.exists(data_dir):
        print(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return audio_paths, labels
    
    # ê° ê°ì • í´ë” íƒìƒ‰
    for emotion_folder in os.listdir(data_dir):
        emotion_path = os.path.join(data_dir, emotion_folder)
        
        if not os.path.isdir(emotion_path):
            continue
        
        # ê°ì • ë¼ë²¨ ë§¤í•‘
        emotion_label = emotion_mapping.get(emotion_folder, emotion_folder) if emotion_mapping else emotion_folder
        
        if emotion_label not in model_config.emotion_labels:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” ê°ì • ë¼ë²¨: {emotion_label}")
            continue
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        for audio_file in os.listdir(emotion_path):
            if audio_file.lower().endswith(('.wav', '.mp3', '.m4a', '.flac')):
                audio_path = os.path.join(emotion_path, audio_file)
                audio_paths.append(audio_path)
                labels.append(emotion_label)
    
    print(f"ë¡œë“œëœ ë°ì´í„°: {len(audio_paths)}ê°œ íŒŒì¼")
    return audio_paths, labels

def load_dataset_from_numbered_folders(data_dir: str) -> Tuple[List[str], List[str]]:
    """íŒŒì¼ëª… ë²ˆí˜¸ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë“œ (ì‚¬ìš©ì ë§ì¶¤)
    
    ë°ì´í„° êµ¬ì¡°:
    /data/ghdrnjs/SER/small/
    â”œâ”€â”€ F2001/wav_48000/F2001_000021.wav â†’ Anxious
    â”œâ”€â”€ F2001/wav_48000/F2001_000031.wav â†’ Kind
    â”œâ”€â”€ F2001/wav_48000/F2001_000091.wav â†’ Dry
    â””â”€â”€ ...
    
    íŒŒì¼ëª… ë²ˆí˜¸ì— ë”°ë¥¸ ê°ì • ë§¤í•‘:
    - 000021 ~ 000030: Anxious
    - 000031 ~ 000040: Kind  
    - 000091 ~ 000100: Dry
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: /data/ghdrnjs/SER/small/)
    
    Returns:
        Tuple[List[str], List[str]]: (ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸, ë¼ë²¨ ë¦¬ìŠ¤íŠ¸)
    """
    
    audio_paths = []
    labels = []
    
    if not os.path.exists(data_dir):
        print(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return audio_paths, labels
    
    # íŒŒì¼ëª… ë²ˆí˜¸ì— ë”°ë¥¸ ê°ì • ë§¤í•‘
    def get_emotion_from_filename(filename: str) -> Optional[str]:
        """
        íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°ì • ë¼ë²¨ ë°˜í™˜
        ì˜ˆ: F2001_000021.wav â†’ 21 â†’ Anxious
        """
        try:
            # íŒŒì¼ëª…ì—ì„œ 6ìë¦¬ ë²ˆí˜¸ ì¶”ì¶œ (F2001_000021.wav â†’ 000021)
            import re
            match = re.search(r'_(\d{6})\.', filename)
            if not match:
                return None
            
            file_num = int(match.group(1))
            
            if 21 <= file_num <= 30:
                return "Anxious"
            elif 31 <= file_num <= 40:
                return "Kind"
            elif 91 <= file_num <= 100:
                return "Dry"
            else:
                return None
        except (ValueError, AttributeError):
            return None
    
    # ê° person í´ë” íƒìƒ‰ (F2001, F2002, M2001 ë“±)
    person_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]
    print(f"ë°œê²¬ëœ person í´ë”: {len(person_folders)}ê°œ")
    
    total_files_processed = 0
    emotion_counts = {"Anxious": 0, "Kind": 0, "Dry": 0}
    
    for person_folder in sorted(person_folders):
        person_path = os.path.join(data_dir, person_folder)
        wav_path = os.path.join(person_path, "wav_48000")
        
        # wav_48000 í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not os.path.exists(wav_path):
            print(f"âš ï¸  {person_folder}: wav_48000 í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            continue
        
        person_file_count = {"Anxious": 0, "Kind": 0, "Dry": 0, "Other": 0}
        
        # wav_48000 í´ë” ë‚´ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸
        for audio_file in os.listdir(wav_path):
            if not audio_file.lower().endswith(('.wav', '.mp3', '.m4a', '.flac')):
                continue
            
            # íŒŒì¼ëª…ìœ¼ë¡œ ê°ì • ë¼ë²¨ ê²°ì •
            emotion_label = get_emotion_from_filename(audio_file)
            
            if emotion_label is not None:
                audio_path = os.path.join(wav_path, audio_file)
                audio_paths.append(audio_path)
                labels.append(emotion_label)
                person_file_count[emotion_label] += 1
                emotion_counts[emotion_label] += 1
            else:
                person_file_count["Other"] += 1
            
            total_files_processed += 1
        
        # ê° personë³„ í†µê³„ ì¶œë ¥
        if sum(person_file_count[e] for e in ["Anxious", "Kind", "Dry"]) > 0:
            print(f"ğŸ“ {person_folder}: "
                  f"Anxious={person_file_count['Anxious']}, "
                  f"Kind={person_file_count['Kind']}, "
                  f"Dry={person_file_count['Dry']}, "
                  f"ê¸°íƒ€={person_file_count['Other']}")
    
    # ì „ì²´ í†µê³„ ì¶œë ¥
    print(f"\n{'='*50}")
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
    print(f"{'='*50}")
    print(f"ì²˜ë¦¬ëœ person í´ë”: {len(person_folders)}ê°œ")
    print(f"ì „ì²´ íŒŒì¼ í™•ì¸: {total_files_processed}ê°œ")
    print(f"ì‚¬ìš©ëœ íŒŒì¼: {len(audio_paths)}ê°œ")
    print(f"\ní´ë˜ìŠ¤ë³„ ë¶„í¬:")
    for emotion, count in emotion_counts.items():
        percentage = (count / len(audio_paths) * 100) if len(audio_paths) > 0 else 0
        print(f"  {emotion}: {count}ê°œ ({percentage:.1f}%)")
    
    if len(audio_paths) == 0:
        print("âŒ ì¡°ê±´ì— ë§ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   íŒŒì¼ëª… íŒ¨í„´ì„ í™•ì¸í•´ì£¼ì„¸ìš”: [PREFIX]_[6ìë¦¬ìˆ«ì].wav")
        print("   ì˜ˆìƒ ë²ˆí˜¸ ë²”ìœ„: 000021-000030(Anxious), 000031-000040(Kind), 000091-000100(Dry)")
    
    return audio_paths, labels

def load_dataset_from_csv(csv_path: str, 
                         audio_column: str = 'file_path',
                         label_column: str = 'emotion') -> Tuple[List[str], List[str]]:
    """CSV íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    
    if not os.path.exists(csv_path):
        print(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return [], []
    
    df = pd.read_csv(csv_path)
    audio_paths = df[audio_column].tolist()
    labels = df[label_column].tolist()
    
    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
    valid_paths = []
    valid_labels = []
    
    for path, label in zip(audio_paths, labels):
        if os.path.exists(path) and label in model_config.emotion_labels:
            valid_paths.append(path)
            valid_labels.append(label)
    
    print(f"CSVì—ì„œ ë¡œë“œëœ ìœ íš¨í•œ ë°ì´í„°: {len(valid_paths)}ê°œ íŒŒì¼")
    return valid_paths, labels

def create_data_splits(audio_paths: List[str], 
                      labels: List[str],
                      train_ratio: float = 0.8,
                      val_ratio: float = 0.1,
                      test_ratio: float = 0.1,
                      random_state: int = 42) -> Tuple[Tuple[List[str], List[str]], ...]:
    """ë°ì´í„°ë¥¼ train/val/testë¡œ ë¶„í• """
    
    # trainê³¼ temp(val+test) ë¶„í• 
    train_paths, temp_paths, train_labels, temp_labels = train_test_split(
        audio_paths, labels,
        test_size=(val_ratio + test_ratio),
        random_state=random_state,
        stratify=labels
    )
    
    # tempë¥¼ valê³¼ testë¡œ ë¶„í• 
    val_paths, test_paths, val_labels, test_labels = train_test_split(
        temp_paths, temp_labels,
        test_size=test_ratio / (val_ratio + test_ratio),
        random_state=random_state,
        stratify=temp_labels
    )
    
    print(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ")
    print(f"  Test: {len(test_paths)}ê°œ")
    
    return (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels)

def create_dataloaders(train_data: Tuple[List[str], List[str]],
                      val_data: Tuple[List[str], List[str]],
                      test_data: Optional[Tuple[List[str], List[str]]] = None,
                      processor: Optional[Wav2Vec2Processor] = None) -> Tuple[DataLoader, ...]:
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    
    if processor is None:
        from transformers import Wav2Vec2Processor
        processor = Wav2Vec2Processor.from_pretrained(model_config.model_name)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SpeechEmotionDataset(
        train_data[0], train_data[1], processor, is_training=True
    )
    
    val_dataset = SpeechEmotionDataset(
        val_data[0], val_data[1], processor, is_training=False
    )
    
    # ë°ì´í„° collator
    data_collator = DataCollator(processor)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=training_config.batch_size,
        shuffle=True,
        num_workers=training_config.dataloader_num_workers,
        collate_fn=data_collator,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=training_config.batch_size,
        shuffle=False,
        num_workers=training_config.dataloader_num_workers,
        collate_fn=data_collator,
        pin_memory=True
    )
    
    if test_data is not None:
        test_dataset = SpeechEmotionDataset(
            test_data[0], test_data[1], processor, is_training=False
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=training_config.batch_size,
            shuffle=False,
            num_workers=training_config.dataloader_num_workers,
            collate_fn=data_collator,
            pin_memory=True
        )
        return train_loader, val_loader, test_loader
    
    return train_loader, val_loader

# ì˜ˆì‹œ ì‚¬ìš©ë²•ì„ ìœ„í•œ í•¨ìˆ˜
def prepare_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ (ì‹¤ì œ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ë©´ ìˆ˜ì • í•„ìš”)"""
    
    # í˜„ì¬ëŠ” ë”ë¯¸ ë°ì´í„° ìƒì„±
    print("ì£¼ì˜: ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹Œ ë”ë¯¸ ë°ì´í„°ì…ë‹ˆë‹¤.")
    print("ì‹¤ì œ ë°ì´í„° ê²½ë¡œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
    
    # ì‹¤ì œ ì‚¬ìš© ì‹œ ì•„ë˜ì™€ ê°™ì´ ë°ì´í„° ë¡œë“œ
    # audio_paths, labels = load_dataset_from_directory("./data/emotions/")
    # ë˜ëŠ”
    # audio_paths, labels = load_dataset_from_csv("./data/emotions.csv")
    
    # ë”ë¯¸ ë°ì´í„°
    audio_paths = []
    labels = []
    
    return audio_paths, labels